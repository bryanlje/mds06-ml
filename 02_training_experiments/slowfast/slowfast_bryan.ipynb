{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Build label map & splits**"
      ],
      "metadata": {
        "id": "sk-am0rbtFkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchmetrics decord fvcore pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlv-BF09KLjb",
        "outputId": "d95c0a54-f9ac-4eda-cbc6-89f6edb616fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, csv, glob, os\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
        "from decord import VideoReader, cpu\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-ziMQw9hQwQV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jFlWfIQXhSqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ffbd62-0a49-446b-a608-042b3a8c726d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aV_Tuy68D3w_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. CONFIGURATION**\n",
        "### This class centralizes all hyperparameters and file paths."
      ],
      "metadata": {
        "id": "wP3eWavtFCaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.root_dir = \"/content/drive/MyDrive/FIT3163,3164/SlowFast\"\n",
        "        self.clips_dir = os.path.join(self.root_dir, \"05_clips/sin_tty_2016_1\")\n",
        "        self.splits_dir = os.path.join(self.root_dir, \"06_splits\")\n",
        "        self.models_dir = os.path.join(self.root_dir, \"07_models/slowfast_player\")\n",
        "        self.best_model_path = os.path.join(self.models_dir, \"best.pt\")\n",
        "\n",
        "        self.labels = [\n",
        "            \"smash\", \"jump_smash\", \"block\",\n",
        "            \"drop\", \"clear\", \"lift\", \"drive\",\n",
        "            \"straight_net\", \"cross_net\", \"serve\",\n",
        "            \"push\", \"tap\",\n",
        "            \"average_joe\"\n",
        "        ]\n",
        "\n",
        "        # Dataset parameters\n",
        "        self.side = 224             # ori: 224\n",
        "        self.slow_t = 8             # 8 frames for slow pathway\n",
        "        self.alpha = 4              # ratio between fast and slow\n",
        "        self.fast_t = self.slow_t * self.alpha\n",
        "        self.fast_target = 224      # ori: 224\n",
        "\n",
        "        # Training parameters\n",
        "        self.epochs = 30\n",
        "        self.batch_size = 4\n",
        "        self.learning_rate = 5e-4\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "# Create a configuration object\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "pOEfK8GlFFCP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. DATA PREPARATION**\n",
        "### This function handles all logic for splitting and saving the dataset."
      ],
      "metadata": {
        "id": "A9_EREhMIlz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_splits(config: Config):\n",
        "    \"\"\"\n",
        "    Finds video clips, shuffles them, and splits them into train, val, and test sets.\n",
        "    Saves the splits as CSV files and the label map as a JSON file.\n",
        "    \"\"\"\n",
        "    os.makedirs(config.splits_dir, exist_ok=True)\n",
        "    os.makedirs(config.models_dir, exist_ok=True)\n",
        "\n",
        "    labels_map = {lab: i for i, lab in enumerate(config.labels)}\n",
        "    with open(os.path.join(config.splits_dir, \"labels_map.json\"), \"w\") as f:\n",
        "        json.dump(labels_map, f, indent=2)\n",
        "\n",
        "    items = []\n",
        "    for label in config.labels:\n",
        "        # Use glob to find all video files for the current label\n",
        "        for clip_path in glob.glob(os.path.join(config.clips_dir, label, \"*.mp4\")):\n",
        "            items.append((clip_path, labels_map[label]))\n",
        "\n",
        "    random.seed(1337)\n",
        "    random.shuffle(items)\n",
        "\n",
        "    total_items = len(items)\n",
        "    train_count = int(0.8 * total_items)\n",
        "    val_count = int(0.1 * total_items)\n",
        "    print(f\"Found {total_items} clips in total, splitting to train ({train_count}) and val ({val_count}).\")\n",
        "\n",
        "    splits = {\n",
        "        \"train.csv\": items[:train_count],\n",
        "        \"val.csv\": items[train_count:train_count + val_count],\n",
        "        \"test.csv\": items[train_count + val_count:]\n",
        "    }\n",
        "\n",
        "    for name, data in splits.items():\n",
        "        with open(os.path.join(config.splits_dir, name), \"w\", newline=\"\") as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            csv_writer.writerows(data)\n",
        "\n",
        "    print({k: len(v) for k, v in splits.items()})"
      ],
      "metadata": {
        "id": "tWgRS404IoRy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data_splits(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmj70hvA6Vj",
        "outputId": "cc31cb7f-1c81-442f-f744-a70b0036982e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 101 clips in total, splitting to train (80) and val (10).\n",
            "{'train.csv': 80, 'val.csv': 10, 'test.csv': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. DATASET**\n",
        "### The ClipDataset class handles video loading and preprocessing."
      ],
      "metadata": {
        "id": "DsPRxWf8JREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, csv_path: str, config: Config, train: bool = True):\n",
        "        self.items = [(p, int(y)) for p, y in csv.reader(open(csv_path))]\n",
        "        self.config = config\n",
        "        self.train = train\n",
        "\n",
        "        # Pre-compute normalization tensors\n",
        "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3, 1, 1)\n",
        "        self.std = torch.tensor([0.225, 0.225, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    def _get_frame_indices(self, num_frames: int):\n",
        "        \"\"\"\n",
        "        Return indices for fast (T = fast_t) and slow (stride alpha).\n",
        "        Train: random crop; Eval: center crop.\n",
        "        \"\"\"\n",
        "        need = self.config.fast_t  # 32\n",
        "        if num_frames >= need:\n",
        "            start = np.random.randint(0, num_frames - need + 1) if self.train else max((num_frames - need)//2, 0)\n",
        "            fast_idx = list(range(start, start + need))\n",
        "        else:\n",
        "            fast_idx = list(range(num_frames)) + [num_frames - 1] * (need - num_frames)\n",
        "        slow_idx = fast_idx[::self.config.alpha]  # 32//4 = 8\n",
        "        return slow_idx, fast_idx\n",
        "\n",
        "    def _read_and_process_frames(self, vr: VideoReader, indices: List[int]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns (C, T, H, W) normalized to kinetics-style mean/std.\n",
        "        \"\"\"\n",
        "        import torch.nn.functional as F\n",
        "        try:\n",
        "            frames = vr.get_batch([min(i, len(vr)-1) for i in indices]).asnumpy()  # (T,H,W,C)\n",
        "        except Exception:\n",
        "            frames = np.stack([vr[min(i, len(vr)-1)].asnumpy() for i in indices], axis=0)\n",
        "\n",
        "        x = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0  # (T,C,H,W)\n",
        "        T, C, H, W = x.shape\n",
        "\n",
        "        if self.train:\n",
        "            scale = np.random.uniform(0.7, 1.0)\n",
        "            nh, nw = max(1, int(H*scale)), max(1, int(W*scale))\n",
        "            top  = 0 if H == nh else np.random.randint(0, H - nh + 1)\n",
        "            left = 0 if W == nw else np.random.randint(0, W - nw + 1)\n",
        "            x = x[:, :, top:top+nh, left:left+nw]\n",
        "            if np.random.rand() < 0.5:\n",
        "                x = torch.flip(x, dims=[-1])\n",
        "\n",
        "        x = F.interpolate(x, size=self.config.side, mode=\"bilinear\", align_corners=False)  # (T,C,224,224)\n",
        "        mean = self.mean.to(x)  # (3,1,1)\n",
        "        std  = self.std.to(x)\n",
        "        x = (x - mean) / std\n",
        "        return x.permute(1, 0, 2, 3)  # (C,T,H,W)\n",
        "\n",
        "    def __getitem__(self, i: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], int]:\n",
        "        \"\"\"Loads and preprocesses a single clip and its label.\"\"\"\n",
        "        path, label = self.items[i]\n",
        "        vr = VideoReader(path, ctx=cpu(0))\n",
        "\n",
        "        slow_indices, fast_indices = self._get_frame_indices(len(vr))\n",
        "\n",
        "        slow_clip = self._read_and_process_frames(vr, slow_indices)\n",
        "        fast_clip = self._read_and_process_frames(vr, fast_indices)\n",
        "\n",
        "        return (slow_clip, fast_clip), label\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.items)"
      ],
      "metadata": {
        "id": "t3nn3vXSB60U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slowfast_collate(batch):\n",
        "    # batch: list of [((slow, fast), y), ...]\n",
        "    slows, fasts, ys = [], [], []\n",
        "    for (s, f), y in batch:\n",
        "        slows.append(s)\n",
        "        fasts.append(f)\n",
        "        ys.append(y)\n",
        "    slow = torch.stack(slows, dim=0)  # (B,C,T,H,W)\n",
        "    fast = torch.stack(fasts, dim=0)  # (B,C,T,H,W)\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return [slow, fast], y\n",
        "\n",
        "train_csv = os.path.join(cfg.splits_dir, \"train.csv\")\n",
        "val_csv   = os.path.join(cfg.splits_dir, \"val.csv\")\n",
        "test_csv  = os.path.join(cfg.splits_dir, \"test.csv\")\n",
        "\n",
        "train_ds = ClipDataset(train_csv, cfg, train=True)\n",
        "val_ds   = ClipDataset(val_csv,   cfg, train=False)\n",
        "test_ds  = ClipDataset(test_csv,  cfg, train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=max(1, cfg.batch_size), shuffle=False,\n",
        "    num_workers=2, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
        ")\n",
        "\n",
        "num_classes = len(cfg.labels)\n",
        "print(\"Classes:\", num_classes, cfg.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhuyMRO0BCIi",
        "outputId": "0c385f68-425e-44e5-ddb6-72130a3bb246"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: 13 ['smash', 'jump_smash', 'block', 'drop', 'clear', 'lift', 'drive', 'straight_net', 'cross_net', 'serve', 'push', 'tap', 'average_joe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. TRAINING AND EVALUATION**\n",
        "### This function orchestrates the entire training process."
      ],
      "metadata": {
        "id": "5A5kc3azJqM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) Model: load hub, replace head\n",
        "# =========================\n",
        "torch.hub._validate_not_a_forked_repo = lambda a,b,c: True  # sometimes needed on Colab\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "# Replace classifier (ResNetBasicHead.proj)\n",
        "in_dim = model.blocks[-1].proj.in_features\n",
        "model.blocks[-1].proj = nn.Linear(in_dim, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: freeze early blocks for faster convergence at small data sizes\n",
        "# for p in model.blocks[:-1].parameters():\n",
        "#     p.requires_grad = False\n",
        "\n",
        "# =========================\n",
        "# 4) Optimizer, loss, metrics\n",
        "# =========================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "acc = MulticlassAccuracy(num_classes=num_classes, average='micro').to(device)\n",
        "f1  = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k156bc16JsZV",
        "outputId": "05714fe9-a74f-4ab4-9ce5-1dc792487c2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R101.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R101.pyth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 480M/480M [00:03<00:00, 138MB/s]\n",
            "/tmp/ipython-input-2592873621.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5) Train / validate\n",
        "# =========================\n",
        "best_f1 = -1.0\n",
        "os.makedirs(cfg.models_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    model.train()\n",
        "    acc.reset(); f1.reset()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    first = True\n",
        "    for (slow_fast, y) in train_loader:\n",
        "        if first:\n",
        "            s, f = slow_fast\n",
        "            print(\"slow:\", tuple(s.shape), \"fast:\", tuple(f.shape))\n",
        "            # Expect slow=(B,3,8,224,224) and fast=(B,3,32,224,224)\n",
        "            first = False\n",
        "\n",
        "        # slow_fast is [slow, fast]\n",
        "        slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "            logits = model(slow_fast)     # (B, num_classes)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        acc.update(logits, y)\n",
        "        f1.update(logits, y)\n",
        "\n",
        "    train_loss = total_loss / len(train_ds)\n",
        "    train_acc  = acc.compute().item()\n",
        "    train_f1   = f1.compute().item()\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    acc.reset(); f1.reset()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "        for (slow_fast, y) in val_loader:\n",
        "            slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            logits = model(slow_fast)\n",
        "            loss = criterion(logits, y)\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            acc.update(logits, y)\n",
        "            f1.update(logits, y)\n",
        "\n",
        "    val_loss /= len(val_ds)\n",
        "    val_acc = acc.compute().item()\n",
        "    val_f1  = f1.compute().item()\n",
        "\n",
        "    print(f\"[{epoch+1:02d}/{cfg.epochs}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc*100:.2f}% f1={train_f1:.3f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc*100:.2f}% f1={val_f1:.3f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"labels\": cfg.labels}, cfg.best_model_path)\n",
        "        print(f\"  ↳ saved new best to {cfg.best_model_path} (val_f1={best_f1:.3f})\")\n",
        "\n",
        "print(\"Best val F1:\", best_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXDAY8gmBImr",
        "outputId": "8f3e6cee-3082-4326-b502-c0bf5dad369e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2705692391.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
            "/tmp/ipython-input-2705692391.py:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/30] train_loss=2.3327 acc=17.50% f1=0.078 | val_loss=8.4316 acc=30.00% f1=0.244\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.244)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[02/30] train_loss=1.6748 acc=51.25% f1=0.250 | val_loss=1.4545 acc=40.00% f1=0.317\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.317)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[03/30] train_loss=1.4734 acc=46.25% f1=0.244 | val_loss=3.7975 acc=20.00% f1=0.167\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[04/30] train_loss=1.5120 acc=45.00% f1=0.227 | val_loss=1.2525 acc=40.00% f1=0.286\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[05/30] train_loss=1.3540 acc=51.25% f1=0.323 | val_loss=1.5239 acc=30.00% f1=0.222\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[06/30] train_loss=1.1042 acc=58.75% f1=0.337 | val_loss=1.9573 acc=50.00% f1=0.333\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.333)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[07/30] train_loss=1.1242 acc=57.50% f1=0.386 | val_loss=5.0956 acc=40.00% f1=0.317\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[08/30] train_loss=1.0175 acc=65.00% f1=0.459 | val_loss=1.8457 acc=50.00% f1=0.414\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.414)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[09/30] train_loss=1.0101 acc=66.25% f1=0.516 | val_loss=17.7391 acc=20.00% f1=0.152\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[10/30] train_loss=1.1155 acc=56.25% f1=0.327 | val_loss=2.9020 acc=50.00% f1=0.381\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[11/30] train_loss=0.8829 acc=72.50% f1=0.486 | val_loss=3.1649 acc=30.00% f1=0.222\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[12/30] train_loss=0.9112 acc=68.75% f1=0.477 | val_loss=1.0126 acc=60.00% f1=0.393\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[13/30] train_loss=0.9868 acc=65.00% f1=0.413 | val_loss=3.5007 acc=60.00% f1=0.438\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.438)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[14/30] train_loss=1.0836 acc=62.50% f1=0.507 | val_loss=28.1861 acc=30.00% f1=0.157\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[15/30] train_loss=0.9934 acc=68.75% f1=0.606 | val_loss=1.9385 acc=40.00% f1=0.438\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.438)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[16/30] train_loss=0.6794 acc=81.25% f1=0.656 | val_loss=1.6801 acc=40.00% f1=0.258\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[17/30] train_loss=0.5228 acc=83.75% f1=0.675 | val_loss=2.1346 acc=20.00% f1=0.146\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[18/30] train_loss=0.4998 acc=85.00% f1=0.842 | val_loss=1.2698 acc=60.00% f1=0.429\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[19/30] train_loss=0.6941 acc=72.50% f1=0.652 | val_loss=1.9365 acc=60.00% f1=0.315\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[20/30] train_loss=0.6117 acc=82.50% f1=0.653 | val_loss=2.3562 acc=40.00% f1=0.258\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[21/30] train_loss=0.5328 acc=81.25% f1=0.815 | val_loss=2.3778 acc=70.00% f1=0.381\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[22/30] train_loss=0.4461 acc=87.50% f1=0.904 | val_loss=2.1729 acc=40.00% f1=0.230\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[23/30] train_loss=0.3370 acc=90.00% f1=0.886 | val_loss=2.2405 acc=30.00% f1=0.185\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[24/30] train_loss=0.2844 acc=91.25% f1=0.822 | val_loss=2.1141 acc=50.00% f1=0.292\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[25/30] train_loss=0.3511 acc=90.00% f1=0.893 | val_loss=2.7439 acc=30.00% f1=0.185\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[26/30] train_loss=0.1858 acc=96.25% f1=0.950 | val_loss=2.1872 acc=50.00% f1=0.292\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[27/30] train_loss=0.2042 acc=93.75% f1=0.928 | val_loss=0.9875 acc=50.00% f1=0.342\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[28/30] train_loss=0.2786 acc=91.25% f1=0.813 | val_loss=1.6923 acc=60.00% f1=0.452\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/best.pt (val_f1=0.452)\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[29/30] train_loss=0.2440 acc=93.75% f1=0.955 | val_loss=2.2267 acc=50.00% f1=0.378\n",
            "slow: (4, 3, 8, 224, 224) fast: (4, 3, 32, 224, 224)\n",
            "[30/30] train_loss=0.1562 acc=97.50% f1=0.985 | val_loss=2.0212 acc=40.00% f1=0.240\n",
            "Best val F1: 0.4523809552192688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate on test set"
      ],
      "metadata": {
        "id": "rWumfwrwgrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestManager:\n",
        "    \"\"\"\n",
        "    Manages the evaluation process for a SlowFast model on a test set.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: 'Config', device: str):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.num_classes = len(config.labels)\n",
        "        self.model = self._load_model()\n",
        "        self.test_loader = self._create_dataloader()\n",
        "        self.metrics = self._initialize_metrics()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Loads the pre-trained SlowFast model and the fine-tuned checkpoint.\"\"\"\n",
        "        print(\"Loading model and best checkpoint...\")\n",
        "\n",
        "        # Disable the internal hub check for local loading\n",
        "        torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "\n",
        "        model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "        in_dim = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Linear(in_dim, self.num_classes)\n",
        "\n",
        "        # Load the state dictionary from the checkpoint file\n",
        "        ckpt = torch.load(self.config.best_model_path, map_location=self.device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def _create_dataloader(self):\n",
        "        \"\"\"Creates and returns the DataLoader for the test set.\"\"\"\n",
        "        test_ds = ClipDataset(os.path.join(self.config.splits_dir, \"test.csv\"), self.config, train=False)\n",
        "        return DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=max(1, self.config.batch_size),\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "            # collate_fn=slowfast_collate,  # Make sure this is imported if needed\n",
        "            persistent_workers=False\n",
        "        )\n",
        "\n",
        "    def _initialize_metrics(self):\n",
        "        \"\"\"Initializes all the evaluation metrics.\"\"\"\n",
        "        return {\n",
        "            'top1': MulticlassAccuracy(num_classes=self.num_classes, average=\"micro\").to(self.device),\n",
        "            'top3': MulticlassAccuracy(num_classes=self.num_classes, top_k=3).to(self.device),\n",
        "            'f1_macro': MulticlassF1Score(num_classes=self.num_classes, average=\"macro\").to(self.device),\n",
        "            'f1_perclass': MulticlassF1Score(num_classes=self.num_classes, average=None).to(self.device),\n",
        "            'cm': MulticlassConfusionMatrix(num_classes=self.num_classes).to(self.device)\n",
        "        }\n",
        "\n",
        "    def run_inference(self):\n",
        "        \"\"\"Runs the inference loop and computes all metrics and predictions.\"\"\"\n",
        "        print(\"Starting inference on the test set...\")\n",
        "        test_loss = 0.0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad(), torch.amp.autocast(self.device, enabled=(self.device == \"cuda\")):\n",
        "            for batch_idx, (slow_fast, y) in enumerate(self.test_loader):\n",
        "                # Ensure input tensors are lists\n",
        "                if not isinstance(slow_fast, list):\n",
        "                    slow_fast = [slow_fast]\n",
        "\n",
        "                slow_fast = [t.to(self.device, non_blocking=True) for t in slow_fast]\n",
        "                y = y.to(self.device, non_blocking=True)\n",
        "\n",
        "                logits = self.model(slow_fast)\n",
        "                loss = criterion(logits, y)\n",
        "                test_loss += loss.item() * y.size(0)\n",
        "\n",
        "                # Update metrics\n",
        "                for metric in self.metrics.values():\n",
        "                    metric.update(logits, y)\n",
        "\n",
        "                # Collect per-sample predictions for later saving\n",
        "                probs = self.softmax(logits)\n",
        "                conf, pred = probs.max(dim=1)\n",
        "                topk_conf, topk_idx = probs.topk(3, dim=1)\n",
        "\n",
        "                start_idx = batch_idx * self.test_loader.batch_size\n",
        "\n",
        "                for i in range(y.size(0)):\n",
        "                    idx = start_idx + i\n",
        "                    path = self.test_loader.dataset.items[idx][0]\n",
        "                    row = {\n",
        "                        \"path\": path,\n",
        "                        \"file\": os.path.basename(path),\n",
        "                        \"true_idx\": int(y[i]),\n",
        "                        \"true_label\": self.config.labels[int(y[i])],\n",
        "                        \"pred_idx\": int(pred[i]),\n",
        "                        \"pred_label\": self.config.labels[int(pred[i])],\n",
        "                        \"pred_prob\": float(conf[i]),\n",
        "                        \"top1_label\": self.config.labels[int(topk_idx[i,0])],\n",
        "                        \"top1_prob\":  float(topk_conf[i,0]),\n",
        "                        \"top2_label\": self.config.labels[int(topk_idx[i,1])],\n",
        "                        \"top2_prob\":  float(topk_conf[i,1]),\n",
        "                        \"top3_label\": self.config.labels[int(topk_idx[i,2])],\n",
        "                        \"top3_prob\":  float(topk_conf[i,2]),\n",
        "                    }\n",
        "                    all_predictions.append(row)\n",
        "\n",
        "        test_loss /= len(self.test_loader.dataset)\n",
        "        return test_loss, all_predictions\n",
        "\n",
        "    def compute_and_print_results(self, test_loss):\n",
        "        \"\"\"Computes and prints the final metrics.\"\"\"\n",
        "        acc1 = self.metrics['top1'].compute().item()\n",
        "        acc3 = self.metrics['top3'].compute().item()\n",
        "        f1M = self.metrics['f1_macro'].compute().item()\n",
        "        percls = self.metrics['f1_perclass'].compute().detach().cpu().tolist()\n",
        "        confmat = self.metrics['cm'].compute().detach().cpu().numpy()\n",
        "\n",
        "        print(f\"\\nTEST: loss={test_loss:.4f} | acc@1={acc1*100:.2f}% | acc@3={acc3*100:.2f}% | macro-F1={f1M:.3f}\")\n",
        "        print(\"\\nPer-class F1:\")\n",
        "        for lab, s in sorted(zip(self.config.labels, percls), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {lab:15s} {s:.3f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\n",
        "        print(confmat)\n",
        "\n",
        "    def save_predictions(self, predictions: list, print_n: int=10):\n",
        "        \"\"\"Saves the list of predictions to a CSV file.\"\"\"\n",
        "        df = pd.DataFrame(predictions)\n",
        "        save_path = os.path.join(self.config.models_dir, \"test_predictions.csv\")\n",
        "        df.to_csv(save_path, index=False)\n",
        "        print(f\"\\nSaved per-sample predictions to: {save_path}\")\n",
        "        print(\"\\nQuick peek at the predictions:\")\n",
        "        print(df.head(print_n)[[\"file\", \"true_label\", \"pred_label\", \"pred_prob\", \"top2_label\", \"top2_prob\", \"top3_label\", \"top3_prob\"]])"
      ],
      "metadata": {
        "id": "A9WccyBwMrMY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "test_manager = TestManager(cfg, device)\n",
        "test_loss, all_predictions = test_manager.run_inference()\n",
        "test_manager.compute_and_print_results(test_loss)\n",
        "test_manager.save_predictions(all_predictions, print_n=len(all_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO-6i20oihsn",
        "outputId": "e999b716-da25-4a2b-c47f-7757bbb46c5e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and best checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting inference on the test set...\n",
            "\n",
            "TEST: loss=1.2208 | acc@1=81.82% | acc@3=93.33% | macro-F1=0.657\n",
            "\n",
            "Per-class F1:\n",
            "  drop            1.000\n",
            "  cross_net       1.000\n",
            "  serve           1.000\n",
            "  lift            0.800\n",
            "  straight_net    0.800\n",
            "  smash           0.000\n",
            "  jump_smash      0.000\n",
            "  block           0.000\n",
            "  clear           0.000\n",
            "  drive           0.000\n",
            "  push            0.000\n",
            "  tap             0.000\n",
            "  average_joe     0.000\n",
            "\n",
            "Confusion Matrix (rows=true, cols=predicted):\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Saved per-sample predictions to: /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/slowfast_player/test_predictions.csv\n",
            "\n",
            "Quick peek at the predictions:\n",
            "              file    true_label    pred_label  pred_prob top2_label  \\\n",
            "0   b04544c712.mp4         serve         serve   0.984270      smash   \n",
            "1   77d247dcc3.mp4          lift         block   0.518831       lift   \n",
            "2   3b1763aec8.mp4          lift          lift   0.915589       drop   \n",
            "3   0bb7831bd7.mp4          drop          drop   0.994717      serve   \n",
            "4   7dfc0f358e.mp4         serve         serve   0.956099       lift   \n",
            "5   31262ac161.mp4  straight_net          push   0.981224      clear   \n",
            "6   9020c203ef.mp4  straight_net  straight_net   0.983791       lift   \n",
            "7   9af0347b2d.mp4          lift          lift   0.995815  cross_net   \n",
            "8   7c2ee2b245.mp4         serve         serve   0.999982      smash   \n",
            "9   c906075c5a.mp4  straight_net  straight_net   0.902073       lift   \n",
            "10  d73cc0e253.mp4     cross_net     cross_net   0.999998       lift   \n",
            "\n",
            "    top2_prob    top3_label     top3_prob  \n",
            "0    0.005689  straight_net  3.213277e-03  \n",
            "1    0.304275          push  1.642168e-01  \n",
            "2    0.035990  straight_net  2.066728e-02  \n",
            "3    0.003776         clear  6.042350e-04  \n",
            "4    0.017341         smash  7.816372e-03  \n",
            "5    0.018148     cross_net  3.369700e-04  \n",
            "6    0.009458         block  2.757874e-03  \n",
            "7    0.003489    jump_smash  3.535260e-04  \n",
            "8    0.000011          lift  2.424937e-06  \n",
            "9    0.085854          drop  9.808124e-03  \n",
            "10   0.000002  straight_net  5.537510e-09  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **end-to-end match inference & overlay**"
      ],
      "metadata": {
        "id": "nU9T0fc3nRx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug-sBuM4nkRl",
        "outputId": "ae3d3ee4-bf4d-417c-d341-dbe9e3f7eefc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, numpy as np, torch\n",
        "from collections import deque, defaultdict\n",
        "from ultralytics import YOLO\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oaoCKiwUgy97"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_slowfast_classifier(cfg, ckpt_path):\n",
        "    torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "    in_dim = model.blocks[-1].proj.in_features\n",
        "    model.blocks[-1].proj = torch.nn.Linear(in_dim, len(cfg.labels))\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "    model.eval().to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mx6acAn2obMz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_pad_square(img_rgb: np.ndarray, side: int = 224) -> np.ndarray:\n",
        "    \"\"\"Keep aspect ratio; resize the longer side to `side`, then pad to (side, side).\"\"\"\n",
        "    h, w = img_rgb.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((side, side, 3), dtype=img_rgb.dtype)\n",
        "    scale = side / max(h, w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    resized = cv2.resize(img_rgb, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    top  = (side - nh) // 2\n",
        "    bottom = side - nh - top\n",
        "    left = (side - nw) // 2\n",
        "    right = side - nw - left\n",
        "    out = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128,128,128))\n",
        "    return out\n",
        "\n",
        "def expand_box(x1, y1, x2, y2, scale: float, W: int, H: int):\n",
        "    \"\"\"Optionally enlarge the bbox to keep some context (e.g., racket).\"\"\"\n",
        "    cx, cy = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
        "    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale\n",
        "    nx1, ny1 = int(max(0, cx - bw / 2)), int(max(0, cy - bh / 2))\n",
        "    nx2, ny2 = int(min(W - 1, cx + bw / 2)), int(min(H - 1, cy + bh / 2))\n",
        "    return nx1, ny1, nx2, ny2"
      ],
      "metadata": {
        "id": "iasMXtJYsCZj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlowFastPredictor:\n",
        "    def __init__(self, cfg, model):\n",
        "        self.cfg = cfg\n",
        "        self.model = model\n",
        "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1).to(device)\n",
        "        self.std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1).to(device)\n",
        "\n",
        "    def _prep(self, frames_rgb_list):\n",
        "        \"\"\"\n",
        "        frames_rgb_list: list of 32 frames, each HxWx3 in RGB\n",
        "        Returns: [slow, fast] tensors shaped (1,C,T,H,W)\n",
        "        \"\"\"\n",
        "        # Stack to (T,H,W,3) -> (T,C,H,W)\n",
        "        x = torch.from_numpy(np.stack(frames_rgb_list)).permute(0,3,1,2).float() / 255.0  # (T,C,H,W)\n",
        "        # Resize treating T as batch\n",
        "        x = F.interpolate(x, size=self.cfg.side, mode=\"bilinear\", align_corners=False)    # (T,C,224,224)\n",
        "        # Normalize\n",
        "        mean = self.mean.to(device=x.device, dtype=x.dtype)\n",
        "        std  = self.std.to(device=x.device, dtype=x.dtype)\n",
        "        x = (x - mean) / std                                                   # (T,C,224,224)\n",
        "        # (C,T,H,W)\n",
        "        x = x.permute(1,0,2,3)\n",
        "        fast = x.unsqueeze(0).to(device)             # (1,C,32,224,224)\n",
        "        slow = x[:, ::self.cfg.alpha, :, :].unsqueeze(0).to(device)  # stride-4 -> (1,C,8,224,224)\n",
        "        return [slow, fast]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_probs(self, frames_rgb_list):\n",
        "        assert len(frames_rgb_list) == self.cfg.fast_t  # 32\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
        "            inp = self._prep(frames_rgb_list)\n",
        "            logits = self.model(inp)                  # (1, num_classes)\n",
        "            probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "        return probs  # (C,)"
      ],
      "metadata": {
        "id": "PsnwdZrHnVua"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_match_video(\n",
        "    cfg,\n",
        "    video_path,\n",
        "    out_path,\n",
        "    yolo_weights=\"yolo11n.pt\", # change to your custom weights if you have them\n",
        "    person_class=0,            # COCO 'person'\n",
        "    det_conf=0.5,\n",
        "    iou=0.5,\n",
        "    pred_thr=0.60,             # minimum prob to show label\n",
        "    cooldown=12                # frames to cool after showing a shot to reduce spam\n",
        "):\n",
        "    # Get video props for the writer\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = max(1.0, cap.get(cv2.CAP_PROP_FPS))\n",
        "    W   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(out_path, fourcc, fps, (W, H))\n",
        "\n",
        "    # Load detector+tracker\n",
        "    yolo = YOLO(yolo_weights)\n",
        "\n",
        "    # Load classifier\n",
        "    clf_model = load_slowfast_classifier(cfg, cfg.best_model_path)\n",
        "    clf = SlowFastPredictor(cfg, clf_model)\n",
        "\n",
        "    # Per-track state\n",
        "    buffers = defaultdict(lambda: deque(maxlen=cfg.fast_t))            # 32-frame RGB crops per track\n",
        "    last_shown_frame = defaultdict(lambda: -99999)                     # cooldown control\n",
        "    hist = defaultdict(lambda: deque(maxlen=5))                        # small temporal smoothing buffer\n",
        "\n",
        "    frame_idx = 0\n",
        "    for res in yolo.track(source=video_path, stream=True, persist=True,\n",
        "                          classes=[person_class], conf=det_conf, iou=iou, verbose=False):\n",
        "        frame_bgr = res.orig_img  # BGR\n",
        "        h, w = frame_bgr.shape[:2]\n",
        "\n",
        "        # If no boxes/ids in this frame, just write it\n",
        "        if res.boxes is None or res.boxes.id is None:\n",
        "            writer.write(frame_bgr)\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        ids = res.boxes.id.int().cpu().numpy()\n",
        "        xyxy = res.boxes.xyxy.int().cpu().numpy()  # (N,4)\n",
        "\n",
        "        to_draw = []  # (x1,y1,x2,y2,label,prob,tid)\n",
        "\n",
        "        for j, tid in enumerate(ids):\n",
        "            x1, y1, x2, y2 = xyxy[j]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w-1, x2), min(h-1, y2)\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            # NEW: enlarge a bit for context (optional, try 1.2–1.4)\n",
        "            x1, y1, x2, y2 = expand_box(x1, y1, x2, y2, scale=1.25, W=w, H=h)\n",
        "\n",
        "            # Crop -> RGB -> letterbox to fixed square\n",
        "            crop = frame_bgr[y1:y2, x1:x2, :]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "            crop_rgb = resize_pad_square(crop_rgb, side=cfg.side)  # now every frame is 224x224\n",
        "\n",
        "            buffers[tid].append(crop_rgb)\n",
        "\n",
        "            label_to_show = None\n",
        "            prob_to_show  = 0.0\n",
        "\n",
        "            # Classify when we have a full 32-frame clip\n",
        "            if len(buffers[tid]) == cfg.fast_t:\n",
        "                probs = clf.predict_probs(list(buffers[tid]))  # (C,)\n",
        "                ci = int(probs.argmax())\n",
        "                pi = float(probs[ci])\n",
        "                hist[tid].append((ci, pi))\n",
        "\n",
        "                # Small smoothing: require at least 2 of the last 3 agreeing + prob >= thr\n",
        "                if len(hist[tid]) >= 3:\n",
        "                    last3 = list(hist[tid])[-3:]\n",
        "                else:\n",
        "                    last3 = list(hist[tid])\n",
        "\n",
        "                # Choose the label with the highest mean prob among last3\n",
        "                if last3:\n",
        "                    classes = [c for c, p in last3 if cfg.labels[c] != \"average_joe\" and p >= pred_thr]\n",
        "                    if classes:\n",
        "                        # pick the most common; break ties by highest avg prob\n",
        "                        uniq = set(classes)\n",
        "                        best_c, best_score = None, -1.0\n",
        "                        for u in uniq:\n",
        "                            avgp = np.mean([p for (c, p) in last3 if c == u])\n",
        "                            score = (classes.count(u), avgp)  # (count, avgp)\n",
        "                            if score > (classes.count(best_c) if best_c is not None else -1, best_score):\n",
        "                                best_c, best_score = u, avgp\n",
        "                        if best_c is not None and (frame_idx - last_shown_frame[tid] >= cooldown):\n",
        "                            label_to_show = cfg.labels[best_c]\n",
        "                            prob_to_show = float(best_score)\n",
        "                            last_shown_frame[tid] = frame_idx\n",
        "\n",
        "            # Queue drawing if we have a confident non-background label\n",
        "            if label_to_show is not None:\n",
        "                to_draw.append((x1, y1, x2, y2, label_to_show, prob_to_show, int(tid)))\n",
        "\n",
        "        # ---- Draw all overlays on this frame ----\n",
        "        for (x1, y1, x2, y2, lab, p, tid) in to_draw:\n",
        "            color = (0, 220, 0)\n",
        "            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n",
        "            txt = f\"#{tid} {lab} {p*100:.1f}%\"\n",
        "            cv2.putText(frame_bgr, txt, (x1, max(20, y1-10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "\n",
        "        writer.write(frame_bgr)\n",
        "        frame_idx += 1\n",
        "\n",
        "    writer.release()\n",
        "    print(f\"Saved annotated video to: {out_path}\")"
      ],
      "metadata": {
        "id": "a36aCc48nXfb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_video  = \"/content/drive/MyDrive/FIT3163,3164/SlowFast/01_raw/lcw_ld_2016_short/1/master.mp4\"\n",
        "out_video = \"/content/match_annotated.mp4\"\n",
        "yolo_weights = \"/content/drive/MyDrive/FIT3163,3164/YOLO/my_yolov8_1.pt\"\n",
        "\n",
        "annotate_match_video(cfg, in_video, out_video,\n",
        "                     yolo_weights=yolo_weights,  # swap if you have a better person/badminton model\n",
        "                     det_conf=0.35, iou=0.5,\n",
        "                     pred_thr=0.60, cooldown=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uex745aPnaJK",
        "outputId": "41d184dd-8bf8-4e09-8996-6a67acb9bbc4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved annotated video to: /content/match_annotated.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utRepM9qoTb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}