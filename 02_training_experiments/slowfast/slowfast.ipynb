{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk-am0rbtFkm"
   },
   "source": [
    "## **Build label map & splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlv-BF09KLjb",
    "outputId": "6e93be61-7574-493a-b90c-1d89009f2026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/sympy-1.13.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.25a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install torchmetrics decord fvcore pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-ziMQw9hQwQV"
   },
   "outputs": [],
   "source": [
    "import json, random, csv, glob, os\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassConfusionMatrix\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFlWfIQXhSqx",
    "outputId": "f80db289-5510-4db6-c43d-0d651991d1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aV_Tuy68D3w_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_all(seed=1023):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_all(2310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP3eWavtFCaZ"
   },
   "source": [
    "## **1. CONFIGURATION**\n",
    "### This class centralizes all hyperparameters and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "pOEfK8GlFFCP"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.root_dir = \".\"\n",
    "        self.clips_dir = os.path.join(self.root_dir, \"3in1\")\n",
    "        self.splits_dir = os.path.join(self.root_dir, \"splits\")\n",
    "        self.models_dir = os.path.join(self.root_dir, \"train1\")\n",
    "        self.best_model_path = os.path.join(self.models_dir, \"best.pt\")\n",
    "\n",
    "        self.labels = [\n",
    "            \"smash\", \"jump_smash\", \"block\",\n",
    "            \"drop\", \"clear\", \"lift\", \"drive\",\n",
    "            \"straight_net\", \"cross_net\", \"serve\",\n",
    "            \"push\", \"tap\",\n",
    "            \"average_joe\"\n",
    "        ]\n",
    "\n",
    "        # Dataset parameters\n",
    "        self.side = 224             # ori: 224\n",
    "        self.slow_t = 8             # 8 frames for slow pathway\n",
    "        self.alpha = 4              # ratio between fast and slow\n",
    "        self.fast_t = self.slow_t * self.alpha\n",
    "        self.fast_target = 224      # ori: 224\n",
    "\n",
    "        # Training parameters\n",
    "        self.epochs = 20\n",
    "        self.batch_size = 8\n",
    "        self.learning_rate = 0.0001\n",
    "        self.weight_decay = 0.001\n",
    "\n",
    "        self.early_stopping_patience = 5\n",
    "\n",
    "# Create a configuration object\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9_EREhMIlz1"
   },
   "source": [
    "## **2. DATA PREPARATION**\n",
    "### This function handles all logic for splitting and saving the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tWgRS404IoRy"
   },
   "outputs": [],
   "source": [
    "def prepare_data_splits(config: Config):\n",
    "    \"\"\"\n",
    "    Finds video clips, shuffles them, and splits them into train, val, and test sets.\n",
    "    Saves the splits as CSV files and the label map as a JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(config.splits_dir, exist_ok=True)\n",
    "    os.makedirs(config.models_dir, exist_ok=True)\n",
    "\n",
    "    labels_map = {lab: i for i, lab in enumerate(config.labels)}\n",
    "    with open(os.path.join(config.splits_dir, \"labels_map.json\"), \"w\") as f:\n",
    "        json.dump(labels_map, f, indent=2)\n",
    "\n",
    "    items = []\n",
    "    for label in config.labels:\n",
    "        # Use glob to find all video files for the current label\n",
    "        for clip_path in glob.glob(os.path.join(config.clips_dir, label, \"*.mp4\")):\n",
    "            items.append((clip_path, labels_map[label]))\n",
    "\n",
    "    random.seed(1337)\n",
    "    random.shuffle(items)\n",
    "\n",
    "    total_items = len(items)\n",
    "    train_count = int(0.8 * total_items)\n",
    "    val_count = int(0.1 * total_items)\n",
    "    print(f\"Found {total_items} clips in total, splitting to train ({train_count}) and val ({val_count}).\")\n",
    "\n",
    "    splits = {\n",
    "        \"train.csv\": items[:train_count],\n",
    "        \"val.csv\": items[train_count:train_count + val_count],\n",
    "        \"test.csv\": items[train_count + val_count:]\n",
    "    }\n",
    "\n",
    "    for name, data in splits.items():\n",
    "        with open(os.path.join(config.splits_dir, name), \"w\", newline=\"\") as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerows(data)\n",
    "\n",
    "    print({k: len(v) for k, v in splits.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqmj70hvA6Vj",
    "outputId": "190bb85c-e0e0-4056-ec90-ffd33048e95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 clips in total, splitting to train (499) and val (62).\n",
      "{'train.csv': 499, 'val.csv': 62, 'test.csv': 63}\n"
     ]
    }
   ],
   "source": [
    "prepare_data_splits(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsPRxWf8JREM"
   },
   "source": [
    "## **3. DATASET**\n",
    "### The ClipDataset class handles video loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "t3nn3vXSB60U"
   },
   "outputs": [],
   "source": [
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, config: Config, train: bool = True):\n",
    "        self.items = [(p, int(y)) for p, y in csv.reader(open(csv_path))]\n",
    "        self.config = config\n",
    "        self.train = train\n",
    "\n",
    "        # Pre-compute normalization tensors\n",
    "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.225, 0.225, 0.225]).view(3, 1, 1)\n",
    "\n",
    "        # Define a composed transform for training\n",
    "        if self.train:\n",
    "            self.train_transforms = v2.Compose([\n",
    "                v2.RandomResizedCrop(\n",
    "                    size=self.config.side,\n",
    "                    scale=(0.7, 1.0),\n",
    "                    ratio=(0.75, 1.333),\n",
    "                    antialias=True\n",
    "                ),\n",
    "                v2.RandomHorizontalFlip(p=0.5),\n",
    "                v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                v2.RandomGrayscale(p=0.2),\n",
    "            ])\n",
    "\n",
    "    def _get_frame_indices(self, num_frames: int):\n",
    "        \"\"\"\n",
    "        Return indices for fast (T = fast_t) and slow (stride alpha).\n",
    "        Train: random crop; Eval: center crop.\n",
    "        \"\"\"\n",
    "        # This part of the code remains unchanged.\n",
    "        need = self.config.fast_t\n",
    "        if num_frames >= need:\n",
    "            start = np.random.randint(0, num_frames - need + 1) if self.train else max((num_frames - need) // 2, 0)\n",
    "            fast_idx = list(range(start, start + need))\n",
    "        else:\n",
    "            fast_idx = list(range(num_frames)) + [num_frames - 1] * (need - num_frames)\n",
    "        slow_idx = fast_idx[::self.config.alpha]\n",
    "        return slow_idx, fast_idx\n",
    "\n",
    "    def _read_and_process_frames(self, vr: VideoReader, indices: List[int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns (C, T, H, W) normalized to kinetics-style mean/std.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            frames = vr.get_batch([min(i, len(vr)-1) for i in indices]).asnumpy()\n",
    "        except Exception:\n",
    "            frames = np.stack([vr[min(i, len(vr)-1)].asnumpy() for i in indices], axis=0)\n",
    "\n",
    "        # Convert to tensor and permute dimensions\n",
    "        x = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0  # (T, C, H, W)\n",
    "\n",
    "        # Apply data augmentation only for training\n",
    "        if self.train:\n",
    "            # Apply the same random transform to all frames\n",
    "            x = self.train_transforms(x)\n",
    "\n",
    "        # Resize to the required size if necessary\n",
    "        x = F.interpolate(x, size=self.config.side, mode=\"bilinear\", align_corners=False) # (T, C, 224, 224)\n",
    "\n",
    "        # Normalize\n",
    "        mean = self.mean.to(x)\n",
    "        std = self.std.to(x)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        return x.permute(1, 0, 2, 3) # (C, T, H, W)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], int]:\n",
    "        \"\"\"Loads and preprocesses a single clip and its label.\"\"\"\n",
    "        path, label = self.items[i]\n",
    "        vr = VideoReader(path, ctx=cpu(0))\n",
    "\n",
    "        # Randomly choose frames from the entire video\n",
    "        slow_indices, fast_indices = self._get_frame_indices(len(vr))\n",
    "\n",
    "        # Get and process clips\n",
    "        slow_clip = self._read_and_process_frames(vr, slow_indices)\n",
    "        fast_clip = self._read_and_process_frames(vr, fast_indices)\n",
    "\n",
    "        return (slow_clip, fast_clip), label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaWSMQwAnd6v"
   },
   "source": [
    "#### **Generate datasets and loaders for training, validation, and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhuyMRO0BCIi",
    "outputId": "d418f1bd-b3e8-4405-d169-5d56948e0e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 13 ['smash', 'jump_smash', 'block', 'drop', 'clear', 'lift', 'drive', 'straight_net', 'cross_net', 'serve', 'push', 'tap', 'average_joe']\n"
     ]
    }
   ],
   "source": [
    "def slowfast_collate(batch):\n",
    "    # batch: list of [((slow, fast), y), ...]\n",
    "    slows, fasts, ys = [], [], []\n",
    "    for (s, f), y in batch:\n",
    "        slows.append(s)\n",
    "        fasts.append(f)\n",
    "        ys.append(y)\n",
    "    slow = torch.stack(slows, dim=0)  # (B,C,T,H,W)\n",
    "    fast = torch.stack(fasts, dim=0)  # (B,C,T,H,W)\n",
    "    y = torch.tensor(ys, dtype=torch.long)\n",
    "    return [slow, fast], y\n",
    "\n",
    "train_csv = os.path.join(cfg.splits_dir, \"train.csv\")\n",
    "val_csv   = os.path.join(cfg.splits_dir, \"val.csv\")\n",
    "test_csv  = os.path.join(cfg.splits_dir, \"test.csv\")\n",
    "\n",
    "train_ds = ClipDataset(train_csv, cfg, train=True)\n",
    "val_ds   = ClipDataset(val_csv,   cfg, train=False)\n",
    "test_ds  = ClipDataset(test_csv,  cfg, train=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=4, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=max(1, cfg.batch_size), shuffle=False,\n",
    "    num_workers=4, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
    ")\n",
    "\n",
    "num_classes = len(cfg.labels)\n",
    "print(\"Classes:\", num_classes, cfg.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A5kc3azJqM-"
   },
   "source": [
    "## **4. TRAINING AND EVALUATION**\n",
    "### This function orchestrates the entire training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVRiC1dVnC3x"
   },
   "source": [
    "#### **Load pre-trained model from hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k156bc16JsZV",
    "outputId": "15411e86-afb8-433a-f563-7bcbcf0fe357"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Model: load hub, replace head\n",
    "# =========================\n",
    "torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
    "\n",
    "# Replace classifier (ResNetBasicHead.proj)\n",
    "in_dim = model.blocks[-1].proj.in_features\n",
    "model.blocks[-1].proj = nn.Sequential(\n",
    "    nn.Dropout(p=0.5), # Add a dropout layer\n",
    "    nn.Linear(in_dim, num_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optional: freeze early blocks for faster convergence at small data sizes\n",
    "for p in model.blocks[:-1].parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf01hSLgnLam"
   },
   "source": [
    "#### **Optional: load weights from checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SpDF2X-m3_v"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = '/content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_dropout0.2/best.pt'\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load the model's state_dict from the checkpoint\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "print(f\"Model weights loaded successfully from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIcUZCSXnRle"
   },
   "source": [
    "#### **Define training components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNULCIKv0RyL",
    "outputId": "154490fb-246b-4cd8-9c29-523b926303a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390020/1097184505.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Optimizer, loss, metrics\n",
    "# =========================\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "acc = MulticlassAccuracy(num_classes=num_classes, average='micro').to(device)\n",
    "f1  = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbCblhb1nXdw"
   },
   "source": [
    "#### **Main training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXDAY8gmBImr",
    "outputId": "e7a3d0ed-dc75-4b69-c0ad-4cb5ecc32d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2390020/2148935506.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
      "Training epoch 0/30: 100%|██████████| 63/63 [01:08<00:00,  1.09s/it]\n",
      "/tmp/ipykernel_2390020/2148935506.py:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[01/30] train_loss=2.3889 acc=20.24% f1=0.111 | val_loss=1.9244 acc=46.77% f1=0.399\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/30: 100%|██████████| 63/63 [01:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[02/30] train_loss=2.1557 acc=31.06% f1=0.201 | val_loss=1.6958 acc=40.32% f1=0.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2/30: 100%|██████████| 63/63 [01:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[03/30] train_loss=1.9953 acc=36.87% f1=0.239 | val_loss=1.6061 acc=51.61% f1=0.504\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.504)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3/30: 100%|██████████| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[04/30] train_loss=1.9188 acc=36.67% f1=0.235 | val_loss=1.5694 acc=48.39% f1=0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4/30: 100%|██████████| 63/63 [01:02<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[05/30] train_loss=1.8649 acc=40.68% f1=0.279 | val_loss=1.4107 acc=53.23% f1=0.508\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.508)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5/30: 100%|██████████| 63/63 [01:03<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[06/30] train_loss=1.8169 acc=41.68% f1=0.306 | val_loss=1.3761 acc=61.29% f1=0.564\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6/30: 100%|██████████| 63/63 [01:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[07/30] train_loss=1.7812 acc=44.09% f1=0.328 | val_loss=1.3861 acc=59.68% f1=0.596\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7/30: 100%|██████████| 63/63 [01:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[08/30] train_loss=1.7473 acc=42.89% f1=0.305 | val_loss=1.3605 acc=54.84% f1=0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8/30: 100%|██████████| 63/63 [01:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[09/30] train_loss=1.7416 acc=45.69% f1=0.324 | val_loss=1.3263 acc=56.45% f1=0.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9/30: 100%|██████████| 63/63 [01:02<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[10/30] train_loss=1.7404 acc=42.08% f1=0.290 | val_loss=1.3020 acc=62.90% f1=0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10/30: 100%|██████████| 63/63 [01:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11/30] train_loss=1.6897 acc=45.69% f1=0.348 | val_loss=1.2278 acc=61.29% f1=0.567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11/30: 100%|██████████| 63/63 [00:58<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[12/30] train_loss=1.6427 acc=49.10% f1=0.368 | val_loss=1.2317 acc=59.68% f1=0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12/30: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[13/30] train_loss=1.6860 acc=45.09% f1=0.341 | val_loss=1.2620 acc=59.68% f1=0.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13/30: 100%|██████████| 63/63 [01:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[14/30] train_loss=1.6408 acc=49.30% f1=0.372 | val_loss=1.2386 acc=69.35% f1=0.643\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14/30: 100%|██████████| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15/30] train_loss=1.5975 acc=50.70% f1=0.379 | val_loss=1.2306 acc=62.90% f1=0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15/30: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[16/30] train_loss=1.6135 acc=50.50% f1=0.414 | val_loss=1.2848 acc=62.90% f1=0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16/30: 100%|██████████| 63/63 [00:59<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[17/30] train_loss=1.6958 acc=45.09% f1=0.333 | val_loss=1.2258 acc=66.13% f1=0.606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 17/30: 100%|██████████| 63/63 [01:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[18/30] train_loss=1.6705 acc=45.29% f1=0.396 | val_loss=1.1673 acc=62.90% f1=0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18/30: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[19/30] train_loss=1.6552 acc=47.90% f1=0.390 | val_loss=1.1798 acc=59.68% f1=0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 19/30: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20/30] train_loss=1.6062 acc=49.10% f1=0.399 | val_loss=1.1502 acc=69.35% f1=0.644\n",
      "  ↳ saved new best to ./train1/best.pt (val_f1=0.644)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 20/30: 100%|██████████| 63/63 [00:58<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21/30] train_loss=1.6175 acc=49.50% f1=0.396 | val_loss=1.1709 acc=66.13% f1=0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 21/30: 100%|██████████| 63/63 [01:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22/30] train_loss=1.5820 acc=47.90% f1=0.387 | val_loss=1.1652 acc=62.90% f1=0.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22/30: 100%|██████████| 63/63 [01:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[23/30] train_loss=1.6004 acc=49.70% f1=0.417 | val_loss=1.1565 acc=64.52% f1=0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 23/30: 100%|██████████| 63/63 [00:58<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[24/30] train_loss=1.6089 acc=48.30% f1=0.393 | val_loss=1.1490 acc=61.29% f1=0.578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 24/30: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[25/30] train_loss=1.5996 acc=49.70% f1=0.419 | val_loss=1.2679 acc=66.13% f1=0.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 25/30: 100%|██████████| 63/63 [01:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[26/30] train_loss=1.6426 acc=46.69% f1=0.358 | val_loss=1.1639 acc=64.52% f1=0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 26/30: 100%|██████████| 63/63 [01:02<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[27/30] train_loss=1.6158 acc=49.30% f1=0.380 | val_loss=1.2070 acc=64.52% f1=0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 27/30: 100%|██████████| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[28/30] train_loss=1.6493 acc=48.50% f1=0.386 | val_loss=1.1410 acc=67.74% f1=0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 28/30: 100%|██████████| 63/63 [01:02<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[29/30] train_loss=1.6068 acc=50.70% f1=0.413 | val_loss=1.1410 acc=64.52% f1=0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 29/30: 100%|██████████| 63/63 [01:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[30/30] train_loss=1.6248 acc=49.70% f1=0.395 | val_loss=1.1796 acc=58.06% f1=0.539\n",
      "Best val F1: 0.6441224813461304\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Train / validate\n",
    "# =========================\n",
    "best_f1 = -1.0\n",
    "os.makedirs(cfg.models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    model.train()\n",
    "    acc.reset(); f1.reset()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    first = True\n",
    "    for (slow_fast, y) in tqdm(train_loader, desc=f'Training epoch {epoch}/{cfg.epochs}'):\n",
    "        if first:\n",
    "            s, f = slow_fast\n",
    "            # print(\"slow:\", tuple(s.shape), \"fast:\", tuple(f.shape))\n",
    "            # Expect slow=(B,3,8,224,224) and fast=(B,3,32,224,224)\n",
    "            first = False\n",
    "\n",
    "        # slow_fast is [slow, fast]\n",
    "        slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            logits = model(slow_fast)     # (B, num_classes)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        acc.update(logits, y)\n",
    "        f1.update(logits, y)\n",
    "\n",
    "    train_loss = total_loss / len(train_ds)\n",
    "    train_acc  = acc.compute().item()\n",
    "    train_f1   = f1.compute().item()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    acc.reset(); f1.reset()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
    "        for (slow_fast, y) in val_loader:\n",
    "            slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            logits = model(slow_fast)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * y.size(0)\n",
    "            acc.update(logits, y)\n",
    "            f1.update(logits, y)\n",
    "\n",
    "    val_loss /= len(val_ds)\n",
    "    val_acc = acc.compute().item()\n",
    "    val_f1  = f1.compute().item()\n",
    "\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    print(f\"\\n[{epoch+1:02d}/{cfg.epochs}] \"\n",
    "          f\"train_loss={train_loss:.4f} acc={train_acc*100:.2f}% f1={train_f1:.3f} | \"\n",
    "          f\"val_loss={val_loss:.4f} acc={val_acc*100:.2f}% f1={val_f1:.3f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\"model\": model.state_dict(), \"labels\": cfg.labels}, cfg.best_model_path)\n",
    "        print(f\"  ↳ saved new best to {cfg.best_model_path} (val_f1={best_f1:.3f})\")\n",
    "\n",
    "print(\"Best val F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWumfwrwgrUE"
   },
   "source": [
    "## **Evaluate on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "A9WccyBwMrMY"
   },
   "outputs": [],
   "source": [
    "class TestManager:\n",
    "    \"\"\"\n",
    "    Manages the evaluation process for a SlowFast model on a test set.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: 'Config', device: str):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.num_classes = len(config.labels)\n",
    "        self.model = self._load_model()\n",
    "        self.test_loader = self._create_dataloader()\n",
    "        self.metrics = self._initialize_metrics()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Loads the pre-trained SlowFast model and the fine-tuned checkpoint.\"\"\"\n",
    "        print(\"Loading model and best checkpoint...\")\n",
    "\n",
    "        # Disable the internal hub check for local loading\n",
    "        torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
    "\n",
    "        model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
    "        in_dim = model.blocks[-1].proj.in_features\n",
    "        model.blocks[-1].proj = nn.Sequential(\n",
    "            nn.Dropout(p=0.2), # Add a dropout layer\n",
    "            nn.Linear(in_dim, num_classes)\n",
    "        )\n",
    "\n",
    "        # Load the state dictionary from the checkpoint file\n",
    "        ckpt = torch.load(self.config.best_model_path, map_location=self.device)\n",
    "        model.load_state_dict(ckpt[\"model\"])\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _create_dataloader(self):\n",
    "        \"\"\"Creates and returns the DataLoader for the test set.\"\"\"\n",
    "        test_ds = ClipDataset(os.path.join(self.config.splits_dir, \"test.csv\"), self.config, train=False)\n",
    "        return DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=max(1, self.config.batch_size),\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            # collate_fn=slowfast_collate,  # Make sure this is imported if needed\n",
    "            persistent_workers=False\n",
    "        )\n",
    "\n",
    "    def _initialize_metrics(self):\n",
    "        \"\"\"Initializes all the evaluation metrics.\"\"\"\n",
    "        return {\n",
    "            'top1': MulticlassAccuracy(num_classes=self.num_classes, average=\"micro\").to(self.device),\n",
    "            'top3': MulticlassAccuracy(num_classes=self.num_classes, top_k=3).to(self.device),\n",
    "            'f1_macro': MulticlassF1Score(num_classes=self.num_classes, average=\"macro\").to(self.device),\n",
    "            'f1_perclass': MulticlassF1Score(num_classes=self.num_classes, average=None).to(self.device),\n",
    "            'cm': MulticlassConfusionMatrix(num_classes=self.num_classes).to(self.device)\n",
    "        }\n",
    "\n",
    "    def run_inference(self):\n",
    "        \"\"\"Runs the inference loop and computes all metrics and predictions.\"\"\"\n",
    "        print(\"Starting inference on the test set...\")\n",
    "        test_loss = 0.0\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(self.device, enabled=(self.device == \"cuda\")):\n",
    "            for batch_idx, (slow_fast, y) in enumerate(self.test_loader):\n",
    "                # Ensure input tensors are lists\n",
    "                if not isinstance(slow_fast, list):\n",
    "                    slow_fast = [slow_fast]\n",
    "\n",
    "                slow_fast = [t.to(self.device, non_blocking=True) for t in slow_fast]\n",
    "                y = y.to(self.device, non_blocking=True)\n",
    "\n",
    "                logits = self.model(slow_fast)\n",
    "                loss = criterion(logits, y)\n",
    "                test_loss += loss.item() * y.size(0)\n",
    "\n",
    "                # Update metrics\n",
    "                for metric in self.metrics.values():\n",
    "                    metric.update(logits, y)\n",
    "\n",
    "                # Collect per-sample predictions for later saving\n",
    "                probs = self.softmax(logits)\n",
    "                conf, pred = probs.max(dim=1)\n",
    "                topk_conf, topk_idx = probs.topk(3, dim=1)\n",
    "\n",
    "                start_idx = batch_idx * self.test_loader.batch_size\n",
    "\n",
    "                for i in range(y.size(0)):\n",
    "                    idx = start_idx + i\n",
    "                    path = self.test_loader.dataset.items[idx][0]\n",
    "                    row = {\n",
    "                        \"path\": path,\n",
    "                        \"file\": os.path.basename(path),\n",
    "                        \"true_idx\": int(y[i]),\n",
    "                        \"true_label\": self.config.labels[int(y[i])],\n",
    "                        \"pred_idx\": int(pred[i]),\n",
    "                        \"pred_label\": self.config.labels[int(pred[i])],\n",
    "                        \"pred_prob\": float(conf[i]),\n",
    "                        \"top1_label\": self.config.labels[int(topk_idx[i,0])],\n",
    "                        \"top1_prob\":  float(topk_conf[i,0]),\n",
    "                        \"top2_label\": self.config.labels[int(topk_idx[i,1])],\n",
    "                        \"top2_prob\":  float(topk_conf[i,1]),\n",
    "                        \"top3_label\": self.config.labels[int(topk_idx[i,2])],\n",
    "                        \"top3_prob\":  float(topk_conf[i,2]),\n",
    "                    }\n",
    "                    all_predictions.append(row)\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        return test_loss, all_predictions\n",
    "\n",
    "    def compute_and_print_results(self, test_loss):\n",
    "        \"\"\"Computes and prints the final metrics.\"\"\"\n",
    "        acc1 = self.metrics['top1'].compute().item()\n",
    "        acc3 = self.metrics['top3'].compute().item()\n",
    "        f1M = self.metrics['f1_macro'].compute().item()\n",
    "        percls = self.metrics['f1_perclass'].compute().detach().cpu().tolist()\n",
    "        confmat = self.metrics['cm'].compute().detach().cpu().numpy()\n",
    "\n",
    "        print(f\"\\nTEST: loss={test_loss:.4f} | acc@1={acc1*100:.2f}% | acc@3={acc3*100:.2f}% | macro-F1={f1M:.3f}\")\n",
    "        print(\"\\nPer-class F1:\")\n",
    "        for lab, s in sorted(zip(self.config.labels, percls), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {lab:15s} {s:.3f}\")\n",
    "\n",
    "        print(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\n",
    "        print(confmat)\n",
    "\n",
    "    def save_predictions(self, predictions: list, print_n: int=10):\n",
    "        \"\"\"Saves the list of predictions to a CSV file.\"\"\"\n",
    "        df = pd.DataFrame(predictions)\n",
    "        save_path = os.path.join(self.config.models_dir, \"test_predictions.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"\\nSaved per-sample predictions to: {save_path}\")\n",
    "        print(\"\\nQuick peek at the predictions:\")\n",
    "        with pd.option_context('display.max_rows', None):\n",
    "            print(df.head(print_n)[[\"file\", \"true_label\", \"pred_label\", \"pred_prob\", \"top2_label\", \"top2_prob\", \"top3_label\", \"top3_prob\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iO-6i20oihsn",
    "outputId": "190bdc71-3e5c-44f7-9de0-e0481cb2edb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and best checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference on the test set...\n",
      "\n",
      "TEST: loss=1.2628 | acc@1=52.38% | acc@3=72.57% | macro-F1=0.391\n",
      "\n",
      "Per-class F1:\n",
      "  serve           0.933\n",
      "  jump_smash      0.700\n",
      "  lift            0.522\n",
      "  clear           0.500\n",
      "  drive           0.500\n",
      "  straight_net    0.435\n",
      "  cross_net       0.400\n",
      "  drop            0.308\n",
      "  smash           0.000\n",
      "  block           0.000\n",
      "  push            0.000\n",
      "  tap             0.000\n",
      "  average_joe     0.000\n",
      "\n",
      "Confusion Matrix (rows=true, cols=predicted):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 7 0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 2 3 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 6 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 2 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 4 0 5 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 7 0 0 0]\n",
      " [0 0 1 0 0 1 1 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "Saved per-sample predictions to: ./train1/test_predictions.csv\n",
      "\n",
      "Quick peek at the predictions:\n",
      "                     file    true_label    pred_label  pred_prob  \\\n",
      "0          0410ddf3ff.mp4  straight_net          lift   0.553756   \n",
      "1          7daf4997a6.mp4     cross_net     cross_net   0.435213   \n",
      "2          a0959950b4.mp4    jump_smash          drop   0.337771   \n",
      "3          2cce91c54a.mp4         clear         clear   0.242039   \n",
      "4          5a24c51acf.mp4         serve         serve   0.804976   \n",
      "5          41c1d3fec0.mp4         serve         serve   0.438440   \n",
      "6     20250930_094405.mp4         serve         serve   0.990772   \n",
      "7   20250930_102527_1.mp4         clear         clear   0.394290   \n",
      "8   20250930_123632_1.mp4         clear    jump_smash   0.317438   \n",
      "9          201801ea69.mp4          lift  straight_net   0.368183   \n",
      "10         d1c54ac65a.mp4         serve         serve   0.903304   \n",
      "11         b1df47d6ff.mp4    jump_smash    jump_smash   0.583223   \n",
      "12         d1760b21d2.mp4         serve         serve   0.711340   \n",
      "13         b7189f5efd.mp4          lift          lift   0.474922   \n",
      "14         a692fd3e65.mp4  straight_net  straight_net   0.539594   \n",
      "15         cc51390f19.mp4          lift          lift   0.567223   \n",
      "16         d8033be14a.mp4    jump_smash          drop   0.291065   \n",
      "17         1a13ef9489.mp4          push          lift   0.395969   \n",
      "18         9b1214d0f8.mp4  straight_net          lift   0.412839   \n",
      "19         a3221cc2b8.mp4          push  straight_net   0.266416   \n",
      "20         0d1d71b9d1.mp4          lift          lift   0.501321   \n",
      "21         ca08a46544.mp4          drop    jump_smash   0.300868   \n",
      "22         84815f8632.mp4   average_joe          drop   0.245939   \n",
      "23         8a34c2ac97.mp4         clear          lift   0.166139   \n",
      "24    20250930_095837.mp4    jump_smash    jump_smash   0.687256   \n",
      "25    20250930_100124.mp4    jump_smash    jump_smash   0.663536   \n",
      "26         1e6a478b28.mp4  straight_net     cross_net   0.294687   \n",
      "27    20250930_095820.mp4    jump_smash    jump_smash   0.895460   \n",
      "28         cdf2145af3.mp4    jump_smash    jump_smash   0.652328   \n",
      "29         ad28b58dcf.mp4         serve         serve   0.537253   \n",
      "30         f23a441d0b.mp4    jump_smash          drop   0.276700   \n",
      "31         f555b05ef5.mp4         serve         serve   0.941890   \n",
      "32  20250930_110302_2.mp4         clear         clear   0.748981   \n",
      "33         7bce8c6a19.mp4          lift  straight_net   0.404235   \n",
      "34         616834abff.mp4  straight_net          drop   0.462949   \n",
      "35         f54f8436b9.mp4          lift          lift   0.642798   \n",
      "36         9235462e2d.mp4         clear          drop   0.549866   \n",
      "37         87daf40623.mp4          lift  straight_net   0.424670   \n",
      "38         2afd87bd7e.mp4  straight_net  straight_net   0.323893   \n",
      "39         ebdb397336.mp4          drop          drop   0.557541   \n",
      "40         ab8ce3888d.mp4         drive     cross_net   0.517833   \n",
      "41         774669c1cd.mp4    jump_smash    jump_smash   0.224546   \n",
      "42         3a535dd4d2.mp4          lift  straight_net   0.631037   \n",
      "43         aa81076d2b.mp4          push         block   0.426938   \n",
      "44         870bd08878.mp4  straight_net  straight_net   0.330227   \n",
      "45         abac824ea6.mp4         drive         clear   0.203375   \n",
      "46         ca129faa7b.mp4     cross_net          lift   0.482541   \n",
      "47         3242df5434.mp4    jump_smash    jump_smash   0.429085   \n",
      "48         2f470fe80a.mp4  straight_net  straight_net   0.619563   \n",
      "49         3a842cf3dd.mp4  straight_net          lift   0.265081   \n",
      "50         6c66a35c2e.mp4          drop    jump_smash   0.429156   \n",
      "51         b2726296f6.mp4         drive         clear   0.360417   \n",
      "52    20250930_101734.mp4         clear          drop   0.280749   \n",
      "53         c85c6c7775.mp4  straight_net  straight_net   0.343901   \n",
      "54         8ceb3fb383.mp4          lift          lift   0.442915   \n",
      "55  20250930_123841_3.mp4         drive         drive   0.485493   \n",
      "56         801a4b0611.mp4  straight_net          lift   0.491407   \n",
      "57         b210461fcd.mp4          lift          lift   0.771807   \n",
      "58         36279c04c2.mp4          drop          drop   0.578021   \n",
      "59         6f8635d805.mp4          push         drive   0.289407   \n",
      "60         dd37889e92.mp4          push  straight_net   0.192884   \n",
      "61  20250930_123841_1.mp4         drive         drive   0.522285   \n",
      "62    20250930_093716.mp4         serve  straight_net   0.537712   \n",
      "\n",
      "      top2_label  top2_prob    top3_label  top3_prob  \n",
      "0   straight_net   0.146159         drive   0.115621  \n",
      "1   straight_net   0.229790          lift   0.128398  \n",
      "2          clear   0.321047    jump_smash   0.251010  \n",
      "3           drop   0.234592          lift   0.134190  \n",
      "4           push   0.039999          lift   0.030430  \n",
      "5   straight_net   0.201518     cross_net   0.176455  \n",
      "6   straight_net   0.004706          push   0.003539  \n",
      "7          drive   0.179815          lift   0.119783  \n",
      "8          clear   0.253083          drop   0.213952  \n",
      "9           lift   0.345876         clear   0.079006  \n",
      "10     cross_net   0.036778          lift   0.022394  \n",
      "11          drop   0.253798         clear   0.074584  \n",
      "12     cross_net   0.114770  straight_net   0.079499  \n",
      "13  straight_net   0.253712     cross_net   0.077076  \n",
      "14     cross_net   0.254389          lift   0.134054  \n",
      "15  straight_net   0.220830     cross_net   0.091339  \n",
      "16    jump_smash   0.285435         clear   0.183214  \n",
      "17     cross_net   0.316311  straight_net   0.123628  \n",
      "18  straight_net   0.326583     cross_net   0.061243  \n",
      "19          lift   0.245434          drop   0.096489  \n",
      "20  straight_net   0.204540     cross_net   0.150818  \n",
      "21          drop   0.273943  straight_net   0.227107  \n",
      "22    jump_smash   0.207101         clear   0.194554  \n",
      "23         clear   0.161658          drop   0.159152  \n",
      "24  straight_net   0.118267         clear   0.104370  \n",
      "25          drop   0.181047          lift   0.044194  \n",
      "26  straight_net   0.280096          lift   0.121413  \n",
      "27  straight_net   0.038923         clear   0.027926  \n",
      "28          drop   0.137271         clear   0.086575  \n",
      "29          lift   0.162896     cross_net   0.111956  \n",
      "30         clear   0.243235    jump_smash   0.221468  \n",
      "31  straight_net   0.019170          drop   0.007655  \n",
      "32          drop   0.167120    jump_smash   0.030198  \n",
      "33          lift   0.312368         serve   0.111378  \n",
      "34  straight_net   0.261728          lift   0.186319  \n",
      "35  straight_net   0.170991     cross_net   0.046929  \n",
      "36         clear   0.368442  straight_net   0.021699  \n",
      "37          lift   0.373309         clear   0.038210  \n",
      "38         block   0.159714          lift   0.156014  \n",
      "39         clear   0.276003    jump_smash   0.040231  \n",
      "40          push   0.263452          lift   0.107070  \n",
      "41  straight_net   0.163641          drop   0.161734  \n",
      "42          lift   0.258977     cross_net   0.020364  \n",
      "43  straight_net   0.117634     cross_net   0.113127  \n",
      "44          lift   0.251223         clear   0.073972  \n",
      "45    jump_smash   0.183018          lift   0.178778  \n",
      "46     cross_net   0.112837  straight_net   0.094279  \n",
      "47         clear   0.301310          drop   0.167051  \n",
      "48          lift   0.254268     cross_net   0.041834  \n",
      "49  straight_net   0.258940     cross_net   0.256925  \n",
      "50          drop   0.372857         clear   0.065174  \n",
      "51    jump_smash   0.152015          drop   0.124557  \n",
      "52          lift   0.233660  straight_net   0.165690  \n",
      "53         serve   0.215208          lift   0.183001  \n",
      "54     cross_net   0.164217  straight_net   0.119209  \n",
      "55          push   0.165181         clear   0.097866  \n",
      "56  straight_net   0.341720          push   0.041133  \n",
      "57  straight_net   0.090220     cross_net   0.055150  \n",
      "58         clear   0.238148    jump_smash   0.079149  \n",
      "59          lift   0.172813  straight_net   0.144956  \n",
      "60          lift   0.170886         smash   0.111633  \n",
      "61          push   0.179092          lift   0.123570  \n",
      "62         serve   0.437156          drop   0.008326  \n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "test_manager = TestManager(cfg, device)\n",
    "test_loss, all_predictions = test_manager.run_inference()\n",
    "test_manager.compute_and_print_results(test_loss)\n",
    "test_manager.save_predictions(all_predictions, print_n=len(all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU9T0fc3nRx8"
   },
   "source": [
    "## **end-to-end match inference & overlay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ug-sBuM4nkRl",
    "outputId": "ae3d3ee4-bf4d-417c-d341-dbe9e3f7eefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install ultralytics opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaoCKiwUgy97"
   },
   "outputs": [],
   "source": [
    "import os, cv2, numpy as np, torch\n",
    "from collections import deque, defaultdict\n",
    "from ultralytics import YOLO\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mx6acAn2obMz"
   },
   "outputs": [],
   "source": [
    "def load_slowfast_classifier(cfg, ckpt_path):\n",
    "    torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
    "    in_dim = model.blocks[-1].proj.in_features\n",
    "    model.blocks[-1].proj = torch.nn.Linear(in_dim, len(cfg.labels))\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.eval().to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iasMXtJYsCZj"
   },
   "outputs": [],
   "source": [
    "def resize_pad_square(img_rgb: np.ndarray, side: int = 224) -> np.ndarray:\n",
    "    \"\"\"Keep aspect ratio; resize the longer side to `side`, then pad to (side, side).\"\"\"\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return np.zeros((side, side, 3), dtype=img_rgb.dtype)\n",
    "    scale = side / max(h, w)\n",
    "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "    resized = cv2.resize(img_rgb, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
    "    top  = (side - nh) // 2\n",
    "    bottom = side - nh - top\n",
    "    left = (side - nw) // 2\n",
    "    right = side - nw - left\n",
    "    out = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128,128,128))\n",
    "    return out\n",
    "\n",
    "def expand_box(x1, y1, x2, y2, scale: float, W: int, H: int):\n",
    "    \"\"\"Optionally enlarge the bbox to keep some context (e.g., racket).\"\"\"\n",
    "    cx, cy = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
    "    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale\n",
    "    nx1, ny1 = int(max(0, cx - bw / 2)), int(max(0, cy - bh / 2))\n",
    "    nx2, ny2 = int(min(W - 1, cx + bw / 2)), int(min(H - 1, cy + bh / 2))\n",
    "    return nx1, ny1, nx2, ny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsnwdZrHnVua"
   },
   "outputs": [],
   "source": [
    "class SlowFastPredictor:\n",
    "    def __init__(self, cfg, model):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1).to(device)\n",
    "        self.std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1).to(device)\n",
    "\n",
    "    def _prep(self, frames_rgb_list):\n",
    "        \"\"\"\n",
    "        frames_rgb_list: list of 32 frames, each HxWx3 in RGB\n",
    "        Returns: [slow, fast] tensors shaped (1,C,T,H,W)\n",
    "        \"\"\"\n",
    "        # Stack to (T,H,W,3) -> (T,C,H,W)\n",
    "        x = torch.from_numpy(np.stack(frames_rgb_list)).permute(0,3,1,2).float() / 255.0  # (T,C,H,W)\n",
    "        # Resize treating T as batch\n",
    "        x = F.interpolate(x, size=self.cfg.side, mode=\"bilinear\", align_corners=False)    # (T,C,224,224)\n",
    "        # Normalize\n",
    "        mean = self.mean.to(device=x.device, dtype=x.dtype)\n",
    "        std  = self.std.to(device=x.device, dtype=x.dtype)\n",
    "        x = (x - mean) / std                                                   # (T,C,224,224)\n",
    "        # (C,T,H,W)\n",
    "        x = x.permute(1,0,2,3)\n",
    "        fast = x.unsqueeze(0).to(device)             # (1,C,32,224,224)\n",
    "        slow = x[:, ::self.cfg.alpha, :, :].unsqueeze(0).to(device)  # stride-4 -> (1,C,8,224,224)\n",
    "        return [slow, fast]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_probs(self, frames_rgb_list):\n",
    "        assert len(frames_rgb_list) == self.cfg.fast_t  # 32\n",
    "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
    "            inp = self._prep(frames_rgb_list)\n",
    "            logits = self.model(inp)                  # (1, num_classes)\n",
    "            probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
    "        return probs  # (C,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a36aCc48nXfb"
   },
   "outputs": [],
   "source": [
    "def annotate_match_video(\n",
    "    cfg,\n",
    "    video_path,\n",
    "    out_path,\n",
    "    yolo_weights=\"yolo11n.pt\", # change to your custom weights if you have them\n",
    "    person_class=0,            # COCO 'person'\n",
    "    det_conf=0.5,\n",
    "    iou=0.5,\n",
    "    pred_thr=0.60,             # minimum prob to show label\n",
    "    cooldown=12                # frames to cool after showing a shot to reduce spam\n",
    "):\n",
    "    # Get video props for the writer\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = max(1.0, cap.get(cv2.CAP_PROP_FPS))\n",
    "    W   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(out_path, fourcc, fps, (W, H))\n",
    "\n",
    "    # Load detector+tracker\n",
    "    yolo = YOLO(yolo_weights)\n",
    "\n",
    "    # Load classifier\n",
    "    clf_model = load_slowfast_classifier(cfg, cfg.best_model_path)\n",
    "    clf = SlowFastPredictor(cfg, clf_model)\n",
    "\n",
    "    # Per-track state\n",
    "    buffers = defaultdict(lambda: deque(maxlen=cfg.fast_t))            # 32-frame RGB crops per track\n",
    "    last_shown_frame = defaultdict(lambda: -99999)                     # cooldown control\n",
    "    hist = defaultdict(lambda: deque(maxlen=5))                        # small temporal smoothing buffer\n",
    "\n",
    "    frame_idx = 0\n",
    "    for res in yolo.track(source=video_path, stream=True, persist=True,\n",
    "                          classes=[person_class], conf=det_conf, iou=iou, verbose=False):\n",
    "        frame_bgr = res.orig_img  # BGR\n",
    "        h, w = frame_bgr.shape[:2]\n",
    "\n",
    "        # If no boxes/ids in this frame, just write it\n",
    "        if res.boxes is None or res.boxes.id is None:\n",
    "            writer.write(frame_bgr)\n",
    "            frame_idx += 1\n",
    "            continue\n",
    "\n",
    "        ids = res.boxes.id.int().cpu().numpy()\n",
    "        xyxy = res.boxes.xyxy.int().cpu().numpy()  # (N,4)\n",
    "\n",
    "        to_draw = []  # (x1,y1,x2,y2,label,prob,tid)\n",
    "\n",
    "        for j, tid in enumerate(ids):\n",
    "            x1, y1, x2, y2 = xyxy[j]\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w-1, x2), min(h-1, y2)\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            # NEW: enlarge a bit for context (optional, try 1.2–1.4)\n",
    "            x1, y1, x2, y2 = expand_box(x1, y1, x2, y2, scale=1.25, W=w, H=h)\n",
    "\n",
    "            # Crop -> RGB -> letterbox to fixed square\n",
    "            crop = frame_bgr[y1:y2, x1:x2, :]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            crop_rgb = resize_pad_square(crop_rgb, side=cfg.side)  # now every frame is 224x224\n",
    "\n",
    "            buffers[tid].append(crop_rgb)\n",
    "\n",
    "            label_to_show = None\n",
    "            prob_to_show  = 0.0\n",
    "\n",
    "            # Classify when we have a full 32-frame clip\n",
    "            if len(buffers[tid]) == cfg.fast_t:\n",
    "                probs = clf.predict_probs(list(buffers[tid]))  # (C,)\n",
    "                ci = int(probs.argmax())\n",
    "                pi = float(probs[ci])\n",
    "                hist[tid].append((ci, pi))\n",
    "\n",
    "                # Small smoothing: require at least 2 of the last 3 agreeing + prob >= thr\n",
    "                if len(hist[tid]) >= 3:\n",
    "                    last3 = list(hist[tid])[-3:]\n",
    "                else:\n",
    "                    last3 = list(hist[tid])\n",
    "\n",
    "                # Choose the label with the highest mean prob among last3\n",
    "                if last3:\n",
    "                    classes = [c for c, p in last3 if cfg.labels[c] != \"average_joe\" and p >= pred_thr]\n",
    "                    if classes:\n",
    "                        # pick the most common; break ties by highest avg prob\n",
    "                        uniq = set(classes)\n",
    "                        best_c, best_score = None, -1.0\n",
    "                        for u in uniq:\n",
    "                            avgp = np.mean([p for (c, p) in last3 if c == u])\n",
    "                            score = (classes.count(u), avgp)  # (count, avgp)\n",
    "                            if score > (classes.count(best_c) if best_c is not None else -1, best_score):\n",
    "                                best_c, best_score = u, avgp\n",
    "                        if best_c is not None and (frame_idx - last_shown_frame[tid] >= cooldown):\n",
    "                            label_to_show = cfg.labels[best_c]\n",
    "                            prob_to_show = float(best_score)\n",
    "                            last_shown_frame[tid] = frame_idx\n",
    "\n",
    "            # Queue drawing if we have a confident non-background label\n",
    "            if label_to_show is not None:\n",
    "                to_draw.append((x1, y1, x2, y2, label_to_show, prob_to_show, int(tid)))\n",
    "\n",
    "        # ---- Draw all overlays on this frame ----\n",
    "        for (x1, y1, x2, y2, lab, p, tid) in to_draw:\n",
    "            color = (0, 220, 0)\n",
    "            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "            txt = f\"#{tid} {lab} {p*100:.1f}%\"\n",
    "            cv2.putText(frame_bgr, txt, (x1, max(20, y1-10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        writer.write(frame_bgr)\n",
    "        frame_idx += 1\n",
    "\n",
    "    writer.release()\n",
    "    print(f\"Saved annotated video to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uex745aPnaJK",
    "outputId": "41d184dd-8bf8-4e09-8996-6a67acb9bbc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved annotated video to: /content/match_annotated.mp4\n"
     ]
    }
   ],
   "source": [
    "in_video  = \"/content/drive/MyDrive/FIT3163,3164/SlowFast/01_raw/lcw_ld_2016_short/1/master.mp4\"\n",
    "out_video = \"/content/match_annotated.mp4\"\n",
    "yolo_weights = \"/content/drive/MyDrive/FIT3163,3164/YOLO/my_yolov8_1.pt\"\n",
    "\n",
    "annotate_match_video(cfg, in_video, out_video,\n",
    "                     yolo_weights=yolo_weights,  # swap if you have a better person/badminton model\n",
    "                     det_conf=0.35, iou=0.5,\n",
    "                     pred_thr=0.60, cooldown=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utRepM9qoTb7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
