{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall opencv-python opencv-python-headless opencv-contrib-python -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkVdmrjGLeUB",
        "outputId": "2cb04b5e-2be5-4888-ab86-1efa981b4810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python 4.12.0.88\n",
            "Uninstalling opencv-python-4.12.0.88:\n",
            "  Successfully uninstalled opencv-python-4.12.0.88\n",
            "Found existing installation: opencv-python-headless 4.12.0.88\n",
            "Uninstalling opencv-python-headless-4.12.0.88:\n",
            "  Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "  Successfully uninstalled opencv-contrib-python-4.12.0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install fvcore opencv-contrib-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMeX5ZH1JY94",
        "outputId": "d0f15edc-478f-4042-ee7d-e6e1f9ad1978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.2/73.2 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMjRLBxKHoQt"
      },
      "outputs": [],
      "source": [
        "import os, csv, json, cv2, math, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hvag1O1gHtei",
        "outputId": "f28a70e7-08c3-47c8-9bed-b75e1bc4f537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utilities**"
      ],
      "metadata": {
        "id": "UR3mu6RiHw4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def _clamp(v, lo, hi):\n",
        "    return max(lo, min(hi, v))\n",
        "\n",
        "def _expand_bbox(b, margin, W, H):\n",
        "    \"\"\"Expand bbox by `margin` while staying in-frame.\"\"\"\n",
        "    x1,y1,x2,y2 = map(float, b)\n",
        "    w, h = x2-x1, y2-y1\n",
        "    cx, cy = (x1+x2)/2., (y1+y2)/2.\n",
        "    w2, h2 = w*margin/2., h*margin/2.\n",
        "    nx1, ny1 = _clamp(int(round(cx - w2)), 0, W-1), _clamp(int(round(cy - h2)), 0, H-1)\n",
        "    nx2, ny2 = _clamp(int(round(cx + w2)), 0, W-1), _clamp(int(round(cy + h2)), 0, H-1)\n",
        "    if nx2 <= nx1 or ny2 <= ny1: return 0,0,W-1,H-1\n",
        "    return nx1, ny1, nx2, ny2\n",
        "\n",
        "def _center_resize_crop(img, side_size, crop_size):\n",
        "    \"\"\"Short-side resize to `side_size`, then center crop `crop_size`.\"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    if h < w:\n",
        "        new_h, new_w = side_size, int(round(w*side_size/h))\n",
        "    else:\n",
        "        new_w, new_h = side_size, int(round(h*side_size/w))\n",
        "    rs = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
        "    y0 = _clamp((new_h - crop_size)//2, 0, max(0, new_h - crop_size))\n",
        "    x0 = _clamp((new_w - crop_size)//2, 0, max(0, new_w - crop_size))\n",
        "    return rs[y0:y0+crop_size, x0:x0+crop_size]\n",
        "\n",
        "def _linspace_idx(a, b, n):\n",
        "    \"\"\"n indices uniformly from [a..b] inclusive, as ints (repeats if n>L).\"\"\"\n",
        "    if n <= 1: return [int(round((a+b)/2))]\n",
        "    return list(np.round(np.linspace(a, b, n)).astype(int))\n",
        "\n",
        "def _color_for_id(tid: int):\n",
        "    # stable-ish distinct color per track id\n",
        "    return (37*tid % 256, 17*tid % 256, 93*tid % 256)"
      ],
      "metadata": {
        "id": "V77RGAUwHt3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load tracking CSV**"
      ],
      "metadata": {
        "id": "EGMUDq0oH329"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tracks_csv(csv_path: str) -> Dict[int, List[Tuple[int, Tuple[int,int,int,int]]]]:\n",
        "    \"\"\"\n",
        "    Returns {track_id: [(frame_idx, (x1,y1,x2,y2)), ...]} sorted by frame.\n",
        "    \"\"\"\n",
        "    print(f\"üé¨ Loading track data from: {csv_path}\")\n",
        "    tracks = defaultdict(list)\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        # Assuming the CSV isn't too large for an in-memory load\n",
        "        data = list(reader)\n",
        "        for r in tqdm(data, desc=\"   -> Reading CSV lines\"):\n",
        "            t   = int(r[\"frame\"])\n",
        "            tid = int(r[\"id\"])\n",
        "            x1,y1,x2,y2 = int(r[\"x1\"]), int(r[\"y1\"]), int(r[\"x2\"]), int(r[\"y2\"])\n",
        "            tracks[tid].append((t, (x1,y1,x2,y2)))\n",
        "\n",
        "    print(f\"   -> Found {len(tracks)} unique tracks. Sorting by frame...\")\n",
        "    for tid in tracks:\n",
        "        tracks[tid].sort(key=lambda x: x[0])\n",
        "    return tracks\n",
        "\n",
        "def interpolate_track(timeline: List[Tuple[int, Tuple[int,int,int,int]]]):\n",
        "    \"\"\"\n",
        "    Linear interpolation of bboxes across missing frames.\n",
        "    Returns (t0, t1, dense_boxes: List[(x1,y1,x2,y2)]) covering every frame in [t0..t1].\n",
        "    \"\"\"\n",
        "    ts  = np.array([t for t,_ in timeline], dtype=np.int32)\n",
        "    bxs = np.array([b for _,b in timeline], dtype=np.float32)  # Nx4\n",
        "    t0, t1 = int(ts[0]), int(ts[-1])\n",
        "    T = np.arange(t0, t1+1, dtype=np.int32)\n",
        "    out = [np.interp(T, ts, bxs[:,k]) for k in range(4)]\n",
        "    dense = np.stack(out, axis=1).astype(np.int32)  # Lx4\n",
        "    return t0, t1, [tuple(map(int, b)) for b in dense]"
      ],
      "metadata": {
        "id": "3r26HZabH1wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dense TV-L1 flow**"
      ],
      "metadata": {
        "id": "07KYOdWlH8Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TVL1FlowDownscaled:\n",
        "    \"\"\"\n",
        "    Downscale to width<=640 for speed. For each decoded frame, returns flow magnitude map.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_width=640):\n",
        "        self.tvl1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
        "        self.target_width = target_width\n",
        "        self.prev_gray = None\n",
        "        self.scale = 1.0\n",
        "        self.small_shape = None\n",
        "\n",
        "    def _prep(self, frame_bgr):\n",
        "        H, W = frame_bgr.shape[:2]\n",
        "        scale = self.target_width / float(W) if W > self.target_width else 1.0\n",
        "        small = cv2.resize(frame_bgr, (int(W*scale), int(H*scale)), interpolation=cv2.INTER_AREA) if scale < 1.0 else frame_bgr\n",
        "        gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
        "        return gray, scale, small.shape[:2]\n",
        "\n",
        "    def mag(self, frame_bgr):\n",
        "        gray, scale, shp = self._prep(frame_bgr)\n",
        "        if self.prev_gray is None:\n",
        "            self.prev_gray = gray\n",
        "            self.scale = scale\n",
        "            self.small_shape = shp\n",
        "            return np.zeros(shp, dtype=np.float32)\n",
        "        flow = self.tvl1.calc(self.prev_gray, gray, None)\n",
        "        self.prev_gray = gray\n",
        "        self.scale = scale\n",
        "        self.small_shape = shp\n",
        "        return np.sqrt(flow[...,0]**2 + flow[...,1]**2)"
      ],
      "metadata": {
        "id": "rUJ911AmH97c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compute actionness**"
      ],
      "metadata": {
        "id": "Pe98QeqqIEVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ActionnessCfg:\n",
        "    \"\"\"\n",
        "    Flow-based actionness. Keep simple and robust.\n",
        "    \"\"\"\n",
        "    ma_len: int = 5           # moving-average window on z-scored series\n",
        "    high_k: float = 1.2       # start when z >= mu + high_k*std\n",
        "    low_k: float  = 0.4       # stop when z <  mu + low_k*std\n",
        "    min_dur_s: float = 0.26   # keep segments >= ~0.26s (‚âà 8 frames @30fps)\n",
        "    max_dur_s: float = 1.20   # clamp segments to <= ~1.2s\n",
        "    bbox_margin: float = 1.25 # enlarge crop a bit so racket/arm stays in\n",
        "\n",
        "def moving_average(x: np.ndarray, k: int) -> np.ndarray:\n",
        "    if k <= 1: return x\n",
        "    w = np.ones(k, dtype=np.float32)/k\n",
        "    return np.convolve(x, w, mode=\"same\")\n",
        "\n",
        "def hysteresis_segments(z_sm: np.ndarray, high_k: float, low_k: float) -> List[Tuple[int,int]]:\n",
        "    mu, sd = float(np.mean(z_sm)), float(np.std(z_sm) + 1e-6)\n",
        "    high, low = mu + high_k*sd, mu + low_k*sd\n",
        "    segs, on, s0 = [], False, None\n",
        "    for i,v in enumerate(z_sm):\n",
        "        if not on and v >= high:\n",
        "            on, s0 = True, i\n",
        "        elif on and v < low:\n",
        "            on = False\n",
        "            segs.append((s0, i))\n",
        "            s0 = None\n",
        "    if on: segs.append((s0, len(z_sm)-1))\n",
        "    return segs\n",
        "\n",
        "def compute_actionness_segments(video_path: str,\n",
        "                                timeline: List[Tuple[int, Tuple[int,int,int,int]]],\n",
        "                                fps: float,\n",
        "                                cfg: ActionnessCfg) -> Tuple[np.ndarray, List[Tuple[int,int]]]:\n",
        "    \"\"\"\n",
        "    Build per-frame actionness inside the player's bbox:\n",
        "      - TV-L1 flow magnitude (95th percentile) + frame-diff (75th percentile)\n",
        "      - Per-track z-score, MA smooth, hysteresis\n",
        "      - Clamp segment durations (min_dur_s .. max_dur_s)\n",
        "\n",
        "    Returns:\n",
        "      z_sm  : smoothed z-score array aligned to [t0..t1]\n",
        "      segsA : list of (t0_abs, t1_abs) frame indices\n",
        "    \"\"\"\n",
        "    # interpolate bboxes densely for this track\n",
        "    t0, t1, dense_boxes = interpolate_track(timeline)    # inclusive indices\n",
        "    L = t1 - t0 + 1\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): raise RuntimeError(f\"Cannot open: {video_path}\")\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # We'll decode frames from t0-1 .. t1 to initialize flow\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, t0-1))\n",
        "    flow = TVL1FlowDownscaled(target_width=640)\n",
        "    mags, diffs = [], []\n",
        "    prev_small_gray = None\n",
        "\n",
        "    for t in tqdm(range(max(0,t0-1), t1+1), desc=f\"      -> Calculating flow ({L} frames)\"):\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "        mag = flow.mag(frame)               # downscaled mag\n",
        "        gray_small = cv2.cvtColor(cv2.resize(frame, (mag.shape[1], mag.shape[0]), interpolation=cv2.INTER_AREA),\n",
        "                                  cv2.COLOR_BGR2GRAY)\n",
        "        if prev_small_gray is None:\n",
        "            diff = np.zeros_like(gray_small, dtype=np.float32)\n",
        "        else:\n",
        "            diff = cv2.absdiff(gray_small, prev_small_gray).astype(np.float32)\n",
        "        prev_small_gray = gray_small\n",
        "\n",
        "        if t >= t0:                         # align to dense series\n",
        "            mags.append(mag); diffs.append(diff)\n",
        "\n",
        "    cap.release()\n",
        "    if len(mags) != L:                      # robust fallback\n",
        "        mags = [np.zeros(flow.small_shape, np.float32) for _ in range(L)]\n",
        "        diffs = [np.zeros(flow.small_shape, np.float32) for _ in range(L)]\n",
        "\n",
        "    # reduce to scalar per frame inside expanded bbox\n",
        "    z_raw = []\n",
        "    scale = flow.scale\n",
        "    for k in range(L):\n",
        "        b = dense_boxes[k]\n",
        "        x1,y1,x2,y2 = _expand_bbox(b, cfg.bbox_margin, W, H)\n",
        "        h_s, w_s = mags[k].shape\n",
        "        xs1,xs2 = int(x1*scale), int(x2*scale)\n",
        "        ys1,ys2 = int(y1*scale), int(y2*scale)\n",
        "        xs1 = _clamp(xs1, 0, w_s-1); xs2 = _clamp(xs2, 0, w_s-1)\n",
        "        ys1 = _clamp(ys1, 0, h_s-1); ys2 = _clamp(ys2, 0, h_s-1)\n",
        "        if xs2 <= xs1 or ys2 <= ys1:\n",
        "            z_raw.append(0.0); continue\n",
        "        rm = mags[k][ys1:ys2, xs1:xs2]\n",
        "        rd = diffs[k][ys1:ys2, xs1:xs2]\n",
        "        # robust percentiles: peak motion > background noise\n",
        "        m95 = float(np.percentile(rm, 95))\n",
        "        d75 = float(np.percentile(rd, 75))\n",
        "        z_raw.append(0.7*m95 + 0.3*d75)\n",
        "\n",
        "    z_raw = np.asarray(z_raw, dtype=np.float32)\n",
        "    mu, sd = float(z_raw.mean()), float(z_raw.std() + 1e-6)\n",
        "    z = (z_raw - mu) / sd\n",
        "    z_sm = moving_average(z, cfg.ma_len)\n",
        "\n",
        "    coarse = hysteresis_segments(z_sm, cfg.high_k, cfg.low_k)\n",
        "\n",
        "    # clamp durations in frames\n",
        "    min_len = max(1, int(round(cfg.min_dur_s * fps)))\n",
        "    max_len = max(min_len+1, int(round(cfg.max_dur_s * fps)))\n",
        "\n",
        "    refined = []\n",
        "    for a,b in coarse:\n",
        "        if (b-a+1) < min_len: continue\n",
        "        # center sub-window around the local peak, limited to max_len\n",
        "        sub = z_sm[a:b+1]\n",
        "        peak = int(np.argmax(sub))\n",
        "        half = max(min((b-a+1)//2, max_len//2), min_len//2)\n",
        "        c0 = a + max(0, peak - half)\n",
        "        c1 = a + min(len(sub)-1, peak + half)\n",
        "        if (c1-c0+1) >= min_len:\n",
        "            refined.append((c0,c1))\n",
        "\n",
        "    # merge overlaps and map to absolute indices\n",
        "    refined = sorted(refined)\n",
        "    merged = []\n",
        "    for seg in refined:\n",
        "        if not merged or seg[0] > merged[-1][1]:\n",
        "            merged.append(list(seg))\n",
        "        else:\n",
        "            merged[-1][1] = max(merged[-1][1], seg[1])\n",
        "    segs_abs = [(t0 + s0, t0 + s1) for s0,s1 in merged]\n",
        "    return z_sm, segs_abs"
      ],
      "metadata": {
        "id": "CAc6PDkGIDad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sampling with SlowFast**"
      ],
      "metadata": {
        "id": "T2QFuCm-ILoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SFInferCfg:\n",
        "    slow_t: int = 8            # <- from your cfg\n",
        "    alpha: int = 4             # <- from your cfg\n",
        "    side: int = 224            # short-side resize\n",
        "    crop: int = 224            # center crop size\n",
        "    mean: Tuple[float,float,float] = (0.45,0.45,0.45)   # Kinetics defaults (pytorchvideo)\n",
        "    std:  Tuple[float,float,float] = (0.225,0.225,0.225)\n",
        "    bbox_margin: float = 1.25\n",
        "    bbox_ema: float = 0.8\n",
        "\n",
        "def sample_indices(L: int, slow_t: int, alpha: int):\n",
        "    \"\"\"\n",
        "    Produce indices for fast (slow_t*alpha) and slow (slow_t) pathways\n",
        "    uniformly spanning the segment of length L.\n",
        "    \"\"\"\n",
        "    need_fast = slow_t * alpha\n",
        "    idx_fast = _linspace_idx(0, max(0, L-1), need_fast)\n",
        "    idx_slow = idx_fast[::alpha]\n",
        "    if len(idx_slow) < slow_t:\n",
        "        idx_slow += [idx_slow[-1]] * (slow_t - len(idx_slow))\n",
        "    elif len(idx_slow) > slow_t:\n",
        "        idx_slow = idx_slow[:slow_t]\n",
        "    return idx_slow, idx_fast\n",
        "\n",
        "def extract_clip_slowfast(video_path: str,\n",
        "                          segment: Tuple[int,int],\n",
        "                          dense_boxes: List[Tuple[int,int,int,int]],\n",
        "                          sfcfg: SFInferCfg):\n",
        "    \"\"\"\n",
        "    Build (slow, fast) tensors with shapes:\n",
        "        slow:  (3, slow_t, 224, 224)\n",
        "        fast:  (3, slow_t*alpha, 224, 224)\n",
        "    \"\"\"\n",
        "    t0, t1 = segment\n",
        "    L = t1 - t0 + 1\n",
        "\n",
        "    # Smooth bboxes with EMA to reduce jitter\n",
        "    smoothed = []\n",
        "    prev = None\n",
        "    for b in dense_boxes:\n",
        "        arr = np.array(b, dtype=np.float32)\n",
        "        prev = arr if prev is None else sfcfg.bbox_ema*prev + (1-sfcfg.bbox_ema)*arr\n",
        "        smoothed.append(tuple(prev.astype(int)))\n",
        "\n",
        "    # Decode frames once\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): raise RuntimeError(f\"Cannot open {video_path}\")\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, t0)\n",
        "    frames = []\n",
        "    for _ in range(L):\n",
        "        ok, fr = cap.read()\n",
        "        if not ok: break\n",
        "        frames.append(fr)\n",
        "    cap.release()\n",
        "    if len(frames) < L and len(frames) > 0:\n",
        "        frames += [frames[-1]] * (L - len(frames))\n",
        "\n",
        "    # Crop per frame with margin, then resize/crop to 224\n",
        "    crops = []\n",
        "    for k in range(L):\n",
        "        x1,y1,x2,y2 = _expand_bbox(smoothed[k], sfcfg.bbox_margin, W, H)\n",
        "        crop = frames[k][y1:y2, x1:x2]\n",
        "        if crop.size == 0: crop = frames[k]\n",
        "        crops.append(_center_resize_crop(crop, sfcfg.side, sfcfg.crop))\n",
        "\n",
        "    # Sample temporal indices\n",
        "    idx_slow, idx_fast = sample_indices(L, sfcfg.slow_t, sfcfg.alpha)\n",
        "    slow_frames = [crops[i] for i in idx_slow]\n",
        "    fast_frames = [crops[i] for i in idx_fast]\n",
        "\n",
        "    # To tensors (C,T,H,W) normalized\n",
        "    def _to_tensor(frames_bgr):\n",
        "        arr = np.stack([cv2.cvtColor(im, cv2.COLOR_BGR2RGB) for im in frames_bgr], axis=0).astype(np.float32)/255.0\n",
        "        mean = np.array(sfcfg.mean, dtype=np.float32).reshape(1,1,1,3)\n",
        "        std  = np.array(sfcfg.std,  dtype=np.float32).reshape(1,1,1,3)\n",
        "        arr = (arr - mean) / std\n",
        "        arr = np.transpose(arr, (3,0,1,2))  # C,T,H,W\n",
        "        return torch.from_numpy(arr)\n",
        "\n",
        "    slow_t = _to_tensor(slow_frames)  # (3, slow_t, 224, 224)\n",
        "    fast_t = _to_tensor(fast_frames)  # (3, slow_t*alpha, 224, 224)\n",
        "    return slow_t, fast_t"
      ],
      "metadata": {
        "id": "i37FzVG-INJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load SlowFast**"
      ],
      "metadata": {
        "id": "TuhMYqWEITaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_slowfast_classifier(cfg, ckpt_path: str, device: Optional[torch.device] = None):\n",
        "    \"\"\"\n",
        "    Loads slowfast_r101 (pytorchvideo), replaces head with len(cfg.labels),\n",
        "    loads checkpoint, sets to eval(). Matches your training code.\n",
        "    \"\"\"\n",
        "    device = device or _device()\n",
        "    print(f\"üß† Loading SlowFast model from: {ckpt_path}\")\n",
        "    torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "    in_dim = model.blocks[-1].proj.in_features\n",
        "    in_dim = model.blocks[-1].proj.in_features\n",
        "    model.blocks[-1].proj = nn.Sequential(\n",
        "        nn.Dropout(p=0.5), # Add a dropout layer\n",
        "        nn.Linear(in_dim, len(cfg.labels))\n",
        "    )\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "    model.eval().to(device)\n",
        "    print(f\"   -> Model loaded and set to evaluation mode on device: {device}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "ycyF5g98IUmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlowFastPredictor:\n",
        "    \"\"\"Thin wrapper that accepts lists of (slow,fast) tensors and returns probs.\"\"\"\n",
        "    def __init__(self, model, device=None):\n",
        "        self.model = model.eval()\n",
        "        self.device = device or _device()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_batch(self, slow_list: List[torch.Tensor], fast_list: List[torch.Tensor]) -> np.ndarray:\n",
        "        slow = torch.stack(slow_list).to(self.device)  # (B,3,T,H,W)\n",
        "        fast = torch.stack(fast_list).to(self.device)  # (B,3,T,H,W)\n",
        "        logits = self.model([slow, fast])              # pytorchvideo expects [slow, fast]\n",
        "        return torch.softmax(logits, dim=1).cpu().numpy()"
      ],
      "metadata": {
        "id": "7TPDaSYtIWfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.root_dir = \"/content/drive/MyDrive/FIT3163,3164/SlowFast\"\n",
        "        self.clips_dir = os.path.join(self.root_dir, \"05_clips/3in1\")\n",
        "        self.splits_dir = os.path.join(self.root_dir, \"06_splits/3in1\")\n",
        "        self.models_dir = os.path.join(self.root_dir, \"07_models/3in1_train3\")\n",
        "        self.best_model_path = os.path.join(self.models_dir, \"best.pt\")\n",
        "\n",
        "        self.labels = [\n",
        "            \"smash\", \"jump_smash\", \"block\",\n",
        "            \"drop\", \"clear\", \"lift\", \"drive\",\n",
        "            \"straight_net\", \"cross_net\", \"serve\",\n",
        "            \"push\", \"tap\",\n",
        "            \"average_joe\"\n",
        "        ]\n",
        "\n",
        "        # Dataset parameters\n",
        "        self.side = 224             # ori: 224\n",
        "        self.slow_t = 8             # 8 frames for slow pathway\n",
        "        self.alpha = 4              # ratio between fast and slow\n",
        "        self.fast_t = self.slow_t * self.alpha\n",
        "        self.fast_target = 224      # ori: 224\n",
        "\n",
        "        # Training parameters\n",
        "        self.epochs = 30\n",
        "        self.batch_size = 8\n",
        "        self.learning_rate = 0.001\n",
        "        self.weight_decay = 0.001\n",
        "\n",
        "        self.early_stopping_patience = 5\n",
        "\n",
        "# Create a configuration object\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "xVUyhwVvI2kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main function**"
      ],
      "metadata": {
        "id": "MDDVvSCnIW9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_video_to_events(\n",
        "    video_path: str,\n",
        "    csv_path: str,\n",
        "    cfg,                                 # your Config\n",
        "    ckpt_path: str,\n",
        "    action_cfg: Optional[ActionnessCfg] = None,\n",
        "    sf_cfg: Optional[SFInferCfg] = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    End-to-end:\n",
        "      1) read StrongSORT CSV\n",
        "      2) per-track actionness -> segments\n",
        "      3) per-segment stabilized crops -> SlowFast sampling\n",
        "      4) batch predict -> events\n",
        "    Returns dict with 'events' and 'debug'.\n",
        "    \"\"\"\n",
        "    print(\"=========================================\")\n",
        "    print(\"üöÄ Starting Action Event Inference Pipeline\")\n",
        "    print(f\"Video: {os.path.basename(video_path)}\")\n",
        "    print(f\"Tracking: {os.path.basename(csv_path)}\")\n",
        "    print(\"=========================================\")\n",
        "\n",
        "    action_cfg = action_cfg or ActionnessCfg()\n",
        "    sf_cfg = sf_cfg or SFInferCfg(slow_t=cfg.slow_t, alpha=cfg.alpha, side=cfg.side, crop=cfg.side)\n",
        "\n",
        "    # 1. Video Meta\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): raise RuntimeError(f\"Cannot open: {video_path}\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"üé• Video Meta: FPS={fps:.2f}, Total Frames={total_frames}\")\n",
        "    cap.release()\n",
        "\n",
        "    # 2. Load Tracking & Model\n",
        "    tracks = load_tracks_csv(csv_path)\n",
        "    model = load_slowfast_classifier(cfg, ckpt_path, _device())\n",
        "    predictor = SlowFastPredictor(model, _device())\n",
        "    total_tracks = len(tracks)\n",
        "    print(f\"üéØ Inference will run on {total_tracks} tracks.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    events = []\n",
        "    debug = {}\n",
        "\n",
        "    # 3. Process Tracks\n",
        "    track_ids = list(tracks.keys())\n",
        "    for i, tid in enumerate(tqdm(track_ids, desc=\"Total Track Progress\")):\n",
        "        timeline = tracks[tid]\n",
        "        print(f\"\\n   [TRACK {i+1}/{total_tracks}] ID: {tid} ({len(timeline)} frames over {timeline[-1][0]-timeline[0][0]+1} total frames)\")\n",
        "\n",
        "        # 3a. Compute Actionness & Segments\n",
        "        z_sm, segs = compute_actionness_segments(video_path, timeline, fps, action_cfg)\n",
        "        debug[int(tid)] = {\"t0\": int(timeline[0][0]), \"z\": list(map(float, z_sm)), \"segments\": [(int(a),int(b)) for a,b in segs]}\n",
        "        print(f\"      -> Found {len(segs)} potential action segments.\")\n",
        "        if not segs:\n",
        "            continue\n",
        "\n",
        "        # 3b. Prepare Clips for SlowFast\n",
        "        t0, t1, dense_boxes = interpolate_track(timeline)\n",
        "        slow_batch, fast_batch, metas = [], [], []\n",
        "\n",
        "        # Use a secondary progress bar for segments within the track\n",
        "        for (a,b) in tqdm(segs, desc=\"      -> Preparing Segments\"):\n",
        "            if a < t0 or b > t1: continue\n",
        "            db = dense_boxes[(a - t0):(b - t0 + 1)]\n",
        "            slow_t, fast_t = extract_clip_slowfast(video_path, (a,b), db, sf_cfg)\n",
        "            slow_batch.append(slow_t); fast_batch.append(fast_t)\n",
        "            metas.append((a,b))\n",
        "\n",
        "        if not slow_batch:\n",
        "            continue\n",
        "\n",
        "        # 3c. Batch Predict\n",
        "        print(f\"      -> Predicting {len(slow_batch)} segments in a batch...\")\n",
        "        probs = predictor.predict_batch(slow_batch, fast_batch)  # (B, K)\n",
        "\n",
        "        # 3d. Collect Events\n",
        "        for (a,b), p in zip(metas, probs):\n",
        "            k = int(np.argmax(p))\n",
        "            events.append({\n",
        "                \"track_id\": int(tid),\n",
        "                \"t0\": int(a),\n",
        "                \"t1\": int(b),\n",
        "                \"label\": cfg.labels[k],\n",
        "                \"p\": float(p[k])\n",
        "            })\n",
        "        print(f\"      -> Finished Track {tid}. Found {len(probs)} events.\")\n",
        "\n",
        "\n",
        "    # 4. Final Output\n",
        "    events.sort(key=lambda e: (e[\"t0\"], e[\"track_id\"]))\n",
        "    print(\"\\n=========================================\")\n",
        "    print(f\"‚úÖ Pipeline Complete. Total Events Found: {len(events)}\")\n",
        "    print(\"=========================================\")\n",
        "    return {\"events\": events, \"debug\": debug}"
      ],
      "metadata": {
        "id": "rqU6HhfHIafg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_VIDEO_PATH = \"/content/drive/MyDrive/FIT3163,3164/REID/test/reid_test_vid_4.mp4\"\n",
        "CSV_PATH = \"/content/annotated_track_4.csv\"\n",
        "\n",
        "device = _device()\n",
        "sf_model_path = cfg.best_model_path  # from your training\n",
        "\n",
        "res = run_full_video_to_events(\n",
        "    video_path=INPUT_VIDEO_PATH,\n",
        "    csv_path=CSV_PATH,\n",
        "    cfg=cfg,\n",
        "    ckpt_path=sf_model_path,\n",
        ")\n",
        "with open(os.path.join(cfg.models_dir, \"events.json\"), \"w\") as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "print(f\"Predicted {len(res['events'])} action events\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "tVj_Qj34IeDz",
        "outputId": "44109eb4-1740-434c-d2d6-d9a445985219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================\n",
            "üöÄ Starting Action Event Inference Pipeline\n",
            "Video: reid_test_vid_4.mp4\n",
            "Tracking: annotated_track_4.csv\n",
            "=========================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2689708530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msf_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m  \u001b[0;31m# from your training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m res = run_full_video_to_events(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvideo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_VIDEO_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2555701820.py\u001b[0m in \u001b[0;36mrun_full_video_to_events\u001b[0;34m(video_path, csv_path, cfg, ckpt_path, action_cfg, sf_cfg)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# 1. Video Meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot open: {video_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overlay on video**"
      ],
      "metadata": {
        "id": "3ZQUWXrkTzL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_events(events_json_path: str):\n",
        "    with open(events_json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    # be flexible (either {\"events\":[...]} or a plain list)\n",
        "    events = data[\"events\"] if isinstance(data, dict) and \"events\" in data else data\n",
        "    return events\n",
        "\n",
        "def build_event_map_by_frame(events):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      event_by_frame: {frame_idx: {track_id: best_event_dict}}\n",
        "      best_event = highest confidence if multiple overlap same frame & track.\n",
        "    \"\"\"\n",
        "    event_by_frame = defaultdict(dict)\n",
        "    for e in events:\n",
        "        tid, t0, t1 = int(e[\"track_id\"]), int(e[\"t0\"]), int(e[\"t1\"])\n",
        "        lab, p = e[\"label\"], float(e.get(\"p\", 1.0))\n",
        "        for t in range(t0, t1+1):\n",
        "            cur = event_by_frame[t].get(tid)\n",
        "            if (cur is None) or (p > float(cur.get(\"p\", 0.0))):\n",
        "                event_by_frame[t][tid] = {\"track_id\": tid, \"t0\": t0, \"t1\": t1, \"label\": lab, \"p\": p}\n",
        "    return event_by_frame\n",
        "\n",
        "def load_tracks_csv(csv_path: str):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      boxes_by_frame: {frame_idx: [(tid, (x1,y1,x2,y2)) ...]}\n",
        "    \"\"\"\n",
        "    boxes_by_frame = defaultdict(list)\n",
        "    with open(csv_path, \"r\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        for row in r:\n",
        "            fi = int(row[\"frame\"])\n",
        "            tid = int(row[\"id\"])\n",
        "            x1,y1,x2,y2 = int(row[\"x1\"]), int(row[\"y1\"]), int(row[\"x2\"]), int(row[\"y2\"])\n",
        "            boxes_by_frame[fi].append((tid, (x1,y1,x2,y2)))\n",
        "    return boxes_by_frame"
      ],
      "metadata": {
        "id": "qZX0KfTgJSdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_full_video_overlay(\n",
        "    video_path: str,\n",
        "    tracks_csv: str,\n",
        "    events_json: str,\n",
        "    out_path: str,\n",
        "    show_ids: bool = True,\n",
        "    label_bg_alpha: float = 0.4\n",
        "):\n",
        "    \"\"\"\n",
        "    Draws YOLO+StrongSORT boxes and overlays action labels on frames\n",
        "    that fall inside classified segments for each track.\n",
        "\n",
        "    - Progress bar drawn above the box shows position inside segment.\n",
        "    - If track has no active segment on a frame, we draw just the box (and ID).\n",
        "    \"\"\"\n",
        "    boxes_by_frame  = load_tracks_csv(tracks_csv)\n",
        "    events          = load_events(events_json)\n",
        "    event_by_frame  = build_event_map_by_frame(events)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Cannot open {video_path}\")\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    W   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    N   = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or None\n",
        "\n",
        "    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (W, H))\n",
        "    i = 0\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "\n",
        "        # draw tracks for this frame\n",
        "        items = boxes_by_frame.get(i, [])\n",
        "        for tid, (x1,y1,x2,y2) in items:\n",
        "            color = _color_for_id(tid)\n",
        "            cv2.rectangle(frame, (x1,y1), (x2,y2), color, 2)\n",
        "\n",
        "            # label overlay if we have an event active now\n",
        "            ev = event_by_frame.get(i, {}).get(tid)\n",
        "            if ev is not None:\n",
        "                label = ev[\"label\"]; p = ev[\"p\"]; t0 = ev[\"t0\"]; t1 = ev[\"t1\"]\n",
        "                # progress 0..1\n",
        "                prog = (i - t0) / max(1, (t1 - t0 + 1))\n",
        "                # translucent box for text\n",
        "                text = f\"{label}  {p:.2f}\"\n",
        "                (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                # place above box if room, else inside\n",
        "                tx, ty = x1, y1 - 8\n",
        "                if ty - th - 6 < 0:\n",
        "                    ty = y1 + th + 12\n",
        "                # background rect\n",
        "                bx1, by1 = tx - 2, ty - th - 6\n",
        "                bx2, by2 = tx + tw + 6, ty + 4\n",
        "                # alpha blend bg\n",
        "                bg = frame.copy()\n",
        "                cv2.rectangle(bg, (bx1,by1), (bx2,by2), color, -1)\n",
        "                frame = cv2.addWeighted(bg, label_bg_alpha, frame, 1 - label_bg_alpha, 0)\n",
        "                # text\n",
        "                cv2.putText(frame, text, (tx, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "                # progress bar on top edge of bbox\n",
        "                bar_h = 6\n",
        "                px1, py1 = x1, max(0, y1 - bar_h - 2)\n",
        "                px2, py2 = x2, max(0, y1 - 2)\n",
        "                # bar background (light)\n",
        "                cv2.rectangle(frame, (px1,py1), (px2,py2), (200,200,200), -1)\n",
        "                # bar fill with track color\n",
        "                fill_w = int((px2 - px1) * _clamp(prog, 0.0, 1.0))\n",
        "                cv2.rectangle(frame, (px1,py1), (px1 + fill_w, py2), color, -1)\n",
        "\n",
        "            elif show_ids:\n",
        "                # only ID label\n",
        "                text = f\"ID {tid}\"\n",
        "                (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "                tx, ty = x1, y1 - 8\n",
        "                if ty - th - 6 < 0: ty = y1 + th + 12\n",
        "                cv2.putText(frame, text, (tx, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "        out.write(frame)\n",
        "        i += 1\n",
        "\n",
        "    cap.release(); out.release()\n",
        "    print(f\"Overlay saved ‚Üí {out_path}\")"
      ],
      "metadata": {
        "id": "Iw2M7jj6T3Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVENTS_JSON = os.path.join(cfg.models_dir, \"events.json\")\n",
        "EVENTS_JSON = 'events_1.json'\n",
        "\n",
        "# render_full_video_overlay(\n",
        "#     video_path=INPUT_VIDEO_PATH,\n",
        "#     tracks_csv=CSV_PATH,\n",
        "#     events_json=EVENTS_JSON,\n",
        "#     out_path=\"/content/out_overlay.mp4\",\n",
        "# )\n",
        "\n",
        "render_full_video_overlay(\n",
        "    video_path=INPUT_VIDEO_PATH,\n",
        "    tracks_csv=\"/content/outputs_webuser_123_manual-test-009_tracks_rectified.csv\",\n",
        "    events_json=EVENTS_JSON,\n",
        "    out_path=\"/content/out_overlay.mp4\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeOdHWq1T5h9",
        "outputId": "ef9e6cee-e1a2-4dff-c50d-d382270e6f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlay saved ‚Üí /content/out_overlay.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFAdJvOWT8YG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}