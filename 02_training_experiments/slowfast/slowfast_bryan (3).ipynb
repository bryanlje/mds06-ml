{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Build label map & splits**"
      ],
      "metadata": {
        "id": "sk-am0rbtFkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchmetrics decord fvcore pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlv-BF09KLjb",
        "outputId": "6e93be61-7574-493a-b90c-1d89009f2026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, csv, glob, os\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassConfusionMatrix\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import v2\n",
        "from decord import VideoReader, cpu\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-ziMQw9hQwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jFlWfIQXhSqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80db289-5510-4db6-c43d-0d651991d1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aV_Tuy68D3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. CONFIGURATION**\n",
        "### This class centralizes all hyperparameters and file paths."
      ],
      "metadata": {
        "id": "wP3eWavtFCaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.root_dir = \"/content/drive/MyDrive/FIT3163,3164/SlowFast\"\n",
        "        self.clips_dir = os.path.join(self.root_dir, \"05_clips/3in1\")\n",
        "        self.splits_dir = os.path.join(self.root_dir, \"06_splits/3in1\")\n",
        "        self.models_dir = os.path.join(self.root_dir, \"07_models/3in1_train3\")\n",
        "        self.best_model_path = os.path.join(self.models_dir, \"best.pt\")\n",
        "\n",
        "        self.labels = [\n",
        "            \"smash\", \"jump_smash\", \"block\",\n",
        "            \"drop\", \"clear\", \"lift\", \"drive\",\n",
        "            \"straight_net\", \"cross_net\", \"serve\",\n",
        "            \"push\", \"tap\",\n",
        "            \"average_joe\"\n",
        "        ]\n",
        "\n",
        "        # Dataset parameters\n",
        "        self.side = 224             # ori: 224\n",
        "        self.slow_t = 8             # 8 frames for slow pathway\n",
        "        self.alpha = 4              # ratio between fast and slow\n",
        "        self.fast_t = self.slow_t * self.alpha\n",
        "        self.fast_target = 224      # ori: 224\n",
        "\n",
        "        # Training parameters\n",
        "        self.epochs = 30\n",
        "        self.batch_size = 8\n",
        "        self.learning_rate = 0.001\n",
        "        self.weight_decay = 0.001\n",
        "\n",
        "        self.early_stopping_patience = 5\n",
        "\n",
        "# Create a configuration object\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "pOEfK8GlFFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. DATA PREPARATION**\n",
        "### This function handles all logic for splitting and saving the dataset."
      ],
      "metadata": {
        "id": "A9_EREhMIlz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_splits(config: Config):\n",
        "    \"\"\"\n",
        "    Finds video clips, shuffles them, and splits them into train, val, and test sets.\n",
        "    Saves the splits as CSV files and the label map as a JSON file.\n",
        "    \"\"\"\n",
        "    os.makedirs(config.splits_dir, exist_ok=True)\n",
        "    os.makedirs(config.models_dir, exist_ok=True)\n",
        "\n",
        "    labels_map = {lab: i for i, lab in enumerate(config.labels)}\n",
        "    with open(os.path.join(config.splits_dir, \"labels_map.json\"), \"w\") as f:\n",
        "        json.dump(labels_map, f, indent=2)\n",
        "\n",
        "    items = []\n",
        "    for label in config.labels:\n",
        "        # Use glob to find all video files for the current label\n",
        "        for clip_path in glob.glob(os.path.join(config.clips_dir, label, \"*.mp4\")):\n",
        "            items.append((clip_path, labels_map[label]))\n",
        "\n",
        "    random.seed(1337)\n",
        "    random.shuffle(items)\n",
        "\n",
        "    total_items = len(items)\n",
        "    train_count = int(0.8 * total_items)\n",
        "    val_count = int(0.1 * total_items)\n",
        "    print(f\"Found {total_items} clips in total, splitting to train ({train_count}) and val ({val_count}).\")\n",
        "\n",
        "    splits = {\n",
        "        \"train.csv\": items[:train_count],\n",
        "        \"val.csv\": items[train_count:train_count + val_count],\n",
        "        \"test.csv\": items[train_count + val_count:]\n",
        "    }\n",
        "\n",
        "    for name, data in splits.items():\n",
        "        with open(os.path.join(config.splits_dir, name), \"w\", newline=\"\") as f:\n",
        "            csv_writer = csv.writer(f)\n",
        "            csv_writer.writerows(data)\n",
        "\n",
        "    print({k: len(v) for k, v in splits.items()})"
      ],
      "metadata": {
        "id": "tWgRS404IoRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data_splits(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmj70hvA6Vj",
        "outputId": "190bb85c-e0e0-4056-ec90-ffd33048e95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 506 clips in total, splitting to train (404) and val (50).\n",
            "{'train.csv': 404, 'val.csv': 50, 'test.csv': 52}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. DATASET**\n",
        "### The ClipDataset class handles video loading and preprocessing."
      ],
      "metadata": {
        "id": "DsPRxWf8JREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipDataset(Dataset):\n",
        "    def __init__(self, csv_path: str, config: Config, train: bool = True):\n",
        "        self.items = [(p, int(y)) for p, y in csv.reader(open(csv_path))]\n",
        "        self.config = config\n",
        "        self.train = train\n",
        "\n",
        "        # Pre-compute normalization tensors\n",
        "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3, 1, 1)\n",
        "        self.std = torch.tensor([0.225, 0.225, 0.225]).view(3, 1, 1)\n",
        "\n",
        "        # Define a composed transform for training\n",
        "        if self.train:\n",
        "            self.train_transforms = v2.Compose([\n",
        "                v2.RandomResizedCrop(\n",
        "                    size=self.config.side,\n",
        "                    scale=(0.7, 1.0),\n",
        "                    ratio=(0.75, 1.333),\n",
        "                    antialias=True\n",
        "                ),\n",
        "                v2.RandomHorizontalFlip(p=0.5),\n",
        "                v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                v2.RandomGrayscale(p=0.2),\n",
        "            ])\n",
        "\n",
        "    def _get_frame_indices(self, num_frames: int):\n",
        "        \"\"\"\n",
        "        Return indices for fast (T = fast_t) and slow (stride alpha).\n",
        "        Train: random crop; Eval: center crop.\n",
        "        \"\"\"\n",
        "        # This part of the code remains unchanged.\n",
        "        need = self.config.fast_t\n",
        "        if num_frames >= need:\n",
        "            start = np.random.randint(0, num_frames - need + 1) if self.train else max((num_frames - need) // 2, 0)\n",
        "            fast_idx = list(range(start, start + need))\n",
        "        else:\n",
        "            fast_idx = list(range(num_frames)) + [num_frames - 1] * (need - num_frames)\n",
        "        slow_idx = fast_idx[::self.config.alpha]\n",
        "        return slow_idx, fast_idx\n",
        "\n",
        "    def _read_and_process_frames(self, vr: VideoReader, indices: List[int]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns (C, T, H, W) normalized to kinetics-style mean/std.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            frames = vr.get_batch([min(i, len(vr)-1) for i in indices]).asnumpy()\n",
        "        except Exception:\n",
        "            frames = np.stack([vr[min(i, len(vr)-1)].asnumpy() for i in indices], axis=0)\n",
        "\n",
        "        # Convert to tensor and permute dimensions\n",
        "        x = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0  # (T, C, H, W)\n",
        "\n",
        "        # Apply data augmentation only for training\n",
        "        if self.train:\n",
        "            # Apply the same random transform to all frames\n",
        "            x = self.train_transforms(x)\n",
        "\n",
        "        # Resize to the required size if necessary\n",
        "        x = F.interpolate(x, size=self.config.side, mode=\"bilinear\", align_corners=False) # (T, C, 224, 224)\n",
        "\n",
        "        # Normalize\n",
        "        mean = self.mean.to(x)\n",
        "        std = self.std.to(x)\n",
        "        x = (x - mean) / std\n",
        "\n",
        "        return x.permute(1, 0, 2, 3) # (C, T, H, W)\n",
        "\n",
        "    def __getitem__(self, i: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], int]:\n",
        "        \"\"\"Loads and preprocesses a single clip and its label.\"\"\"\n",
        "        path, label = self.items[i]\n",
        "        vr = VideoReader(path, ctx=cpu(0))\n",
        "\n",
        "        # Randomly choose frames from the entire video\n",
        "        slow_indices, fast_indices = self._get_frame_indices(len(vr))\n",
        "\n",
        "        # Get and process clips\n",
        "        slow_clip = self._read_and_process_frames(vr, slow_indices)\n",
        "        fast_clip = self._read_and_process_frames(vr, fast_indices)\n",
        "\n",
        "        return (slow_clip, fast_clip), label\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.items)"
      ],
      "metadata": {
        "id": "t3nn3vXSB60U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generate datasets and loaders for training, validation, and testing**"
      ],
      "metadata": {
        "id": "oaWSMQwAnd6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def slowfast_collate(batch):\n",
        "    # batch: list of [((slow, fast), y), ...]\n",
        "    slows, fasts, ys = [], [], []\n",
        "    for (s, f), y in batch:\n",
        "        slows.append(s)\n",
        "        fasts.append(f)\n",
        "        ys.append(y)\n",
        "    slow = torch.stack(slows, dim=0)  # (B,C,T,H,W)\n",
        "    fast = torch.stack(fasts, dim=0)  # (B,C,T,H,W)\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return [slow, fast], y\n",
        "\n",
        "train_csv = os.path.join(cfg.splits_dir, \"train.csv\")\n",
        "val_csv   = os.path.join(cfg.splits_dir, \"val.csv\")\n",
        "test_csv  = os.path.join(cfg.splits_dir, \"test.csv\")\n",
        "\n",
        "train_ds = ClipDataset(train_csv, cfg, train=True)\n",
        "val_ds   = ClipDataset(val_csv,   cfg, train=False)\n",
        "test_ds  = ClipDataset(test_csv,  cfg, train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=max(1, cfg.batch_size), shuffle=False,\n",
        "    num_workers=2, pin_memory=True, collate_fn=slowfast_collate, persistent_workers=False\n",
        ")\n",
        "\n",
        "num_classes = len(cfg.labels)\n",
        "print(\"Classes:\", num_classes, cfg.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhuyMRO0BCIi",
        "outputId": "d418f1bd-b3e8-4405-d169-5d56948e0e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: 13 ['smash', 'jump_smash', 'block', 'drop', 'clear', 'lift', 'drive', 'straight_net', 'cross_net', 'serve', 'push', 'tap', 'average_joe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. TRAINING AND EVALUATION**\n",
        "### This function orchestrates the entire training process."
      ],
      "metadata": {
        "id": "5A5kc3azJqM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load pre-trained model from hub**"
      ],
      "metadata": {
        "id": "pVRiC1dVnC3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) Model: load hub, replace head\n",
        "# =========================\n",
        "torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "\n",
        "# Replace classifier (ResNetBasicHead.proj)\n",
        "in_dim = model.blocks[-1].proj.in_features\n",
        "model.blocks[-1].proj = nn.Sequential(\n",
        "    nn.Dropout(p=0.5), # Add a dropout layer\n",
        "    nn.Linear(in_dim, num_classes)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: freeze early blocks for faster convergence at small data sizes\n",
        "for p in model.blocks[:-1].parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k156bc16JsZV",
        "outputId": "15411e86-afb8-433a-f563-7bcbcf0fe357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Optional: load weights from checkpoint**"
      ],
      "metadata": {
        "id": "vf01hSLgnLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_dropout0.2/best.pt'\n",
        "\n",
        "# Load the saved checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Load the model's state_dict from the checkpoint\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "print(f\"Model weights loaded successfully from {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "8SpDF2X-m3_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define training components**"
      ],
      "metadata": {
        "id": "NIcUZCSXnRle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4) Optimizer, loss, metrics\n",
        "# =========================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "acc = MulticlassAccuracy(num_classes=num_classes, average='micro').to(device)\n",
        "f1  = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNULCIKv0RyL",
        "outputId": "154490fb-246b-4cd8-9c29-523b926303a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1924732400.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Main training loop**"
      ],
      "metadata": {
        "id": "SbCblhb1nXdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5) Train / validate\n",
        "# =========================\n",
        "best_f1 = -1.0\n",
        "os.makedirs(cfg.models_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    model.train()\n",
        "    acc.reset(); f1.reset()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    first = True\n",
        "    for (slow_fast, y) in train_loader:\n",
        "        if first:\n",
        "            s, f = slow_fast\n",
        "            # print(\"slow:\", tuple(s.shape), \"fast:\", tuple(f.shape))\n",
        "            # Expect slow=(B,3,8,224,224) and fast=(B,3,32,224,224)\n",
        "            first = False\n",
        "\n",
        "        # slow_fast is [slow, fast]\n",
        "        slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "            logits = model(slow_fast)     # (B, num_classes)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        acc.update(logits, y)\n",
        "        f1.update(logits, y)\n",
        "\n",
        "    train_loss = total_loss / len(train_ds)\n",
        "    train_acc  = acc.compute().item()\n",
        "    train_f1   = f1.compute().item()\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    acc.reset(); f1.reset()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "        for (slow_fast, y) in val_loader:\n",
        "            slow_fast = [t.to(device, non_blocking=True) for t in slow_fast]\n",
        "            y = y.to(device, non_blocking=True)\n",
        "            logits = model(slow_fast)\n",
        "            loss = criterion(logits, y)\n",
        "            val_loss += loss.item() * y.size(0)\n",
        "            acc.update(logits, y)\n",
        "            f1.update(logits, y)\n",
        "\n",
        "    val_loss /= len(val_ds)\n",
        "    val_acc = acc.compute().item()\n",
        "    val_f1  = f1.compute().item()\n",
        "\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    print(f\"\\n[{epoch+1:02d}/{cfg.epochs}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc*100:.2f}% f1={train_f1:.3f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc*100:.2f}% f1={val_f1:.3f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save({\"model\": model.state_dict(), \"labels\": cfg.labels}, cfg.best_model_path)\n",
        "        print(f\"  ↳ saved new best to {cfg.best_model_path} (val_f1={best_f1:.3f})\")\n",
        "\n",
        "print(\"Best val F1:\", best_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXDAY8gmBImr",
        "outputId": "e7a3d0ed-dc75-4b69-c0ad-4cb5ecc32d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1708380312.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
            "/tmp/ipython-input-1708380312.py:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[01/30] train_loss=2.3816 acc=18.56% f1=0.066 | val_loss=2.3100 acc=20.00% f1=0.060\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/best.pt (val_f1=0.060)\n",
            "\n",
            "[02/30] train_loss=2.1701 acc=25.99% f1=0.119 | val_loss=2.1914 acc=26.00% f1=0.138\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/best.pt (val_f1=0.138)\n",
            "\n",
            "[03/30] train_loss=1.9949 acc=31.68% f1=0.168 | val_loss=2.0802 acc=22.00% f1=0.066\n",
            "\n",
            "[04/30] train_loss=1.9057 acc=35.40% f1=0.195 | val_loss=1.9340 acc=54.00% f1=0.348\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/best.pt (val_f1=0.348)\n",
            "\n",
            "[05/30] train_loss=1.8550 acc=36.39% f1=0.221 | val_loss=1.8920 acc=44.00% f1=0.307\n",
            "\n",
            "[06/30] train_loss=1.8198 acc=37.13% f1=0.235 | val_loss=1.9351 acc=38.00% f1=0.219\n",
            "\n",
            "[07/30] train_loss=1.7714 acc=39.36% f1=0.253 | val_loss=1.7670 acc=56.00% f1=0.391\n",
            "  ↳ saved new best to /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/best.pt (val_f1=0.391)\n",
            "\n",
            "[08/30] train_loss=1.6558 acc=43.56% f1=0.251 | val_loss=1.8375 acc=40.00% f1=0.236\n",
            "\n",
            "[09/30] train_loss=1.6999 acc=39.85% f1=0.256 | val_loss=1.7220 acc=46.00% f1=0.349\n",
            "\n",
            "[10/30] train_loss=1.6359 acc=45.05% f1=0.343 | val_loss=1.7127 acc=58.00% f1=0.384\n",
            "\n",
            "[11/30] train_loss=1.6498 acc=41.34% f1=0.321 | val_loss=1.6674 acc=54.00% f1=0.377\n",
            "\n",
            "[12/30] train_loss=1.6066 acc=41.09% f1=0.289 | val_loss=1.6709 acc=54.00% f1=0.378\n",
            "\n",
            "[13/30] train_loss=1.5935 acc=42.33% f1=0.313 | val_loss=1.7480 acc=48.00% f1=0.368\n",
            "\n",
            "[14/30] train_loss=1.5998 acc=47.77% f1=0.339 | val_loss=1.7427 acc=40.00% f1=0.302\n",
            "\n",
            "[15/30] train_loss=1.5277 acc=44.55% f1=0.344 | val_loss=1.7149 acc=48.00% f1=0.332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate on test set**"
      ],
      "metadata": {
        "id": "rWumfwrwgrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestManager:\n",
        "    \"\"\"\n",
        "    Manages the evaluation process for a SlowFast model on a test set.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: 'Config', device: str):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.num_classes = len(config.labels)\n",
        "        self.model = self._load_model()\n",
        "        self.test_loader = self._create_dataloader()\n",
        "        self.metrics = self._initialize_metrics()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Loads the pre-trained SlowFast model and the fine-tuned checkpoint.\"\"\"\n",
        "        print(\"Loading model and best checkpoint...\")\n",
        "\n",
        "        # Disable the internal hub check for local loading\n",
        "        torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "\n",
        "        model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "        in_dim = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Sequential(\n",
        "            nn.Dropout(p=0.2), # Add a dropout layer\n",
        "            nn.Linear(in_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        # Load the state dictionary from the checkpoint file\n",
        "        ckpt = torch.load(self.config.best_model_path, map_location=self.device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def _create_dataloader(self):\n",
        "        \"\"\"Creates and returns the DataLoader for the test set.\"\"\"\n",
        "        test_ds = ClipDataset(os.path.join(self.config.splits_dir, \"test.csv\"), self.config, train=False)\n",
        "        return DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=max(1, self.config.batch_size),\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "            # collate_fn=slowfast_collate,  # Make sure this is imported if needed\n",
        "            persistent_workers=False\n",
        "        )\n",
        "\n",
        "    def _initialize_metrics(self):\n",
        "        \"\"\"Initializes all the evaluation metrics.\"\"\"\n",
        "        return {\n",
        "            'top1': MulticlassAccuracy(num_classes=self.num_classes, average=\"micro\").to(self.device),\n",
        "            'top3': MulticlassAccuracy(num_classes=self.num_classes, top_k=3).to(self.device),\n",
        "            'f1_macro': MulticlassF1Score(num_classes=self.num_classes, average=\"macro\").to(self.device),\n",
        "            'f1_perclass': MulticlassF1Score(num_classes=self.num_classes, average=None).to(self.device),\n",
        "            'cm': MulticlassConfusionMatrix(num_classes=self.num_classes).to(self.device)\n",
        "        }\n",
        "\n",
        "    def run_inference(self):\n",
        "        \"\"\"Runs the inference loop and computes all metrics and predictions.\"\"\"\n",
        "        print(\"Starting inference on the test set...\")\n",
        "        test_loss = 0.0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad(), torch.amp.autocast(self.device, enabled=(self.device == \"cuda\")):\n",
        "            for batch_idx, (slow_fast, y) in enumerate(self.test_loader):\n",
        "                # Ensure input tensors are lists\n",
        "                if not isinstance(slow_fast, list):\n",
        "                    slow_fast = [slow_fast]\n",
        "\n",
        "                slow_fast = [t.to(self.device, non_blocking=True) for t in slow_fast]\n",
        "                y = y.to(self.device, non_blocking=True)\n",
        "\n",
        "                logits = self.model(slow_fast)\n",
        "                loss = criterion(logits, y)\n",
        "                test_loss += loss.item() * y.size(0)\n",
        "\n",
        "                # Update metrics\n",
        "                for metric in self.metrics.values():\n",
        "                    metric.update(logits, y)\n",
        "\n",
        "                # Collect per-sample predictions for later saving\n",
        "                probs = self.softmax(logits)\n",
        "                conf, pred = probs.max(dim=1)\n",
        "                topk_conf, topk_idx = probs.topk(3, dim=1)\n",
        "\n",
        "                start_idx = batch_idx * self.test_loader.batch_size\n",
        "\n",
        "                for i in range(y.size(0)):\n",
        "                    idx = start_idx + i\n",
        "                    path = self.test_loader.dataset.items[idx][0]\n",
        "                    row = {\n",
        "                        \"path\": path,\n",
        "                        \"file\": os.path.basename(path),\n",
        "                        \"true_idx\": int(y[i]),\n",
        "                        \"true_label\": self.config.labels[int(y[i])],\n",
        "                        \"pred_idx\": int(pred[i]),\n",
        "                        \"pred_label\": self.config.labels[int(pred[i])],\n",
        "                        \"pred_prob\": float(conf[i]),\n",
        "                        \"top1_label\": self.config.labels[int(topk_idx[i,0])],\n",
        "                        \"top1_prob\":  float(topk_conf[i,0]),\n",
        "                        \"top2_label\": self.config.labels[int(topk_idx[i,1])],\n",
        "                        \"top2_prob\":  float(topk_conf[i,1]),\n",
        "                        \"top3_label\": self.config.labels[int(topk_idx[i,2])],\n",
        "                        \"top3_prob\":  float(topk_conf[i,2]),\n",
        "                    }\n",
        "                    all_predictions.append(row)\n",
        "\n",
        "        test_loss /= len(self.test_loader.dataset)\n",
        "        return test_loss, all_predictions\n",
        "\n",
        "    def compute_and_print_results(self, test_loss):\n",
        "        \"\"\"Computes and prints the final metrics.\"\"\"\n",
        "        acc1 = self.metrics['top1'].compute().item()\n",
        "        acc3 = self.metrics['top3'].compute().item()\n",
        "        f1M = self.metrics['f1_macro'].compute().item()\n",
        "        percls = self.metrics['f1_perclass'].compute().detach().cpu().tolist()\n",
        "        confmat = self.metrics['cm'].compute().detach().cpu().numpy()\n",
        "\n",
        "        print(f\"\\nTEST: loss={test_loss:.4f} | acc@1={acc1*100:.2f}% | acc@3={acc3*100:.2f}% | macro-F1={f1M:.3f}\")\n",
        "        print(\"\\nPer-class F1:\")\n",
        "        for lab, s in sorted(zip(self.config.labels, percls), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {lab:15s} {s:.3f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\n",
        "        print(confmat)\n",
        "\n",
        "    def save_predictions(self, predictions: list, print_n: int=10):\n",
        "        \"\"\"Saves the list of predictions to a CSV file.\"\"\"\n",
        "        df = pd.DataFrame(predictions)\n",
        "        save_path = os.path.join(self.config.models_dir, \"test_predictions.csv\")\n",
        "        df.to_csv(save_path, index=False)\n",
        "        print(f\"\\nSaved per-sample predictions to: {save_path}\")\n",
        "        print(\"\\nQuick peek at the predictions:\")\n",
        "        print(df.head(print_n)[[\"file\", \"true_label\", \"pred_label\", \"pred_prob\", \"top2_label\", \"top2_prob\", \"top3_label\", \"top3_prob\"]])"
      ],
      "metadata": {
        "id": "A9WccyBwMrMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "test_manager = TestManager(cfg, device)\n",
        "test_loss, all_predictions = test_manager.run_inference()\n",
        "test_manager.compute_and_print_results(test_loss)\n",
        "test_manager.save_predictions(all_predictions, print_n=len(all_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO-6i20oihsn",
        "outputId": "190bdc71-3e5c-44f7-9de0-e0481cb2edb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and best checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting inference on the test set...\n",
            "\n",
            "TEST: loss=1.5431 | acc@1=57.69% | acc@3=75.73% | macro-F1=0.447\n",
            "\n",
            "Per-class F1:\n",
            "  serve           1.000\n",
            "  jump_smash      0.857\n",
            "  lift            0.769\n",
            "  straight_net    0.696\n",
            "  cross_net       0.500\n",
            "  drop            0.400\n",
            "  clear           0.250\n",
            "  smash           0.000\n",
            "  block           0.000\n",
            "  drive           0.000\n",
            "  push            0.000\n",
            "  tap             0.000\n",
            "  average_joe     0.000\n",
            "\n",
            "Confusion Matrix (rows=true, cols=predicted):\n",
            "[[ 0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
            " [ 0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  0  1  1  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  1  1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 10  0  1  1  0  1  0  0]\n",
            " [ 0  0  0  0  3  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  8  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  1  0  3  4  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  3  0  0  0]\n",
            " [ 0  0  0  0  0  1  1  1  2  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "\n",
            "Saved per-sample predictions to: /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/test_predictions.csv\n",
            "\n",
            "Quick peek at the predictions:\n",
            "              file    true_label    pred_label  pred_prob    top2_label  \\\n",
            "0   5c708fa987.mp4          drop         clear   0.343933          drop   \n",
            "1   afcd8f974d.mp4          lift          lift   0.951497  straight_net   \n",
            "2   536c881a94.mp4    jump_smash    jump_smash   0.832652          drop   \n",
            "3   3f06a7393e.mp4         drive         clear   0.296031         drive   \n",
            "4   ab83731f5d.mp4          lift          lift   0.690797         clear   \n",
            "5   7561f13887.mp4     cross_net     cross_net   0.995091          push   \n",
            "6   e553b446af.mp4         smash     cross_net   0.997572          push   \n",
            "7   1aeba18e58.mp4  straight_net  straight_net   0.866147          lift   \n",
            "8   96eed883d7.mp4         clear         clear   0.725631    jump_smash   \n",
            "9   7daf4997a6.mp4     cross_net  straight_net   0.789317     cross_net   \n",
            "10  5f6bad2aa9.mp4  straight_net  straight_net   0.731871          lift   \n",
            "11  7c30236b94.mp4     cross_net     cross_net   0.999402         drive   \n",
            "12  e2a8610d4a.mp4          push     cross_net   0.745575          push   \n",
            "13  c67cbda531.mp4          lift          lift   0.866078          push   \n",
            "14  1ee37e4121.mp4         serve         serve   1.000000         smash   \n",
            "15  ca2df5a73c.mp4          lift  straight_net   0.164825          lift   \n",
            "16  1e6a478b28.mp4  straight_net  straight_net   0.983787          push   \n",
            "17  8ad079d707.mp4  straight_net  straight_net   0.999699          drop   \n",
            "18  03b69ddc77.mp4          lift          push   0.392367  straight_net   \n",
            "19  cdf2145af3.mp4    jump_smash    jump_smash   0.440167         clear   \n",
            "20  5ec10cded7.mp4         serve         serve   1.000000          push   \n",
            "21  055439e167.mp4          lift          lift   0.934582  straight_net   \n",
            "22  301741faa6.mp4          push     cross_net   0.928181          push   \n",
            "23  53a50cdebc.mp4     cross_net          lift   0.746533          push   \n",
            "24  59daa684b4.mp4          lift     cross_net   0.276962          lift   \n",
            "25  179cf04236.mp4    jump_smash    jump_smash   0.535707          drop   \n",
            "26  3832e42975.mp4          lift          lift   0.907455         clear   \n",
            "27  aa81076d2b.mp4          push         drive   0.824931         block   \n",
            "28  798093f8cf.mp4         serve         serve   0.999996          lift   \n",
            "29  b848f1a992.mp4  straight_net  straight_net   0.943305          lift   \n",
            "30  7e74de3477.mp4     cross_net     cross_net   0.998278          push   \n",
            "31  eabe95dd19.mp4          drop          drop   0.581834    jump_smash   \n",
            "32  eb7ba507f2.mp4  straight_net  straight_net   0.999975          drop   \n",
            "33  25ef888817.mp4          lift          lift   0.486159  straight_net   \n",
            "34  8150c8a453.mp4         clear          drop   0.351249         clear   \n",
            "35  a5681f71d6.mp4          push          lift   0.596826  straight_net   \n",
            "36  364c6bcbf1.mp4          lift          lift   0.253807          push   \n",
            "37  c44388c37b.mp4     cross_net  straight_net   0.646057          lift   \n",
            "38  65af47769e.mp4  straight_net         drive   0.457095         block   \n",
            "39  ce4e3f4c76.mp4          lift          lift   0.692277          push   \n",
            "40  37f138f5a5.mp4         clear          lift   0.467164  straight_net   \n",
            "41  ebab7d7654.mp4     cross_net  straight_net   0.474225          push   \n",
            "42  8b668ee246.mp4     cross_net     cross_net   0.991506          push   \n",
            "43  f5c1e87f95.mp4          drop    jump_smash   0.307699         drive   \n",
            "44  d24bb91e3e.mp4  straight_net  straight_net   0.999560          drop   \n",
            "45  007356ca4f.mp4         drive         clear   0.394077          lift   \n",
            "46  5fa97098fc.mp4         drive         clear   0.493458         drive   \n",
            "47  201801ea69.mp4          lift          lift   0.794014         clear   \n",
            "48  11592f13f0.mp4  straight_net  straight_net   0.965902          lift   \n",
            "49  571830f0ab.mp4          lift          lift   0.989418  straight_net   \n",
            "50  f34a948eea.mp4          push  straight_net   0.616161          push   \n",
            "51  0c5c1d1d20.mp4  straight_net          push   0.381927     cross_net   \n",
            "\n",
            "       top2_prob    top3_label     top3_prob  \n",
            "0   3.256289e-01         drive  1.067537e-01  \n",
            "1   4.677464e-02         clear  5.303856e-04  \n",
            "2   1.529011e-01           tap  8.052187e-03  \n",
            "3   1.827357e-01    jump_smash  1.693333e-01  \n",
            "4   2.818463e-01         drive  1.022623e-02  \n",
            "5   4.810508e-03           tap  3.940413e-05  \n",
            "6   1.923887e-03         drive  4.347618e-04  \n",
            "7   1.260049e-01     cross_net  5.779338e-03  \n",
            "8   9.014960e-02          drop  8.669602e-02  \n",
            "9   8.345785e-02          push  6.745435e-02  \n",
            "10  1.159686e-01     cross_net  9.442003e-02  \n",
            "11  4.059862e-04          push  1.559114e-04  \n",
            "12  2.435941e-01  straight_net  3.958537e-03  \n",
            "13  5.378108e-02  straight_net  3.300438e-02  \n",
            "14  2.741272e-09         block  7.235330e-10  \n",
            "15  1.383908e-01    jump_smash  1.200014e-01  \n",
            "16  5.658771e-03          lift  4.364227e-03  \n",
            "17  1.781091e-04           tap  4.197541e-05  \n",
            "18  1.856129e-01         block  1.201923e-01  \n",
            "19  3.553286e-01          drop  1.224390e-01  \n",
            "20  1.444980e-07         smash  5.680770e-08  \n",
            "21  3.084475e-02          push  2.115790e-02  \n",
            "22  4.616630e-02         drive  7.369052e-03  \n",
            "23  8.566122e-02     cross_net  4.693848e-02  \n",
            "24  2.734677e-01          push  1.592011e-01  \n",
            "25  4.546486e-01         clear  5.095277e-03  \n",
            "26  5.562609e-02         serve  1.285597e-02  \n",
            "27  6.379846e-02          push  3.195482e-02  \n",
            "28  1.881206e-06          drop  6.866712e-07  \n",
            "29  2.426981e-02     cross_net  1.648605e-02  \n",
            "30  1.503784e-03         drive  2.011441e-04  \n",
            "31  3.215546e-01         clear  3.990483e-02  \n",
            "32  1.937382e-05          lift  3.951435e-06  \n",
            "33  4.345156e-01         clear  4.323345e-02  \n",
            "34  2.449718e-01  straight_net  8.432995e-02  \n",
            "35  2.481117e-01          push  8.955348e-02  \n",
            "36  1.972795e-01         block  1.722384e-01  \n",
            "37  3.045061e-01         clear  2.815800e-02  \n",
            "38  1.790009e-01     cross_net  8.195259e-02  \n",
            "39  7.484183e-02  straight_net  6.326980e-02  \n",
            "40  1.956961e-01     cross_net  1.649537e-01  \n",
            "41  1.458360e-01          lift  1.351399e-01  \n",
            "42  8.237006e-03         drive  1.565029e-04  \n",
            "43  1.877254e-01         clear  1.527688e-01  \n",
            "44  3.646926e-04         clear  3.799050e-05  \n",
            "45  1.818367e-01    jump_smash  1.582913e-01  \n",
            "46  2.529123e-01         smash  1.282244e-01  \n",
            "47  1.145002e-01    jump_smash  2.682690e-02  \n",
            "48  2.074369e-02          push  1.088854e-02  \n",
            "49  4.863152e-03         clear  3.290573e-03  \n",
            "50  1.177401e-01          lift  8.870146e-02  \n",
            "51  1.504803e-01  straight_net  1.465645e-01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **end-to-end match inference & overlay**"
      ],
      "metadata": {
        "id": "nU9T0fc3nRx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug-sBuM4nkRl",
        "outputId": "ae3d3ee4-bf4d-417c-d341-dbe9e3f7eefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, numpy as np, torch\n",
        "from collections import deque, defaultdict\n",
        "from ultralytics import YOLO\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oaoCKiwUgy97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_slowfast_classifier(cfg, ckpt_path):\n",
        "    torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "    in_dim = model.blocks[-1].proj.in_features\n",
        "    model.blocks[-1].proj = torch.nn.Linear(in_dim, len(cfg.labels))\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "    model.eval().to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mx6acAn2obMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_pad_square(img_rgb: np.ndarray, side: int = 224) -> np.ndarray:\n",
        "    \"\"\"Keep aspect ratio; resize the longer side to `side`, then pad to (side, side).\"\"\"\n",
        "    h, w = img_rgb.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((side, side, 3), dtype=img_rgb.dtype)\n",
        "    scale = side / max(h, w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    resized = cv2.resize(img_rgb, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    top  = (side - nh) // 2\n",
        "    bottom = side - nh - top\n",
        "    left = (side - nw) // 2\n",
        "    right = side - nw - left\n",
        "    out = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128,128,128))\n",
        "    return out\n",
        "\n",
        "def expand_box(x1, y1, x2, y2, scale: float, W: int, H: int):\n",
        "    \"\"\"Optionally enlarge the bbox to keep some context (e.g., racket).\"\"\"\n",
        "    cx, cy = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
        "    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale\n",
        "    nx1, ny1 = int(max(0, cx - bw / 2)), int(max(0, cy - bh / 2))\n",
        "    nx2, ny2 = int(min(W - 1, cx + bw / 2)), int(min(H - 1, cy + bh / 2))\n",
        "    return nx1, ny1, nx2, ny2"
      ],
      "metadata": {
        "id": "iasMXtJYsCZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlowFastPredictor:\n",
        "    def __init__(self, cfg, model):\n",
        "        self.cfg = cfg\n",
        "        self.model = model\n",
        "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1).to(device)\n",
        "        self.std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1).to(device)\n",
        "\n",
        "    def _prep(self, frames_rgb_list):\n",
        "        \"\"\"\n",
        "        frames_rgb_list: list of 32 frames, each HxWx3 in RGB\n",
        "        Returns: [slow, fast] tensors shaped (1,C,T,H,W)\n",
        "        \"\"\"\n",
        "        # Stack to (T,H,W,3) -> (T,C,H,W)\n",
        "        x = torch.from_numpy(np.stack(frames_rgb_list)).permute(0,3,1,2).float() / 255.0  # (T,C,H,W)\n",
        "        # Resize treating T as batch\n",
        "        x = F.interpolate(x, size=self.cfg.side, mode=\"bilinear\", align_corners=False)    # (T,C,224,224)\n",
        "        # Normalize\n",
        "        mean = self.mean.to(device=x.device, dtype=x.dtype)\n",
        "        std  = self.std.to(device=x.device, dtype=x.dtype)\n",
        "        x = (x - mean) / std                                                   # (T,C,224,224)\n",
        "        # (C,T,H,W)\n",
        "        x = x.permute(1,0,2,3)\n",
        "        fast = x.unsqueeze(0).to(device)             # (1,C,32,224,224)\n",
        "        slow = x[:, ::self.cfg.alpha, :, :].unsqueeze(0).to(device)  # stride-4 -> (1,C,8,224,224)\n",
        "        return [slow, fast]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_probs(self, frames_rgb_list):\n",
        "        assert len(frames_rgb_list) == self.cfg.fast_t  # 32\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
        "            inp = self._prep(frames_rgb_list)\n",
        "            logits = self.model(inp)                  # (1, num_classes)\n",
        "            probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "        return probs  # (C,)"
      ],
      "metadata": {
        "id": "PsnwdZrHnVua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_match_video(\n",
        "    cfg,\n",
        "    video_path,\n",
        "    out_path,\n",
        "    yolo_weights=\"yolo11n.pt\", # change to your custom weights if you have them\n",
        "    person_class=0,            # COCO 'person'\n",
        "    det_conf=0.5,\n",
        "    iou=0.5,\n",
        "    pred_thr=0.60,             # minimum prob to show label\n",
        "    cooldown=12                # frames to cool after showing a shot to reduce spam\n",
        "):\n",
        "    # Get video props for the writer\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = max(1.0, cap.get(cv2.CAP_PROP_FPS))\n",
        "    W   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(out_path, fourcc, fps, (W, H))\n",
        "\n",
        "    # Load detector+tracker\n",
        "    yolo = YOLO(yolo_weights)\n",
        "\n",
        "    # Load classifier\n",
        "    clf_model = load_slowfast_classifier(cfg, cfg.best_model_path)\n",
        "    clf = SlowFastPredictor(cfg, clf_model)\n",
        "\n",
        "    # Per-track state\n",
        "    buffers = defaultdict(lambda: deque(maxlen=cfg.fast_t))            # 32-frame RGB crops per track\n",
        "    last_shown_frame = defaultdict(lambda: -99999)                     # cooldown control\n",
        "    hist = defaultdict(lambda: deque(maxlen=5))                        # small temporal smoothing buffer\n",
        "\n",
        "    frame_idx = 0\n",
        "    for res in yolo.track(source=video_path, stream=True, persist=True,\n",
        "                          classes=[person_class], conf=det_conf, iou=iou, verbose=False):\n",
        "        frame_bgr = res.orig_img  # BGR\n",
        "        h, w = frame_bgr.shape[:2]\n",
        "\n",
        "        # If no boxes/ids in this frame, just write it\n",
        "        if res.boxes is None or res.boxes.id is None:\n",
        "            writer.write(frame_bgr)\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        ids = res.boxes.id.int().cpu().numpy()\n",
        "        xyxy = res.boxes.xyxy.int().cpu().numpy()  # (N,4)\n",
        "\n",
        "        to_draw = []  # (x1,y1,x2,y2,label,prob,tid)\n",
        "\n",
        "        for j, tid in enumerate(ids):\n",
        "            x1, y1, x2, y2 = xyxy[j]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w-1, x2), min(h-1, y2)\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            # NEW: enlarge a bit for context (optional, try 1.2–1.4)\n",
        "            x1, y1, x2, y2 = expand_box(x1, y1, x2, y2, scale=1.25, W=w, H=h)\n",
        "\n",
        "            # Crop -> RGB -> letterbox to fixed square\n",
        "            crop = frame_bgr[y1:y2, x1:x2, :]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "            crop_rgb = resize_pad_square(crop_rgb, side=cfg.side)  # now every frame is 224x224\n",
        "\n",
        "            buffers[tid].append(crop_rgb)\n",
        "\n",
        "            label_to_show = None\n",
        "            prob_to_show  = 0.0\n",
        "\n",
        "            # Classify when we have a full 32-frame clip\n",
        "            if len(buffers[tid]) == cfg.fast_t:\n",
        "                probs = clf.predict_probs(list(buffers[tid]))  # (C,)\n",
        "                ci = int(probs.argmax())\n",
        "                pi = float(probs[ci])\n",
        "                hist[tid].append((ci, pi))\n",
        "\n",
        "                # Small smoothing: require at least 2 of the last 3 agreeing + prob >= thr\n",
        "                if len(hist[tid]) >= 3:\n",
        "                    last3 = list(hist[tid])[-3:]\n",
        "                else:\n",
        "                    last3 = list(hist[tid])\n",
        "\n",
        "                # Choose the label with the highest mean prob among last3\n",
        "                if last3:\n",
        "                    classes = [c for c, p in last3 if cfg.labels[c] != \"average_joe\" and p >= pred_thr]\n",
        "                    if classes:\n",
        "                        # pick the most common; break ties by highest avg prob\n",
        "                        uniq = set(classes)\n",
        "                        best_c, best_score = None, -1.0\n",
        "                        for u in uniq:\n",
        "                            avgp = np.mean([p for (c, p) in last3 if c == u])\n",
        "                            score = (classes.count(u), avgp)  # (count, avgp)\n",
        "                            if score > (classes.count(best_c) if best_c is not None else -1, best_score):\n",
        "                                best_c, best_score = u, avgp\n",
        "                        if best_c is not None and (frame_idx - last_shown_frame[tid] >= cooldown):\n",
        "                            label_to_show = cfg.labels[best_c]\n",
        "                            prob_to_show = float(best_score)\n",
        "                            last_shown_frame[tid] = frame_idx\n",
        "\n",
        "            # Queue drawing if we have a confident non-background label\n",
        "            if label_to_show is not None:\n",
        "                to_draw.append((x1, y1, x2, y2, label_to_show, prob_to_show, int(tid)))\n",
        "\n",
        "        # ---- Draw all overlays on this frame ----\n",
        "        for (x1, y1, x2, y2, lab, p, tid) in to_draw:\n",
        "            color = (0, 220, 0)\n",
        "            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n",
        "            txt = f\"#{tid} {lab} {p*100:.1f}%\"\n",
        "            cv2.putText(frame_bgr, txt, (x1, max(20, y1-10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "\n",
        "        writer.write(frame_bgr)\n",
        "        frame_idx += 1\n",
        "\n",
        "    writer.release()\n",
        "    print(f\"Saved annotated video to: {out_path}\")"
      ],
      "metadata": {
        "id": "a36aCc48nXfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_video  = \"/content/drive/MyDrive/FIT3163,3164/SlowFast/01_raw/lcw_ld_2016_short/1/master.mp4\"\n",
        "out_video = \"/content/match_annotated.mp4\"\n",
        "yolo_weights = \"/content/drive/MyDrive/FIT3163,3164/YOLO/my_yolov8_1.pt\"\n",
        "\n",
        "annotate_match_video(cfg, in_video, out_video,\n",
        "                     yolo_weights=yolo_weights,  # swap if you have a better person/badminton model\n",
        "                     det_conf=0.35, iou=0.5,\n",
        "                     pred_thr=0.60, cooldown=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uex745aPnaJK",
        "outputId": "41d184dd-8bf8-4e09-8996-6a67acb9bbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved annotated video to: /content/match_annotated.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utRepM9qoTb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}