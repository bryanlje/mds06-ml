{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h5cTXHLtJGES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7b140b-e93a-4d3e-9f35-93f20d1c1d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.180-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.15-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.180-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.15-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.180 ultralytics-thop-2.0.15\n",
            "Collecting lap\n",
            "  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap) (2.0.2)\n",
            "Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lap\n",
            "Successfully installed lap-0.5.12\n",
            "Collecting torchreid\n",
            "  Downloading torchreid-0.2.5.tar.gz (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchreid\n",
            "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchreid: filename=torchreid-0.2.5-py3-none-any.whl size=144324 sha256=5c29d45c228c40566dd42b20ce4e96b3a2e870641732d537f3e85ee6a00906dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/dc/08/b478469bab07b5ede9e962968ebe3c8961c10c5fc106a6c697\n",
            "Successfully built torchreid\n",
            "Installing collected packages: torchreid\n",
            "Successfully installed torchreid-0.2.5\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110460 sha256=d168298e30050d43c6fa91b659649170ba27d8e5b759d74aa0a99b799f12eea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics\n",
        "!pip install lap  # for Hungarian algorithm\n",
        "!pip install torchreid\n",
        "!pip install opencv-python\n",
        "!pip install pandas\n",
        "!pip install filterpy  # for Kalman filter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-ziMQw9hQwQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bcd37b-5b47-4c80-d277-bea184b530b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFlWfIQXhSqx",
        "outputId": "bfb64d1c-46fb-4259-9b25-cfe8d23a1008"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_cvat_to_training_data(video_dir, output_base_dir):\n",
        "    \"\"\"\n",
        "    Convert CVAT annotations to ReID training format\n",
        "\n",
        "    Args:\n",
        "        video_dir: Directory containing video and gt.txt\n",
        "        output_base_dir: Where to save training crops\n",
        "    \"\"\"\n",
        "\n",
        "    # File paths\n",
        "    video_path = None\n",
        "    gt_path = None\n",
        "\n",
        "    # Find video and annotation files\n",
        "    for file in os.listdir(video_dir):\n",
        "        if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_path = os.path.join(video_dir, file)\n",
        "            # print(f\"found video file {video_path}\")\n",
        "        elif file == 'gt' or file.endswith('gt.txt'):\n",
        "            gt_path = os.path.join(video_dir, 'gt', 'gt.txt')\n",
        "            # print(f\"found annotation file {gt_path}\")\n",
        "\n",
        "    if not video_path:\n",
        "        raise ValueError(f\"No video file found in {video_dir}\")\n",
        "\n",
        "    # if not os.path.exists(gt_path):\n",
        "    #     gt_path = os.path.join(video_dir, 'gt.txt')  # Alternative location\n",
        "\n",
        "    print(f\"Processing video: {video_path}\")\n",
        "    print(f\"Using annotations: {gt_path}\")\n",
        "\n",
        "    # Read annotations\n",
        "    # CVAT MOT format: frame,id,x,y,w,h,conf,class,visibility\n",
        "    try:\n",
        "        annotations = pd.read_csv(gt_path, header=None,\n",
        "                                 names=['frame', 'id', 'x', 'y', 'w', 'h', 'conf', 'class', 'vis'])\n",
        "    except:\n",
        "        print(f\"Error reading {gt_path}. Check file format.\")\n",
        "        return\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video: {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video name for unique player IDs\n",
        "    video_name = os.path.basename(video_path).split('.')[0]\n",
        "\n",
        "    # Create output directories for each unique player\n",
        "    unique_ids = annotations['id'].unique()\n",
        "    player_dirs = {}\n",
        "\n",
        "    for pid in unique_ids:\n",
        "        # Create unique player ID across all videos\n",
        "        global_player_id = f\"{video_name}_player_{pid:03d}\"\n",
        "        player_dir = os.path.join(output_base_dir, global_player_id)\n",
        "        os.makedirs(player_dir, exist_ok=True)\n",
        "        player_dirs[pid] = player_dir\n",
        "        print(f\"Created directory: {player_dir}\")\n",
        "\n",
        "    # Process frames\n",
        "    frame_count = 1\n",
        "    crops_extracted = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Get annotations for current frame\n",
        "        frame_annotations = annotations[annotations['frame'] == frame_count]\n",
        "\n",
        "        for _, row in frame_annotations.iterrows():\n",
        "            x, y, w, h = int(row['x']), int(row['y']), int(row['w']), int(row['h'])\n",
        "            player_id = int(row['id'])\n",
        "\n",
        "            # Validate crop boundaries\n",
        "            if x >= 0 and y >= 0 and x + w <= frame.shape[1] and y + h <= frame.shape[0]:\n",
        "                # Extract player crop\n",
        "                crop = frame[y:y+h, x:x+w]\n",
        "\n",
        "                # Skip very small crops\n",
        "                if crop.shape[0] > 30 and crop.shape[1] > 20:\n",
        "                    # Save crop\n",
        "                    crop_filename = f\"frame_{frame_count:06d}.jpg\"\n",
        "                    crop_path = os.path.join(player_dirs[player_id], crop_filename)\n",
        "                    cv2.imwrite(crop_path, crop)\n",
        "                    crops_extracted += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Progress indicator\n",
        "        if frame_count % 100 == 0:\n",
        "            print(f\"Processed {frame_count} frames, extracted {crops_extracted} crops\")\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Completed! Total crops extracted: {crops_extracted}\")\n",
        "\n",
        "    # Print summary\n",
        "    for pid, directory in player_dirs.items():\n",
        "        crop_count = len([f for f in os.listdir(directory) if f.endswith('.jpg')])\n",
        "        print(f\"Player {pid}: {crop_count} crops in {directory}\")"
      ],
      "metadata": {
        "id": "eFN8wSdx1XTD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_videos(reid_base_dir=\"/content/drive/MyDrive/FIT3163,3164/REID/train\", training_output_dir=\"/content/reid_training_data\"):\n",
        "    \"\"\"Process all CVAT annotated videos\"\"\"\n",
        "\n",
        "    # Directory structure after CVAT export:\n",
        "    # /content/drive/MyDrive/badminton_annotations/\n",
        "    # ├── video1/\n",
        "    # │   ├── video1.mp4\n",
        "    # │   └── gt/gt.txt\n",
        "    # ├── video2/\n",
        "    # │   ├── video2.mp4\n",
        "    # │   └── gt/gt.txt\n",
        "\n",
        "    # Process each video directory\n",
        "    for video_folder in os.listdir(reid_base_dir):\n",
        "        video_dir_path = os.path.join(reid_base_dir, video_folder)\n",
        "        if os.path.isdir(video_dir_path):\n",
        "            print(f\"\\nProcessing {video_dir_path}...\")\n",
        "            process_cvat_to_training_data(video_dir_path, training_output_dir)\n",
        "        else:\n",
        "            print(f\"{video_dir_path} not found\")"
      ],
      "metadata": {
        "id": "LvgjViY6kvhv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_badminton_reid_model(register_dataset=False,\n",
        "                               dataset_name='badminton',\n",
        "                               data_dir=\"/content/reid_training_data\",\n",
        "                               test_identity_ratio=0.2,\n",
        "                               min_test_ids=4,\n",
        "                               query_per_id=1,\n",
        "                               per_video_as_camid=True,\n",
        "                               seed=42):\n",
        "    \"\"\"Train custom ReID model on badminton data.\"\"\"\n",
        "    import os\n",
        "    import glob\n",
        "    import random\n",
        "    import numpy as np\n",
        "    from collections import defaultdict\n",
        "    import torch\n",
        "    import torchreid\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 0) Check data root\n",
        "    if not os.path.isdir(data_dir):\n",
        "        print(f\"Error: Directory '{data_dir}' not found. Please ensure your data is at this path.\")\n",
        "        return\n",
        "\n",
        "    # 1) Collect identity folders (skip hidden/system dirs)\n",
        "    all_dirs = [\n",
        "        d for d in os.listdir(data_dir)\n",
        "        if os.path.isdir(os.path.join(data_dir, d))\n",
        "        and not d.startswith('.')\n",
        "        and d.lower() != 'market1501'  # avoid treating an external dataset dir as an ID\n",
        "        and d != '__MACOSX'\n",
        "        and d != 'lost+found'\n",
        "    ]\n",
        "    if len(all_dirs) == 0:\n",
        "        print(f\"No identity folders found under {data_dir}\")\n",
        "        return\n",
        "\n",
        "    # 2) Parse identity folders into (img_path, pid, camid)\n",
        "    ALLOWED_EXTS = ('.jpg', '.jpeg', '.png', '.bmp')\n",
        "    pid_to_id = {}            # map identity_name -> global int pid\n",
        "    camid_by_video = {}       # map video_name -> camid\n",
        "    next_pid = 0\n",
        "    next_cam = 0\n",
        "    pid_to_imgs = defaultdict(list)  # pid -> list[(path, pid, camid)]\n",
        "    total_images = 0\n",
        "\n",
        "    def infer_video_name(identity_folder_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Heuristic to derive 'video name' from folder name to assign camid.\n",
        "        e.g. 'bryan_1_player_003' -> 'bryan_1'\n",
        "        Fallback: whole folder name if pattern not present.\n",
        "        \"\"\"\n",
        "        if '_player_' in identity_folder_name:\n",
        "            return identity_folder_name.split('_player_')[0]\n",
        "        if '#p' in identity_folder_name.lower():\n",
        "            return identity_folder_name.split('#')[0]\n",
        "        return identity_folder_name  # fallback\n",
        "\n",
        "    for ident in sorted(all_dirs):\n",
        "        # Avoid hidden/temporary folders inside id dir (handled below)\n",
        "        identity_dir = os.path.join(data_dir, ident)\n",
        "\n",
        "        # Assign PID\n",
        "        if ident not in pid_to_id:\n",
        "            pid_to_id[ident] = next_pid\n",
        "            next_pid += 1\n",
        "        pid = pid_to_id[ident]\n",
        "\n",
        "        # CamID: per video (recommended), or a single cam=0 for all\n",
        "        video_name = infer_video_name(ident)\n",
        "        if per_video_as_camid:\n",
        "            if video_name not in camid_by_video:\n",
        "                camid_by_video[video_name] = next_cam\n",
        "                next_cam += 1\n",
        "            camid = camid_by_video[video_name]\n",
        "        else:\n",
        "            camid = 0\n",
        "\n",
        "        # Gather images\n",
        "        img_paths = []\n",
        "        for fn in os.listdir(identity_dir):\n",
        "            if fn.startswith('.') or fn == '.ipynb_checkpoints':\n",
        "                continue\n",
        "            if fn.lower().endswith(ALLOWED_EXTS):\n",
        "                img_paths.append(os.path.join(identity_dir, fn))\n",
        "\n",
        "        if len(img_paths) == 0:\n",
        "            print(f\"Warning: identity '{ident}' has no images, skipping.\")\n",
        "            continue\n",
        "\n",
        "        for p in sorted(img_paths):\n",
        "            pid_to_imgs[p] = 1  # ensure path uniqueness\n",
        "        for p in sorted(img_paths):\n",
        "            pid_to_imgs[pid].append((p, pid, camid))\n",
        "            total_images += 1\n",
        "\n",
        "    total_identities = len(pid_to_id)\n",
        "    print(f\"Training data summary:\")\n",
        "    print(f\"- Total identities: {total_identities}:\")\n",
        "    for i, identity in enumerate(pid_to_id):\n",
        "        print(f\"{i}. {identity}\")\n",
        "    print(f\"\\n- Total images: {total_images}\")\n",
        "    if total_identities > 0:\n",
        "        print(f\"- Average images per identity: {total_images/total_identities:.1f}\")\n",
        "    if total_identities < 10:\n",
        "        print(\"Warning: Less than 10 identities. Consider collecting more data.\")\n",
        "\n",
        "    # 3) Split identities into train vs test (query/gallery from test IDs only)\n",
        "    all_pids = list(pid_to_imgs.keys())\n",
        "    # pid_to_imgs keys contain ints (pids) and also we inserted a dict of paths above by mistake—fix:\n",
        "    # Ensure we only keep integer keys (the real pid keys)\n",
        "    all_pids = [k for k in pid_to_imgs.keys() if isinstance(k, int)]\n",
        "    random.shuffle(all_pids)\n",
        "\n",
        "    num_test_ids = max(min_test_ids, int(round(len(all_pids) * test_identity_ratio)))\n",
        "    num_test_ids = min(num_test_ids, len(all_pids))  # guard\n",
        "    test_pids = set(all_pids[:num_test_ids])\n",
        "    train_pids = set(all_pids[num_test_ids:])\n",
        "\n",
        "    # Build train/query/gallery lists\n",
        "    train_set_data = []\n",
        "    query_data = []\n",
        "    gallery_data = []\n",
        "\n",
        "    # Training set (will be relabeled to 0..N_train-1)\n",
        "    for pid in sorted(train_pids):\n",
        "        train_set_data.extend(pid_to_imgs[pid])\n",
        "\n",
        "    # Relabel training PIDs to be contiguous starting at 0\n",
        "    unique_train_pids = sorted({pid for _, pid, _ in train_set_data})\n",
        "    pid_remap = {pid: i for i, pid in enumerate(unique_train_pids)}\n",
        "    train_set_data = [(p, pid_remap[pid], cam) for (p, pid, cam) in train_set_data]\n",
        "\n",
        "    QUERY_CAM_OFFSET = 1000  # force query camids to differ from gallery camids\n",
        "\n",
        "    # Test set: for each pid, pick query_per_id images for QUERY (camid+offset), rest go to GALLERY (original camid)\n",
        "    moved_back_to_train = []\n",
        "    for pid in sorted(test_pids):\n",
        "        items = pid_to_imgs[pid]  # list of (path, pid, camid)\n",
        "        if len(items) < (query_per_id + 1):  # need >= 2 images per pid for eval\n",
        "            moved_back_to_train.append(pid)\n",
        "            continue\n",
        "\n",
        "        idxs = list(range(len(items)))\n",
        "        random.shuffle(idxs)\n",
        "        q_idxs = set(idxs[:query_per_id])  # choose query images\n",
        "\n",
        "        for i, (p, pid_i, cam) in enumerate(items):\n",
        "            if i in q_idxs:\n",
        "                # IMPORTANT: make query camid different from gallery camid\n",
        "                query_data.append((p, pid_i, cam + QUERY_CAM_OFFSET))\n",
        "            else:\n",
        "                gallery_data.append((p, pid_i, cam))\n",
        "\n",
        "    # If some IDs had too few images, move them to train (and relabel)\n",
        "    if moved_back_to_train:\n",
        "        print(f\"Info: {len(moved_back_to_train)} IDs had <2 images; moving them to train.\")\n",
        "        for pid in moved_back_to_train:\n",
        "            # add to train and relabel\n",
        "            for (p, _, c) in pid_to_imgs[pid]:\n",
        "                train_set_data.append((p, None, c))  # placeholder, relabel next\n",
        "        # relabel again\n",
        "        unique_train_pids = sorted({pid for _, pid, _ in train_set_data if pid is not None})\n",
        "        # Add newly moved pids at the end\n",
        "        next_id = len(unique_train_pids)\n",
        "        # Map old pid -> new contiguous id\n",
        "        pid_remap = {}\n",
        "        i = 0\n",
        "        for pid in sorted({pid for _, pid, _ in train_set_data if pid is not None}):\n",
        "            pid_remap[pid] = i\n",
        "            i += 1\n",
        "        # assign new IDs for placeholders (None) in the order they appear by original pid\n",
        "        for idx, (p, pid, c) in enumerate(train_set_data):\n",
        "            if pid is None:\n",
        "                # get original pid from path by searching pid_to_imgs\n",
        "                # (slow path but safe; could store alongside above if needed)\n",
        "                # We'll infer pid from folder name: robust, since path contains identity dir name.\n",
        "                ident_dir = os.path.basename(os.path.dirname(p))\n",
        "                orig_pid = pid_to_id.get(ident_dir, None)\n",
        "                if orig_pid is None:\n",
        "                    # fallback: put into class 0\n",
        "                    new_id = 0\n",
        "                else:\n",
        "                    if orig_pid not in pid_remap:\n",
        "                        pid_remap[orig_pid] = i\n",
        "                        i += 1\n",
        "                    new_id = pid_remap[orig_pid]\n",
        "                train_set_data[idx] = (p, new_id, c)\n",
        "\n",
        "    # Sanity checks\n",
        "    def uniq_pids(lst): return np.unique([it[1] for it in lst]) if len(lst) else np.array([])\n",
        "    print(\"\\nData Split Summary:\")\n",
        "    print(f\"- Training identities: {len(uniq_pids(train_set_data))}\\n- {uniq_pids(train_set_data)}\")\n",
        "    print(f\"- Query identities: {len(uniq_pids(query_data))}\\n- {uniq_pids(query_data)}\")\n",
        "    print(f\"- Gallery identities: {len(uniq_pids(gallery_data))}\\n- {uniq_pids(gallery_data)}\")\n",
        "\n",
        "    if any(pid not in [g[1] for g in gallery_data] for pid in [q[1] for q in query_data]):\n",
        "        raise AssertionError(\"The query set contains identities not found in the gallery set.\")\n",
        "\n",
        "    # 4) Define custom dataset class\n",
        "    class BadmintonReID(torchreid.data.ImageDataset):\n",
        "        def __init__(self, root='', **kwargs):\n",
        "            self.train = train_set_data\n",
        "            self.query = query_data\n",
        "            self.gallery = gallery_data\n",
        "            super(BadmintonReID, self).__init__(self.train, self.query, self.gallery, **kwargs)\n",
        "\n",
        "    # Register once\n",
        "    if register_dataset:\n",
        "        torchreid.data.register_image_dataset(dataset_name, BadmintonReID)\n",
        "\n",
        "    # Choose a safe num_instances for RandomIdentitySampler\n",
        "    imgs_per_train_pid = defaultdict(int)\n",
        "    for _, pid, _ in train_set_data:\n",
        "        imgs_per_train_pid[pid] += 1\n",
        "    min_imgs_per_pid = min(imgs_per_train_pid.values()) if imgs_per_train_pid else 1\n",
        "    num_instances = max(2, min(4, min_imgs_per_pid))  # cap at 4, at least 2\n",
        "\n",
        "    # IMPORTANT: root should be a parent folder (not the custom dataset dir) if you also want Market1501.\n",
        "    # Start with only your dataset; add Market1501 later after verifying pipeline.\n",
        "    data_root = \"/content\"  # a common root\n",
        "    # Make sure torchreid can find your custom dataset by name:\n",
        "    # It will instantiate BadmintonReID (which uses the prebuilt lists above).\n",
        "    datamanager = torchreid.data.datamanager.ImageDataManager(\n",
        "        root=data_root,\n",
        "        sources=[dataset_name],          # start with custom only; add 'market1501' when ready\n",
        "        targets=dataset_name,\n",
        "        height=256,\n",
        "        width=256,\n",
        "        transforms=['random_flip', 'random_crop', 'color_jitter', 'normalize'],\n",
        "        num_instances=num_instances,\n",
        "        workers=2,\n",
        "        train_sampler='RandomIdentitySampler',\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataManager initialized:\")\n",
        "    print(f\"- Training identities: {datamanager.num_train_pids}\")\n",
        "\n",
        "    # Build model (use softmax head so the TripletEngine can combine CE + Triplet)\n",
        "    model = torchreid.models.build_model(\n",
        "        name='osnet_x1_0',\n",
        "        num_classes=datamanager.num_train_pids,\n",
        "        loss='triplet',\n",
        "        pretrained=True,\n",
        "        use_gpu=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        print(\"Moving model to GPU...\")\n",
        "    else:\n",
        "        print(\"Warning: Using CPU for model...\")\n",
        "\n",
        "    # Optimizer & scheduler\n",
        "    optimizer = torchreid.optim.build_optimizer(\n",
        "        model,\n",
        "        optim='adam',\n",
        "        lr=3e-4,\n",
        "        weight_decay=5e-4\n",
        "    )\n",
        "    scheduler = torchreid.optim.build_lr_scheduler(\n",
        "        optimizer,\n",
        "        lr_scheduler='single_step',\n",
        "        stepsize=[20, 40]\n",
        "    )\n",
        "\n",
        "    engine = torchreid.engine.ImageTripletEngine(\n",
        "        datamanager,\n",
        "        model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        margin=0.3,\n",
        "        weight_t=1.0,\n",
        "        weight_x=1.0,\n",
        "        label_smooth=True\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    output_dir = \"/content/badminton_reid_model\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    engine.run(\n",
        "        save_dir=output_dir,\n",
        "        max_epoch=1,       # bump this once pipeline works (you had 1)\n",
        "        eval_freq=10,\n",
        "        print_freq=10,\n",
        "        test_only=False,\n",
        "    )\n",
        "\n",
        "    # Evaluate + visualize rank\n",
        "    print(\"\\nPerforming visual ranking...\")\n",
        "    engine.run(\n",
        "        save_dir=output_dir,\n",
        "        max_epoch=0,\n",
        "        test_only=True,\n",
        "        visrank=True,\n",
        "        visrank_topk=5,\n",
        "        use_metric_cuhk03=False,\n",
        "        ranks=[1, 5, 10],\n",
        "        rerank=False\n",
        "    )\n",
        "\n",
        "    print(f\"Training completed! Model saved to: {output_dir}\")\n",
        "    return output_dir\n"
      ],
      "metadata": {
        "id": "AG_YyRz24BWT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_custom_strongsort_config():\n",
        "    \"\"\"Create custom StrongSORT configuration for badminton\"\"\"\n",
        "\n",
        "    import yaml\n",
        "\n",
        "    # Custom configuration for badminton tracking\n",
        "    strongsort_config = {\n",
        "        'tracker_type': 'strongsort',\n",
        "        'reid_weights': '/content/badminton_reid_model/model.pth.tar',\n",
        "        'max_dist': 0.2,           # Maximum distance for matching\n",
        "        'min_confidence': 0.3,      # Minimum detection confidence\n",
        "        'nms_max_overlap': 0.7,     # Non-maximum suppression\n",
        "        'max_iou_distance': 0.7,    # Maximum IoU distance\n",
        "        'max_age': 30,              # Frames to keep lost tracks\n",
        "        'n_init': 3,                # Frames to confirm track\n",
        "        'nn_budget': 100,           # Feature budget for ReID\n",
        "        'ema_alpha': 0.9,           # Exponential moving average for features\n",
        "    }\n",
        "\n",
        "    # Save configuration\n",
        "    config_path = \"/content/badminton_strongsort.yaml\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(strongsort_config, f)\n",
        "\n",
        "    print(f\"Custom configuration saved to: {config_path}\")\n",
        "    return config_path"
      ],
      "metadata": {
        "id": "WG7a8VtAQKgl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_custom_strongsort(yolo_model_path, reid_model_path, test_video_path):\n",
        "    \"\"\"Run StrongSORT with custom trained ReID model\"\"\"\n",
        "\n",
        "    # Setup custom configuration\n",
        "    config_path = setup_custom_strongsort_config()\n",
        "\n",
        "    # Run tracking with custom ReID model\n",
        "    output_dir = \"runs/track/custom_badminton\"\n",
        "\n",
        "    !python track.py \\\n",
        "        --yolo-model {yolo_model_path} \\\n",
        "        --reid-model {reid_model_path} \\\n",
        "        --tracking-method strongsort \\\n",
        "        --source {test_video_path} \\\n",
        "        --imgsz 640 \\\n",
        "        --conf-thres 0.4 \\\n",
        "        --iou-thres 0.5 \\\n",
        "        --max-det 300 \\\n",
        "        --device 0 \\\n",
        "        --save \\\n",
        "        --save-txt \\\n",
        "        --save-conf \\\n",
        "        --project runs/track \\\n",
        "        --name custom_badminton \\\n",
        "        --exist-ok\n",
        "\n",
        "    print(f\"Custom tracking completed! Results saved to: {output_dir}\")"
      ],
      "metadata": {
        "id": "NLpJRdgs8fdE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Execute the complete pipeline step by step\"\"\"\n",
        "\n",
        "print(\"=== BADMINTON PLAYER TRACKING PIPELINE ===\\n\")\n",
        "\n",
        "# Step 1: Verify YOLOv8 model\n",
        "yolo_model_path = \"/content/drive/MyDrive/FIT3163,3164/YOLO/dataset_2/runs/detect/train5/weights/best.pt\""
      ],
      "metadata": {
        "id": "iuWpEgqE8tGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a7a74c-8c03-4764-d86d-2a791c21657c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BADMINTON PLAYER TRACKING PIPELINE ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Process annotations\n",
        "print(\"\\n🔄 Step 2: Processing annotations...\")\n",
        "process_all_videos(reid_base_dir=\"/content/drive/MyDrive/FIT3163,3164/REID/train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-xp2q7cQf-R",
        "outputId": "6b070550-bb85-4f2e-8db6-ea605375a7f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Step 2: Processing annotations...\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/phua_1...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_1/phua_vid_1.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_1/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/phua_vid_1_player_001\n",
            "Created directory: /content/reid_training_data/phua_vid_1_player_002\n",
            "Created directory: /content/reid_training_data/phua_vid_1_player_003\n",
            "Created directory: /content/reid_training_data/phua_vid_1_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Completed! Total crops extracted: 1076\n",
            "Player 1: 269 crops in /content/reid_training_data/phua_vid_1_player_001\n",
            "Player 2: 269 crops in /content/reid_training_data/phua_vid_1_player_002\n",
            "Player 3: 269 crops in /content/reid_training_data/phua_vid_1_player_003\n",
            "Player 4: 269 crops in /content/reid_training_data/phua_vid_1_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/phua_2...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_2/phua_vid_2.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_2/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/phua_vid_2_player_001\n",
            "Created directory: /content/reid_training_data/phua_vid_2_player_002\n",
            "Created directory: /content/reid_training_data/phua_vid_2_player_003\n",
            "Created directory: /content/reid_training_data/phua_vid_2_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 500 crops\n",
            "Processed 300 frames, extracted 500 crops\n",
            "Completed! Total crops extracted: 500\n",
            "Player 1: 125 crops in /content/reid_training_data/phua_vid_2_player_001\n",
            "Player 2: 125 crops in /content/reid_training_data/phua_vid_2_player_002\n",
            "Player 3: 125 crops in /content/reid_training_data/phua_vid_2_player_003\n",
            "Player 4: 125 crops in /content/reid_training_data/phua_vid_2_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/bryan_1...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/bryan_1/bryan_vid_1.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/bryan_1/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/bryan_vid_1_player_001\n",
            "Created directory: /content/reid_training_data/bryan_vid_1_player_002\n",
            "Created directory: /content/reid_training_data/bryan_vid_1_player_003\n",
            "Created directory: /content/reid_training_data/bryan_vid_1_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1000 crops\n",
            "Completed! Total crops extracted: 1000\n",
            "Player 1: 250 crops in /content/reid_training_data/bryan_vid_1_player_001\n",
            "Player 2: 250 crops in /content/reid_training_data/bryan_vid_1_player_002\n",
            "Player 3: 250 crops in /content/reid_training_data/bryan_vid_1_player_003\n",
            "Player 4: 250 crops in /content/reid_training_data/bryan_vid_1_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_1...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_1/ting_vid_1.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_1/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_1_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_1_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_1_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_1_player_004\n",
            "Processed 100 frames, extracted 393 crops\n",
            "Processed 200 frames, extracted 793 crops\n",
            "Processed 300 frames, extracted 1193 crops\n",
            "Processed 400 frames, extracted 1593 crops\n",
            "Processed 500 frames, extracted 1993 crops\n",
            "Processed 600 frames, extracted 2393 crops\n",
            "Completed! Total crops extracted: 2673\n",
            "Player 1: 669 crops in /content/reid_training_data/ting_vid_1_player_001\n",
            "Player 2: 669 crops in /content/reid_training_data/ting_vid_1_player_002\n",
            "Player 3: 669 crops in /content/reid_training_data/ting_vid_1_player_003\n",
            "Player 4: 666 crops in /content/reid_training_data/ting_vid_1_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_2...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_2/ting_vid_2.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_2/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_2_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_2_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_2_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_2_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Processed 500 frames, extracted 1996 crops\n",
            "Completed! Total crops extracted: 2216\n",
            "Player 1: 554 crops in /content/reid_training_data/ting_vid_2_player_001\n",
            "Player 2: 554 crops in /content/reid_training_data/ting_vid_2_player_002\n",
            "Player 3: 554 crops in /content/reid_training_data/ting_vid_2_player_003\n",
            "Player 4: 554 crops in /content/reid_training_data/ting_vid_2_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_3...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_3/ting_vid_3.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_3/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_3_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_3_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_3_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_3_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Processed 500 frames, extracted 1996 crops\n",
            "Processed 600 frames, extracted 2396 crops\n",
            "Completed! Total crops extracted: 2404\n",
            "Player 1: 601 crops in /content/reid_training_data/ting_vid_3_player_001\n",
            "Player 2: 601 crops in /content/reid_training_data/ting_vid_3_player_002\n",
            "Player 3: 601 crops in /content/reid_training_data/ting_vid_3_player_003\n",
            "Player 4: 601 crops in /content/reid_training_data/ting_vid_3_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_4...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_4/ting_vid_4.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_4/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_4_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_4_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_4_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_4_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Completed! Total crops extracted: 1932\n",
            "Player 1: 483 crops in /content/reid_training_data/ting_vid_4_player_001\n",
            "Player 2: 483 crops in /content/reid_training_data/ting_vid_4_player_002\n",
            "Player 3: 483 crops in /content/reid_training_data/ting_vid_4_player_003\n",
            "Player 4: 483 crops in /content/reid_training_data/ting_vid_4_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_9...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_9/ting_vid_9.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_9/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_9_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_9_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_9_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_9_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Processed 500 frames, extracted 1996 crops\n",
            "Processed 600 frames, extracted 2396 crops\n",
            "Completed! Total crops extracted: 2440\n",
            "Player 1: 610 crops in /content/reid_training_data/ting_vid_9_player_001\n",
            "Player 2: 610 crops in /content/reid_training_data/ting_vid_9_player_002\n",
            "Player 3: 610 crops in /content/reid_training_data/ting_vid_9_player_003\n",
            "Player 4: 610 crops in /content/reid_training_data/ting_vid_9_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_11...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_11/ting_vid_11.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_11/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_11_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_11_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_11_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_11_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Processed 500 frames, extracted 1996 crops\n",
            "Processed 600 frames, extracted 2396 crops\n",
            "Completed! Total crops extracted: 2576\n",
            "Player 1: 644 crops in /content/reid_training_data/ting_vid_11_player_001\n",
            "Player 2: 644 crops in /content/reid_training_data/ting_vid_11_player_002\n",
            "Player 3: 644 crops in /content/reid_training_data/ting_vid_11_player_003\n",
            "Player 4: 644 crops in /content/reid_training_data/ting_vid_11_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_5...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_5/ting_vid_5.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_5/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_5_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_5_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_5_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_5_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Completed! Total crops extracted: 1836\n",
            "Player 1: 459 crops in /content/reid_training_data/ting_vid_5_player_001\n",
            "Player 2: 459 crops in /content/reid_training_data/ting_vid_5_player_002\n",
            "Player 3: 459 crops in /content/reid_training_data/ting_vid_5_player_003\n",
            "Player 4: 459 crops in /content/reid_training_data/ting_vid_5_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_10...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_10/ting_vid_10.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_10/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_10_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_10_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_10_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_10_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Completed! Total crops extracted: 664\n",
            "Player 1: 166 crops in /content/reid_training_data/ting_vid_10_player_001\n",
            "Player 2: 166 crops in /content/reid_training_data/ting_vid_10_player_002\n",
            "Player 3: 166 crops in /content/reid_training_data/ting_vid_10_player_003\n",
            "Player 4: 166 crops in /content/reid_training_data/ting_vid_10_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_6...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_6/ting_vid_6.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_6/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_6_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_6_player_002\n",
            "Processed 100 frames, extracted 198 crops\n",
            "Processed 200 frames, extracted 398 crops\n",
            "Processed 300 frames, extracted 598 crops\n",
            "Processed 400 frames, extracted 798 crops\n",
            "Processed 500 frames, extracted 998 crops\n",
            "Completed! Total crops extracted: 1060\n",
            "Player 1: 530 crops in /content/reid_training_data/ting_vid_6_player_001\n",
            "Player 2: 530 crops in /content/reid_training_data/ting_vid_6_player_002\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_7...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_7/ting_vid_7.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_7/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_7_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_7_player_002\n",
            "Created directory: /content/reid_training_data/ting_vid_7_player_003\n",
            "Created directory: /content/reid_training_data/ting_vid_7_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Completed! Total crops extracted: 1684\n",
            "Player 1: 421 crops in /content/reid_training_data/ting_vid_7_player_001\n",
            "Player 2: 421 crops in /content/reid_training_data/ting_vid_7_player_002\n",
            "Player 3: 421 crops in /content/reid_training_data/ting_vid_7_player_003\n",
            "Player 4: 421 crops in /content/reid_training_data/ting_vid_7_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/ting_8...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_8/ting_vid_8.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/ting_8/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/ting_vid_8_player_001\n",
            "Created directory: /content/reid_training_data/ting_vid_8_player_002\n",
            "Processed 100 frames, extracted 198 crops\n",
            "Processed 200 frames, extracted 398 crops\n",
            "Processed 300 frames, extracted 598 crops\n",
            "Processed 400 frames, extracted 798 crops\n",
            "Processed 500 frames, extracted 998 crops\n",
            "Processed 600 frames, extracted 1198 crops\n",
            "Completed! Total crops extracted: 1322\n",
            "Player 1: 661 crops in /content/reid_training_data/ting_vid_8_player_001\n",
            "Player 2: 661 crops in /content/reid_training_data/ting_vid_8_player_002\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/phua_3...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_3/phua_vid_3.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_3/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/phua_vid_3_player_001\n",
            "Created directory: /content/reid_training_data/phua_vid_3_player_002\n",
            "Created directory: /content/reid_training_data/phua_vid_3_player_003\n",
            "Created directory: /content/reid_training_data/phua_vid_3_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Completed! Total crops extracted: 724\n",
            "Player 1: 181 crops in /content/reid_training_data/phua_vid_3_player_001\n",
            "Player 2: 181 crops in /content/reid_training_data/phua_vid_3_player_002\n",
            "Player 3: 181 crops in /content/reid_training_data/phua_vid_3_player_003\n",
            "Player 4: 181 crops in /content/reid_training_data/phua_vid_3_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/phua_4...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_4/phua_vid_4.mp4\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/phua_4/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/phua_vid_4_player_001\n",
            "Created directory: /content/reid_training_data/phua_vid_4_player_002\n",
            "Created directory: /content/reid_training_data/phua_vid_4_player_003\n",
            "Created directory: /content/reid_training_data/phua_vid_4_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Completed! Total crops extracted: 624\n",
            "Player 1: 156 crops in /content/reid_training_data/phua_vid_4_player_001\n",
            "Player 2: 156 crops in /content/reid_training_data/phua_vid_4_player_002\n",
            "Player 3: 156 crops in /content/reid_training_data/phua_vid_4_player_003\n",
            "Player 4: 156 crops in /content/reid_training_data/phua_vid_4_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_1...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_1/thom_vid_1.mov\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_1/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/thom_vid_1_player_001\n",
            "Created directory: /content/reid_training_data/thom_vid_1_player_002\n",
            "Created directory: /content/reid_training_data/thom_vid_1_player_003\n",
            "Created directory: /content/reid_training_data/thom_vid_1_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Completed! Total crops extracted: 1272\n",
            "Player 1: 318 crops in /content/reid_training_data/thom_vid_1_player_001\n",
            "Player 2: 318 crops in /content/reid_training_data/thom_vid_1_player_002\n",
            "Player 3: 318 crops in /content/reid_training_data/thom_vid_1_player_003\n",
            "Player 4: 318 crops in /content/reid_training_data/thom_vid_1_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_2...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_2/thom_vid_2.mov\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_2/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/thom_vid_2_player_001\n",
            "Created directory: /content/reid_training_data/thom_vid_2_player_002\n",
            "Created directory: /content/reid_training_data/thom_vid_2_player_003\n",
            "Created directory: /content/reid_training_data/thom_vid_2_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Completed! Total crops extracted: 1396\n",
            "Player 1: 349 crops in /content/reid_training_data/thom_vid_2_player_001\n",
            "Player 2: 349 crops in /content/reid_training_data/thom_vid_2_player_002\n",
            "Player 3: 349 crops in /content/reid_training_data/thom_vid_2_player_003\n",
            "Player 4: 349 crops in /content/reid_training_data/thom_vid_2_player_004\n",
            "\n",
            "Processing /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_3...\n",
            "Processing video: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_3/thom_vid_3.mov\n",
            "Using annotations: /content/drive/MyDrive/FIT3163,3164/REID/train/thomas_3/gt/gt.txt\n",
            "Created directory: /content/reid_training_data/thom_vid_3_player_001\n",
            "Created directory: /content/reid_training_data/thom_vid_3_player_002\n",
            "Created directory: /content/reid_training_data/thom_vid_3_player_003\n",
            "Created directory: /content/reid_training_data/thom_vid_3_player_004\n",
            "Processed 100 frames, extracted 396 crops\n",
            "Processed 200 frames, extracted 796 crops\n",
            "Processed 300 frames, extracted 1196 crops\n",
            "Processed 400 frames, extracted 1596 crops\n",
            "Completed! Total crops extracted: 1804\n",
            "Player 1: 451 crops in /content/reid_training_data/thom_vid_3_player_001\n",
            "Player 2: 451 crops in /content/reid_training_data/thom_vid_3_player_002\n",
            "Player 3: 451 crops in /content/reid_training_data/thom_vid_3_player_003\n",
            "Player 4: 451 crops in /content/reid_training_data/thom_vid_3_player_004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train ReID model\n",
        "print(\"\\n🧠 Step 3: Training ReID model...\")\n",
        "reid_model_dir = train_badminton_reid_model(\n",
        "    register_dataset=True,\n",
        "    dataset_name='badminton1'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXllkJl_Qhyz",
        "outputId": "1dcf3071-f4ae-4346-f23e-b90e430f93ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Step 3: Training ReID model...\n",
            "Training data summary:\n",
            "- Total identities: 72:\n",
            "0. bryan_vid_1_player_001\n",
            "1. bryan_vid_1_player_002\n",
            "2. bryan_vid_1_player_003\n",
            "3. bryan_vid_1_player_004\n",
            "4. phua_vid_1_player_001\n",
            "5. phua_vid_1_player_002\n",
            "6. phua_vid_1_player_003\n",
            "7. phua_vid_1_player_004\n",
            "8. phua_vid_2_player_001\n",
            "9. phua_vid_2_player_002\n",
            "10. phua_vid_2_player_003\n",
            "11. phua_vid_2_player_004\n",
            "12. phua_vid_3_player_001\n",
            "13. phua_vid_3_player_002\n",
            "14. phua_vid_3_player_003\n",
            "15. phua_vid_3_player_004\n",
            "16. phua_vid_4_player_001\n",
            "17. phua_vid_4_player_002\n",
            "18. phua_vid_4_player_003\n",
            "19. phua_vid_4_player_004\n",
            "20. thom_vid_1_player_001\n",
            "21. thom_vid_1_player_002\n",
            "22. thom_vid_1_player_003\n",
            "23. thom_vid_1_player_004\n",
            "24. thom_vid_2_player_001\n",
            "25. thom_vid_2_player_002\n",
            "26. thom_vid_2_player_003\n",
            "27. thom_vid_2_player_004\n",
            "28. thom_vid_3_player_001\n",
            "29. thom_vid_3_player_002\n",
            "30. thom_vid_3_player_003\n",
            "31. thom_vid_3_player_004\n",
            "32. ting_vid_10_player_001\n",
            "33. ting_vid_10_player_002\n",
            "34. ting_vid_10_player_003\n",
            "35. ting_vid_10_player_004\n",
            "36. ting_vid_11_player_001\n",
            "37. ting_vid_11_player_002\n",
            "38. ting_vid_11_player_003\n",
            "39. ting_vid_11_player_004\n",
            "40. ting_vid_1_player_001\n",
            "41. ting_vid_1_player_002\n",
            "42. ting_vid_1_player_003\n",
            "43. ting_vid_1_player_004\n",
            "44. ting_vid_2_player_001\n",
            "45. ting_vid_2_player_002\n",
            "46. ting_vid_2_player_003\n",
            "47. ting_vid_2_player_004\n",
            "48. ting_vid_3_player_001\n",
            "49. ting_vid_3_player_002\n",
            "50. ting_vid_3_player_003\n",
            "51. ting_vid_3_player_004\n",
            "52. ting_vid_4_player_001\n",
            "53. ting_vid_4_player_002\n",
            "54. ting_vid_4_player_003\n",
            "55. ting_vid_4_player_004\n",
            "56. ting_vid_5_player_001\n",
            "57. ting_vid_5_player_002\n",
            "58. ting_vid_5_player_003\n",
            "59. ting_vid_5_player_004\n",
            "60. ting_vid_6_player_001\n",
            "61. ting_vid_6_player_002\n",
            "62. ting_vid_7_player_001\n",
            "63. ting_vid_7_player_002\n",
            "64. ting_vid_7_player_003\n",
            "65. ting_vid_7_player_004\n",
            "66. ting_vid_8_player_001\n",
            "67. ting_vid_8_player_002\n",
            "68. ting_vid_9_player_001\n",
            "69. ting_vid_9_player_002\n",
            "70. ting_vid_9_player_003\n",
            "71. ting_vid_9_player_004\n",
            "\n",
            "- Total images: 29203\n",
            "- Average images per identity: 405.6\n",
            "\n",
            "Data Split Summary:\n",
            "- Training identities: 58\n",
            "- [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57]\n",
            "- Query identities: 14\n",
            "- [ 4  7 15 18 20 30 33 36 42 43 46 48 49 52]\n",
            "- Gallery identities: 14\n",
            "- [ 4  7 15 18 20 30 33 36 42 43 46 48 49 52]\n",
            "Building train transforms ...\n",
            "+ resize to 256x256\n",
            "+ random flip\n",
            "+ random crop (enlarge to 288x288 and crop 256x256)\n",
            "+ color jitter\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "Building test transforms ...\n",
            "+ resize to 256x256\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "=> Loading train (source) dataset\n",
            "=> Loaded BadmintonReID\n",
            "  ----------------------------------------\n",
            "  subset   | # ids | # images | # cameras\n",
            "  ----------------------------------------\n",
            "  train    |    58 |    23175 |        19\n",
            "  query    |    14 |       14 |        11\n",
            "  gallery  |    14 |     6014 |        11\n",
            "  ----------------------------------------\n",
            "=> Loading test (target) dataset\n",
            "=> Loaded BadmintonReID\n",
            "  ----------------------------------------\n",
            "  subset   | # ids | # images | # cameras\n",
            "  ----------------------------------------\n",
            "  train    |    58 |    23175 |        19\n",
            "  query    |    14 |       14 |        11\n",
            "  gallery  |    14 |     6014 |        11\n",
            "  ----------------------------------------\n",
            "\n",
            "\n",
            "  **************** Summary ****************\n",
            "  source            : ['badminton1']\n",
            "  # source datasets : 1\n",
            "  # source ids      : 58\n",
            "  # source images   : 23175\n",
            "  # source cameras  : 19\n",
            "  target            : ['badminton1']\n",
            "  *****************************************\n",
            "\n",
            "\n",
            "\n",
            "DataManager initialized:\n",
            "- Training identities: 58\n",
            "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
            "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
            "Moving model to GPU...\n",
            "=> Start training\n",
            "epoch: [1/1][10/721]\ttime 0.357 (0.460)\tdata 0.000 (0.090)\teta 0:05:27\tloss_t 0.8487 (1.5429)\tloss_x 3.9304 (4.0410)\tacc 6.2500 (5.9375)\tlr 0.000300\n",
            "epoch: [1/1][20/721]\ttime 0.355 (0.408)\tdata 0.001 (0.045)\teta 0:04:45\tloss_t 1.0266 (1.3079)\tloss_x 3.7791 (4.0090)\tacc 6.2500 (5.6250)\tlr 0.000300\n",
            "epoch: [1/1][30/721]\ttime 0.360 (0.393)\tdata 0.000 (0.030)\teta 0:04:31\tloss_t 0.3358 (1.1013)\tloss_x 3.5923 (3.9154)\tacc 25.0000 (10.3125)\tlr 0.000300\n",
            "epoch: [1/1][40/721]\ttime 0.451 (0.398)\tdata 0.000 (0.024)\teta 0:04:30\tloss_t 0.3187 (0.9655)\tloss_x 3.5530 (3.8398)\tacc 31.2500 (12.0312)\tlr 0.000300\n",
            "epoch: [1/1][50/721]\ttime 0.360 (0.390)\tdata 0.001 (0.020)\teta 0:04:21\tloss_t 1.0189 (0.8835)\tloss_x 3.2310 (3.7453)\tacc 28.1250 (15.3750)\tlr 0.000300\n",
            "epoch: [1/1][60/721]\ttime 0.360 (0.385)\tdata 0.000 (0.016)\teta 0:04:14\tloss_t 0.0249 (0.8224)\tloss_x 3.3181 (3.6672)\tacc 37.5000 (18.4896)\tlr 0.000300\n",
            "epoch: [1/1][70/721]\ttime 0.440 (0.384)\tdata 0.000 (0.014)\teta 0:04:09\tloss_t 1.1177 (0.7641)\tloss_x 3.0225 (3.5857)\tacc 53.1250 (21.2054)\tlr 0.000300\n",
            "epoch: [1/1][80/721]\ttime 0.364 (0.385)\tdata 0.001 (0.013)\teta 0:04:07\tloss_t 0.3062 (0.6989)\tloss_x 2.9610 (3.5091)\tacc 46.8750 (23.6719)\tlr 0.000300\n",
            "epoch: [1/1][90/721]\ttime 0.363 (0.383)\tdata 0.000 (0.011)\teta 0:04:01\tloss_t 0.0000 (0.6912)\tloss_x 2.7501 (3.4232)\tacc 50.0000 (26.2153)\tlr 0.000300\n",
            "epoch: [1/1][100/721]\ttime 0.350 (0.381)\tdata 0.000 (0.010)\teta 0:03:56\tloss_t 0.6512 (0.6370)\tloss_x 2.6556 (3.3487)\tacc 40.6250 (28.9062)\tlr 0.000300\n",
            "epoch: [1/1][110/721]\ttime 0.348 (0.382)\tdata 0.000 (0.010)\teta 0:03:53\tloss_t 0.1115 (0.5976)\tloss_x 2.3827 (3.2690)\tacc 62.5000 (31.5057)\tlr 0.000300\n",
            "epoch: [1/1][120/721]\ttime 0.368 (0.382)\tdata 0.000 (0.009)\teta 0:03:49\tloss_t 0.0223 (0.5701)\tloss_x 2.2214 (3.1954)\tacc 68.7500 (33.9062)\tlr 0.000300\n",
            "epoch: [1/1][130/721]\ttime 0.366 (0.381)\tdata 0.001 (0.008)\teta 0:03:45\tloss_t 0.1053 (0.5476)\tloss_x 2.4433 (3.1291)\tacc 46.8750 (35.4087)\tlr 0.000300\n",
            "epoch: [1/1][140/721]\ttime 0.368 (0.380)\tdata 0.000 (0.008)\teta 0:03:40\tloss_t 0.3262 (0.5190)\tloss_x 2.1926 (3.0641)\tacc 65.6250 (37.2991)\tlr 0.000300\n",
            "epoch: [1/1][150/721]\ttime 0.498 (0.383)\tdata 0.001 (0.008)\teta 0:03:38\tloss_t 0.0127 (0.4986)\tloss_x 1.7872 (2.9949)\tacc 71.8750 (39.2917)\tlr 0.000300\n",
            "epoch: [1/1][160/721]\ttime 0.363 (0.382)\tdata 0.000 (0.007)\teta 0:03:34\tloss_t 0.0000 (0.4776)\tloss_x 1.6923 (2.9372)\tacc 75.0000 (41.0742)\tlr 0.000300\n",
            "epoch: [1/1][170/721]\ttime 0.360 (0.381)\tdata 0.000 (0.007)\teta 0:03:29\tloss_t 0.2876 (0.4592)\tloss_x 2.0242 (2.8792)\tacc 75.0000 (42.7390)\tlr 0.000300\n",
            "epoch: [1/1][180/721]\ttime 0.442 (0.381)\tdata 0.001 (0.006)\teta 0:03:26\tloss_t 0.2256 (0.4450)\tloss_x 1.7570 (2.8213)\tacc 68.7500 (43.9410)\tlr 0.000300\n",
            "epoch: [1/1][190/721]\ttime 0.361 (0.382)\tdata 0.000 (0.006)\teta 0:03:22\tloss_t 0.0000 (0.4313)\tloss_x 1.6892 (2.7700)\tacc 68.7500 (45.3289)\tlr 0.000300\n",
            "epoch: [1/1][200/721]\ttime 0.355 (0.381)\tdata 0.000 (0.006)\teta 0:03:18\tloss_t 0.4442 (0.4206)\tloss_x 1.8151 (2.7171)\tacc 65.6250 (46.7344)\tlr 0.000300\n",
            "epoch: [1/1][210/721]\ttime 0.365 (0.380)\tdata 0.001 (0.006)\teta 0:03:14\tloss_t 0.1562 (0.4068)\tloss_x 1.5818 (2.6665)\tacc 81.2500 (48.0506)\tlr 0.000300\n",
            "epoch: [1/1][220/721]\ttime 0.414 (0.381)\tdata 0.021 (0.006)\teta 0:03:10\tloss_t 0.0000 (0.4008)\tloss_x 1.5668 (2.6190)\tacc 81.2500 (49.2614)\tlr 0.000300\n",
            "epoch: [1/1][230/721]\ttime 0.359 (0.381)\tdata 0.000 (0.006)\teta 0:03:07\tloss_t 0.0000 (0.3900)\tloss_x 1.6085 (2.5757)\tacc 78.1250 (50.6250)\tlr 0.000300\n",
            "epoch: [1/1][240/721]\ttime 0.362 (0.380)\tdata 0.001 (0.005)\teta 0:03:02\tloss_t 0.5286 (0.3873)\tloss_x 1.5157 (2.5346)\tacc 93.7500 (51.8099)\tlr 0.000300\n",
            "epoch: [1/1][250/721]\ttime 0.360 (0.379)\tdata 0.001 (0.005)\teta 0:02:58\tloss_t 0.0000 (0.3753)\tloss_x 1.5700 (2.4934)\tacc 93.7500 (53.1625)\tlr 0.000300\n",
            "epoch: [1/1][260/721]\ttime 0.376 (0.380)\tdata 0.001 (0.005)\teta 0:02:55\tloss_t 0.1405 (0.3634)\tloss_x 1.2710 (2.4524)\tacc 100.0000 (54.3149)\tlr 0.000300\n",
            "epoch: [1/1][270/721]\ttime 0.363 (0.380)\tdata 0.001 (0.005)\teta 0:02:51\tloss_t 0.0000 (0.3538)\tloss_x 1.5065 (2.4148)\tacc 68.7500 (55.4745)\tlr 0.000300\n",
            "epoch: [1/1][280/721]\ttime 0.381 (0.379)\tdata 0.001 (0.005)\teta 0:02:47\tloss_t 0.0000 (0.3497)\tloss_x 1.3253 (2.3793)\tacc 81.2500 (56.4621)\tlr 0.000300\n",
            "epoch: [1/1][290/721]\ttime 0.407 (0.379)\tdata 0.000 (0.005)\teta 0:02:43\tloss_t 0.0844 (0.3431)\tloss_x 1.3103 (2.3443)\tacc 96.8750 (57.4569)\tlr 0.000300\n",
            "epoch: [1/1][300/721]\ttime 0.340 (0.380)\tdata 0.000 (0.004)\teta 0:02:39\tloss_t 0.0000 (0.3391)\tloss_x 1.1956 (2.3096)\tacc 96.8750 (58.5000)\tlr 0.000300\n",
            "epoch: [1/1][310/721]\ttime 0.361 (0.379)\tdata 0.001 (0.004)\teta 0:02:35\tloss_t 0.0000 (0.3317)\tloss_x 1.3116 (2.2772)\tacc 90.6250 (59.3548)\tlr 0.000300\n",
            "epoch: [1/1][320/721]\ttime 0.362 (0.379)\tdata 0.001 (0.004)\teta 0:02:31\tloss_t 0.2802 (0.3339)\tloss_x 1.2966 (2.2468)\tacc 90.6250 (60.1660)\tlr 0.000300\n",
            "epoch: [1/1][330/721]\ttime 0.411 (0.379)\tdata 0.000 (0.004)\teta 0:02:28\tloss_t 0.0786 (0.3307)\tloss_x 1.2879 (2.2175)\tacc 87.5000 (60.9280)\tlr 0.000300\n",
            "epoch: [1/1][340/721]\ttime 0.365 (0.379)\tdata 0.000 (0.004)\teta 0:02:24\tloss_t 0.0000 (0.3237)\tloss_x 1.1635 (2.1884)\tacc 87.5000 (61.7004)\tlr 0.000300\n",
            "epoch: [1/1][350/721]\ttime 0.364 (0.379)\tdata 0.000 (0.004)\teta 0:02:20\tloss_t 0.0573 (0.3151)\tloss_x 1.1719 (2.1609)\tacc 100.0000 (62.5357)\tlr 0.000300\n",
            "epoch: [1/1][360/721]\ttime 0.365 (0.378)\tdata 0.001 (0.004)\teta 0:02:16\tloss_t 0.1373 (0.3116)\tloss_x 1.1056 (2.1329)\tacc 100.0000 (63.3681)\tlr 0.000300\n",
            "epoch: [1/1][370/721]\ttime 0.380 (0.379)\tdata 0.001 (0.004)\teta 0:02:13\tloss_t 0.0000 (0.3058)\tloss_x 1.0753 (2.1069)\tacc 96.8750 (64.0541)\tlr 0.000300\n",
            "epoch: [1/1][380/721]\ttime 0.369 (0.379)\tdata 0.000 (0.004)\teta 0:02:09\tloss_t 0.0000 (0.3007)\tloss_x 1.2019 (2.0819)\tacc 84.3750 (64.6546)\tlr 0.000300\n",
            "epoch: [1/1][390/721]\ttime 0.363 (0.378)\tdata 0.001 (0.004)\teta 0:02:05\tloss_t 0.1368 (0.2960)\tloss_x 1.0525 (2.0570)\tacc 100.0000 (65.3446)\tlr 0.000300\n",
            "epoch: [1/1][400/721]\ttime 0.396 (0.378)\tdata 0.001 (0.004)\teta 0:02:01\tloss_t 0.1066 (0.2913)\tloss_x 1.0242 (2.0338)\tacc 93.7500 (65.8828)\tlr 0.000300\n",
            "epoch: [1/1][410/721]\ttime 0.349 (0.378)\tdata 0.007 (0.004)\teta 0:01:57\tloss_t 0.0000 (0.2864)\tloss_x 1.0346 (2.0110)\tacc 100.0000 (66.4710)\tlr 0.000300\n",
            "epoch: [1/1][420/721]\ttime 0.361 (0.378)\tdata 0.001 (0.004)\teta 0:01:53\tloss_t 0.2087 (0.2840)\tloss_x 1.1162 (1.9883)\tacc 90.6250 (67.0759)\tlr 0.000300\n",
            "epoch: [1/1][430/721]\ttime 0.361 (0.378)\tdata 0.000 (0.004)\teta 0:01:49\tloss_t 0.0357 (0.2785)\tloss_x 1.0469 (1.9670)\tacc 93.7500 (67.6235)\tlr 0.000300\n",
            "epoch: [1/1][440/721]\ttime 0.465 (0.378)\tdata 0.000 (0.003)\teta 0:01:46\tloss_t 0.1605 (0.2731)\tloss_x 1.0744 (1.9466)\tacc 96.8750 (68.1818)\tlr 0.000300\n",
            "epoch: [1/1][450/721]\ttime 0.374 (0.378)\tdata 0.001 (0.003)\teta 0:01:42\tloss_t 0.2013 (0.2704)\tloss_x 1.0788 (1.9267)\tacc 100.0000 (68.7083)\tlr 0.000300\n",
            "epoch: [1/1][460/721]\ttime 0.363 (0.378)\tdata 0.000 (0.003)\teta 0:01:38\tloss_t 0.0017 (0.2651)\tloss_x 1.0150 (1.9067)\tacc 87.5000 (69.2255)\tlr 0.000300\n",
            "epoch: [1/1][470/721]\ttime 0.367 (0.378)\tdata 0.000 (0.003)\teta 0:01:34\tloss_t 0.0000 (0.2621)\tloss_x 0.9960 (1.8878)\tacc 96.8750 (69.7008)\tlr 0.000300\n",
            "epoch: [1/1][480/721]\ttime 0.462 (0.379)\tdata 0.005 (0.003)\teta 0:01:31\tloss_t 0.1752 (0.2589)\tloss_x 1.0323 (1.8700)\tacc 87.5000 (70.1367)\tlr 0.000300\n",
            "epoch: [1/1][490/721]\ttime 0.366 (0.378)\tdata 0.000 (0.003)\teta 0:01:27\tloss_t 0.0000 (0.2564)\tloss_x 0.9975 (1.8515)\tacc 84.3750 (70.6505)\tlr 0.000300\n",
            "epoch: [1/1][500/721]\ttime 0.367 (0.378)\tdata 0.000 (0.003)\teta 0:01:23\tloss_t 0.1539 (0.2535)\tloss_x 0.9288 (1.8347)\tacc 100.0000 (71.0875)\tlr 0.000300\n",
            "epoch: [1/1][510/721]\ttime 0.359 (0.378)\tdata 0.003 (0.003)\teta 0:01:19\tloss_t 0.0993 (0.2515)\tloss_x 0.9483 (1.8177)\tacc 93.7500 (71.5257)\tlr 0.000300\n",
            "epoch: [1/1][520/721]\ttime 0.364 (0.378)\tdata 0.003 (0.003)\teta 0:01:16\tloss_t 0.0000 (0.2489)\tloss_x 0.9813 (1.8022)\tacc 87.5000 (71.8329)\tlr 0.000300\n",
            "epoch: [1/1][530/721]\ttime 0.365 (0.378)\tdata 0.000 (0.003)\teta 0:01:12\tloss_t 0.0000 (0.2460)\tloss_x 0.9016 (1.7868)\tacc 100.0000 (72.2406)\tlr 0.000300\n",
            "epoch: [1/1][540/721]\ttime 0.363 (0.378)\tdata 0.000 (0.003)\teta 0:01:08\tloss_t 0.0000 (0.2436)\tloss_x 0.9245 (1.7714)\tacc 87.5000 (72.5694)\tlr 0.000300\n",
            "epoch: [1/1][550/721]\ttime 0.447 (0.378)\tdata 0.020 (0.003)\teta 0:01:04\tloss_t 0.0867 (0.2415)\tloss_x 1.0688 (1.7569)\tacc 93.7500 (72.9318)\tlr 0.000300\n",
            "epoch: [1/1][560/721]\ttime 0.361 (0.378)\tdata 0.001 (0.003)\teta 0:01:00\tloss_t 0.0000 (0.2394)\tloss_x 0.9101 (1.7423)\tacc 87.5000 (73.2533)\tlr 0.000300\n",
            "epoch: [1/1][570/721]\ttime 0.362 (0.378)\tdata 0.001 (0.003)\teta 0:00:57\tloss_t 0.0000 (0.2368)\tloss_x 0.9896 (1.7284)\tacc 96.8750 (73.6294)\tlr 0.000300\n",
            "epoch: [1/1][580/721]\ttime 0.367 (0.378)\tdata 0.001 (0.003)\teta 0:00:53\tloss_t 0.0000 (0.2340)\tloss_x 0.9547 (1.7148)\tacc 87.5000 (73.9601)\tlr 0.000300\n",
            "epoch: [1/1][590/721]\ttime 0.419 (0.378)\tdata 0.000 (0.003)\teta 0:00:49\tloss_t 0.0000 (0.2327)\tloss_x 0.8591 (1.7020)\tacc 100.0000 (74.2267)\tlr 0.000300\n",
            "epoch: [1/1][600/721]\ttime 0.361 (0.378)\tdata 0.000 (0.003)\teta 0:00:45\tloss_t 0.0000 (0.2313)\tloss_x 0.9477 (1.6892)\tacc 87.5000 (74.5365)\tlr 0.000300\n",
            "epoch: [1/1][610/721]\ttime 0.363 (0.378)\tdata 0.000 (0.003)\teta 0:00:41\tloss_t 0.0000 (0.2298)\tloss_x 0.8587 (1.6774)\tacc 100.0000 (74.7387)\tlr 0.000300\n",
            "epoch: [1/1][620/721]\ttime 0.359 (0.377)\tdata 0.000 (0.003)\teta 0:00:38\tloss_t 0.0000 (0.2275)\tloss_x 0.9187 (1.6656)\tacc 87.5000 (75.0302)\tlr 0.000300\n",
            "epoch: [1/1][630/721]\ttime 0.442 (0.378)\tdata 0.000 (0.003)\teta 0:00:34\tloss_t 0.0000 (0.2288)\tloss_x 0.9060 (1.6545)\tacc 87.5000 (75.2232)\tlr 0.000300\n",
            "epoch: [1/1][640/721]\ttime 0.362 (0.378)\tdata 0.000 (0.003)\teta 0:00:30\tloss_t 0.0000 (0.2297)\tloss_x 0.9386 (1.6437)\tacc 93.7500 (75.4639)\tlr 0.000300\n",
            "epoch: [1/1][650/721]\ttime 0.361 (0.377)\tdata 0.000 (0.003)\teta 0:00:26\tloss_t 0.6180 (0.2305)\tloss_x 0.9910 (1.6329)\tacc 87.5000 (75.6827)\tlr 0.000300\n",
            "epoch: [1/1][660/721]\ttime 0.423 (0.377)\tdata 0.010 (0.003)\teta 0:00:23\tloss_t 0.6030 (0.2295)\tloss_x 1.0559 (1.6224)\tacc 68.7500 (75.8570)\tlr 0.000300\n",
            "epoch: [1/1][670/721]\ttime 0.372 (0.378)\tdata 0.000 (0.003)\teta 0:00:19\tloss_t 0.0000 (0.2278)\tloss_x 0.9696 (1.6123)\tacc 75.0000 (76.0448)\tlr 0.000300\n",
            "epoch: [1/1][680/721]\ttime 0.366 (0.377)\tdata 0.000 (0.003)\teta 0:00:15\tloss_t 0.0000 (0.2269)\tloss_x 0.9587 (1.6023)\tacc 87.5000 (76.2776)\tlr 0.000300\n",
            "epoch: [1/1][690/721]\ttime 0.364 (0.377)\tdata 0.001 (0.003)\teta 0:00:11\tloss_t 0.0000 (0.2270)\tloss_x 0.8500 (1.5924)\tacc 100.0000 (76.4900)\tlr 0.000300\n",
            "epoch: [1/1][700/721]\ttime 0.426 (0.378)\tdata 0.012 (0.003)\teta 0:00:07\tloss_t 0.0319 (0.2271)\tloss_x 0.8251 (1.5823)\tacc 100.0000 (76.7679)\tlr 0.000300\n",
            "=> Final test\n",
            "##### Evaluating badminton1 (source) #####\n",
            "Extracting features from query set ...\n",
            "Done, obtained 14-by-512 matrix\n",
            "Extracting features from gallery set ...\n",
            "Done, obtained 6014-by-512 matrix\n",
            "Speed: 0.0572 sec/batch\n",
            "Computing distance matrix with metric=euclidean ...\n",
            "Computing CMC and mAP ...\n",
            "** Results **\n",
            "mAP: 94.7%\n",
            "CMC curve\n",
            "Rank-1  : 100.0%\n",
            "Rank-5  : 100.0%\n",
            "Rank-10 : 100.0%\n",
            "Rank-20 : 100.0%\n",
            "Checkpoint saved to \"/content/badminton_reid_model/model/model.pth.tar-1\"\n",
            "Elapsed 0:04:52\n",
            "\n",
            "Performing visual ranking...\n",
            "##### Evaluating badminton1 (source) #####\n",
            "Extracting features from query set ...\n",
            "Done, obtained 14-by-512 matrix\n",
            "Extracting features from gallery set ...\n",
            "Done, obtained 6014-by-512 matrix\n",
            "Speed: 0.0540 sec/batch\n",
            "Computing distance matrix with metric=euclidean ...\n",
            "Computing CMC and mAP ...\n",
            "** Results **\n",
            "mAP: 94.7%\n",
            "CMC curve\n",
            "Rank-1  : 100.0%\n",
            "Rank-5  : 100.0%\n",
            "Rank-10 : 100.0%\n",
            "# query: 14\n",
            "# gallery 6014\n",
            "Visualizing top-5 ranks ...\n",
            "Done. Images have been saved to \"/content/badminton_reid_model/visrank_badminton1\" ...\n",
            "Training completed! Model saved to: /content/badminton_reid_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Test custom tracking\n",
        "print(\"\\n🎯 Step 5: Testing custom tracking...\")\n",
        "reid_model_path = \"/content/badminton_reid_model/model/model.pth.tar-1\"\n",
        "run_custom_strongsort(yolo_model_path, reid_model_path, \"test_video.mp4\")\n",
        "\n",
        "print(\"\\n🎉 Pipeline setup complete!\")"
      ],
      "metadata": {
        "id": "Cf47GhGYQkEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c2254-0851-4534-86ef-90976a924750"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 Step 5: Testing custom tracking...\n",
            "Custom configuration saved to: /content/badminton_strongsort.yaml\n",
            "python3: can't open file '/content/track.py': [Errno 2] No such file or directory\n",
            "Custom tracking completed! Results saved to: runs/track/custom_badminton\n",
            "\n",
            "🎉 Pipeline setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries for the script to be self-contained\n",
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import sys\n",
        "from types import SimpleNamespace\n",
        "\n",
        "# Add the parent directory of boxmot to the system path to allow imports\n",
        "# This is necessary because the files are not in a standard Python package\n",
        "# In a real-world scenario, you would install boxmot as a package\n",
        "# We'll assume the boxmot directory is a sibling to the current script's location.\n",
        "# If this causes issues, you might need to adjust the path.\n",
        "try:\n",
        "    from boxmot.tracker_zoo import create_tracker\n",
        "    from boxmot.utils import TRACKER_CONFIGS, WEIGHTS\n",
        "    from boxmot.utils.checks import RequirementsChecker\n",
        "    from boxmot.engine.detectors import default_imgsz, get_yolo_inferer, is_ultralytics_model\n",
        "    from boxmot import TRACKERS\n",
        "    from ultralytics import YOLO\n",
        "    from ultralytics.utils.plotting import Annotator, colors\n",
        "    from ultralytics.utils import plotting\n",
        "    # Make every drawing call a no-op to disable default plotting\n",
        "    plotting.Annotator.box       = lambda *args, **kwargs: None\n",
        "    plotting.Annotator.box_label = lambda *args, **kwargs: None\n",
        "    plotting.Annotator.line      = lambda *args, **kwargs: None\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing boxmot modules: {e}\")\n",
        "    print(\"Please ensure you have boxmot and ultralytics installed.\")\n",
        "    sys.exit()\n",
        "\n",
        "def setup_custom_strongsort_config(reid_model_path):\n",
        "    \"\"\"\n",
        "    Create custom StrongSORT configuration for badminton.\n",
        "\n",
        "    Args:\n",
        "        reid_model_path (str): Path to the trained ReID model weights.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the generated configuration file.\n",
        "    \"\"\"\n",
        "    strongsort_config = {\n",
        "        'reid_weights': reid_model_path,\n",
        "        'max_dist': 0.2,\n",
        "        'min_confidence': 0.3,\n",
        "        'nms_max_overlap': 0.7,\n",
        "        'max_iou_distance': 0.7,\n",
        "        'max_age': 30,\n",
        "        'n_init': 3,\n",
        "        'nn_budget': 100,\n",
        "        'ema_alpha': 0.9,\n",
        "    }\n",
        "\n",
        "    config_path = \"/content/badminton_strongsort.yaml\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(strongsort_config, f)\n",
        "\n",
        "    print(f\"Custom configuration saved to: {config_path}\")\n",
        "    return config_path\n",
        "\n",
        "def on_predict_start(predictor, persist=False):\n",
        "    \"\"\"\n",
        "    Initialize trackers for object tracking during prediction.\n",
        "    Args:\n",
        "        predictor (object): The predictor object to initialize trackers for.\n",
        "        persist (bool, optional): Whether to persist the trackers if they already exist. Defaults to False.\n",
        "    \"\"\"\n",
        "    assert predictor.custom_args.tracking_method in TRACKERS, \\\n",
        "        f\"'{predictor.custom_args.tracking_method}' is not supported. Supported ones are {TRACKERS}\"\n",
        "\n",
        "    tracking_config = TRACKER_CONFIGS / (predictor.custom_args.tracking_method + '.yaml')\n",
        "    trackers = []\n",
        "    for i in range(predictor.dataset.bs):\n",
        "        tracker = create_tracker(\n",
        "            predictor.custom_args.tracking_method,\n",
        "            tracking_config,\n",
        "            predictor.custom_args.reid_model,\n",
        "            predictor.device,\n",
        "            predictor.custom_args.half,\n",
        "            predictor.custom_args.per_class,\n",
        "        )\n",
        "        trackers.append(tracker)\n",
        "\n",
        "    predictor.trackers = trackers\n",
        "\n",
        "# callback to plot trajectories on each frame\n",
        "def plot_trajectories(predictor):\n",
        "    # predictor.results is a list of Results, one per frame in the batch\n",
        "    for i, result in enumerate(predictor.results):\n",
        "        tracker = predictor.trackers[i]\n",
        "        # This function might cause issues as it's not a standard boxmot function,\n",
        "        # and the original code has it defined for a different purpose.\n",
        "        # We'll comment it out to avoid errors, as the main goal is to save results.\n",
        "        # result.orig_img = tracker.plot_results(result.orig_img, predictor.custom_args.show_trajectories)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_track(args):\n",
        "    \"\"\"\n",
        "    Runs the tracking pipeline using YOLO and StrongSORT.\n",
        "    This function is a reimplementation of the user's `track()` function\n",
        "    but is called programmatically.\n",
        "    \"\"\"\n",
        "    if args.imgsz is None:\n",
        "        args.imgsz = default_imgsz(args.yolo_model)\n",
        "    yolo = YOLO(\n",
        "        args.yolo_model if is_ultralytics_model(args.yolo_model) else \"yolov8n.pt\",\n",
        "    )\n",
        "\n",
        "    results = yolo.track(\n",
        "        source=args.source,\n",
        "        conf=args.conf,\n",
        "        iou=args.iou,\n",
        "        agnostic_nms=args.agnostic_nms,\n",
        "        show=True,\n",
        "        stream=True,\n",
        "        device=args.device,\n",
        "        show_conf=args.show_conf,\n",
        "        save_txt=args.save_txt,\n",
        "        show_labels=args.show_labels,\n",
        "        save=args.save,\n",
        "        # verbose=args.verbose,\n",
        "        exist_ok=args.exist_ok,\n",
        "        project=args.project,\n",
        "        name=args.name,\n",
        "        classes=args.classes,\n",
        "        imgsz=args.imgsz,\n",
        "        vid_stride=args.vid_stride,\n",
        "        line_width=args.line_width,\n",
        "        save_crop=args.save_crop,\n",
        "    )\n",
        "\n",
        "    yolo.add_callback(\"on_predict_start\", partial(on_predict_start, persist=True))\n",
        "    # yolo.add_callback(\"on_predict_postprocess_end\", plot_trajectories) # Commented out to prevent errors\n",
        "\n",
        "    if not is_ultralytics_model(args.yolo_model):\n",
        "        m = get_yolo_inferer(args.yolo_model)\n",
        "        yolo_model = m(\n",
        "            model=args.yolo_model,\n",
        "            device=yolo.predictor.device,\n",
        "            args=yolo.predictor.args,\n",
        "        )\n",
        "        yolo.predictor.model = yolo_model\n",
        "\n",
        "        if not is_ultralytics_model(args.yolo_model):\n",
        "            yolo.add_callback(\n",
        "                \"on_predict_batch_start\", lambda p: yolo_model.update_im_paths(p)\n",
        "            )\n",
        "            yolo.predictor.preprocess = lambda imgs: yolo_model.preprocess(im=imgs)\n",
        "            yolo.predictor.postprocess = lambda preds, im, im0s: yolo_model.postprocess(\n",
        "                preds=preds, im=im, im0s=im0s\n",
        "            )\n",
        "\n",
        "    yolo.predictor.custom_args = args\n",
        "\n",
        "    for _ in results:\n",
        "        pass\n",
        "\n",
        "def evaluate_badminton_tracking(yolo_model_path, reid_model_path, test_video_path):\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the tracking evaluation.\n",
        "\n",
        "    Args:\n",
        "        yolo_model_path (str): Path to the YOLO detection model.\n",
        "        reid_model_path (str): Path to the trained ReID model.\n",
        "        test_video_path (str): Path to the test video file.\n",
        "    \"\"\"\n",
        "    # Create the custom configuration file\n",
        "    config_path = setup_custom_strongsort_config(reid_model_path)\n",
        "\n",
        "    # Create a dummy arguments object to pass to the track function\n",
        "    # This mimics the arguments a user would provide from the command line\n",
        "    args = SimpleNamespace(\n",
        "        yolo_model=yolo_model_path,\n",
        "        reid_model=reid_model_path,\n",
        "        tracking_method='strongsort',\n",
        "        source=test_video_path,\n",
        "        imgsz=640,\n",
        "        conf=0.4,\n",
        "        iou=0.5,\n",
        "        agnostic_nms=False,\n",
        "        show_conf=False,\n",
        "        show_labels=True,\n",
        "        save=True,\n",
        "        save_txt=True,\n",
        "        save_conf=True,\n",
        "        project=\"runs/track\",\n",
        "        name=\"custom_badminton\",\n",
        "        exist_ok=True,\n",
        "        device='0' if torch.cuda.is_available() else 'cpu',\n",
        "        classes=None, # None means track all classes\n",
        "        vid_stride=1,\n",
        "        line_width=None,\n",
        "        save_crop=False,\n",
        "        half=True, # Use half-precision if available\n",
        "        per_class=False,\n",
        "        show_trajectories=True, # Note: this feature might not work as intended in the current setup\n",
        "    )\n",
        "\n",
        "    # Run the tracking\n",
        "    run_track(args)\n",
        "\n",
        "    print(f\"\\nEvaluation completed!\")\n",
        "    print(f\"Results saved to: {args.project}/{args.name}\")\n",
        "    print(\"Look for the output video and the tracking .txt files in this directory.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the paths to your models and test video\n",
        "    YOLO_MODEL_PATH = \"/content/drive/MyDrive/FIT3163,3164/YOLO/my_yolov8_1.pt\"\n",
        "    REID_MODEL_PATH = \"/content/badminton_reid_model/model/model.pth.tar-1\"\n",
        "    TEST_VIDEO_PATH = \"/content/drive/MyDrive/FIT3163,3164/REID/test/reid_test_vid_1.mp4\"\n",
        "\n",
        "    evaluate_badminton_tracking(YOLO_MODEL_PATH, REID_MODEL_PATH, TEST_VIDEO_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pyUePBp2h_by",
        "outputId": "6a90b1a6-20c5-486d-bd1c-512c440d9857"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing boxmot modules: No module named 'boxmot'\n",
            "Please ensure you have boxmot and ultralytics installed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3097829601.py\", line 18, in <cell line: 0>\n",
            "    from boxmot.tracker_zoo import create_tracker\n",
            "ModuleNotFoundError: No module named 'boxmot'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-3097829601.py\", line 33, in <cell line: 0>\n",
            "    sys.exit()\n",
            "SystemExit\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3097829601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mboxmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker_zoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_tracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mboxmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTRACKER_CONFIGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boxmot'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3097829601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please ensure you have boxmot and ultralytics installed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QelrUDFBfJUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}