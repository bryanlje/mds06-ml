{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Build label map & splits**"
      ],
      "metadata": {
        "id": "sk-am0rbtFkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchmetrics decord fvcore pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlv-BF09KLjb",
        "outputId": "20982ff2-ba6a-425f-d16c-8eefbeabaaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random, csv, glob, os\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassConfusionMatrix\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torchvision.transforms import v2\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from dataclasses import dataclass\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "-ziMQw9hQwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jFlWfIQXhSqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d620082-584e-4af6-f276-38e4694331ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aV_Tuy68D3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. CONFIGURATION**"
      ],
      "metadata": {
        "id": "wP3eWavtFCaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)"
      ],
      "metadata": {
        "id": "AafOzBbQ4s74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.root_dir = \"/content/drive/MyDrive/FIT3163,3164/SlowFast\"\n",
        "        self.clips_dir = os.path.join(self.root_dir, \"05_clips/3in1\")\n",
        "        self.splits_dir = os.path.join(self.root_dir, \"06_splits/3in1\")\n",
        "        self.models_dir = os.path.join(self.root_dir, \"07_models/3in1_train4\")\n",
        "        self.best_model_path = os.path.join(self.models_dir, \"best.pt\")\n",
        "\n",
        "        self.labels = [\n",
        "            \"smash\", \"jump_smash\", \"block\",\n",
        "            \"drop\", \"clear\", \"lift\", \"drive\",\n",
        "            \"straight_net\", \"cross_net\", \"serve\",\n",
        "            \"push\", \"tap\",\n",
        "            \"average_joe\"\n",
        "        ]\n",
        "\n",
        "        # Training parameters\n",
        "        self.epochs = 30\n",
        "        self.batch_size = 8\n",
        "        self.learning_rate = 0.005\n",
        "        self.weight_decay = 0.001\n",
        "\n",
        "        self.early_stopping_patience = 5\n",
        "\n",
        "# Create a configuration object\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "xr1YOc2450CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class VideoCfg:\n",
        "    clips_dir: str\n",
        "    splits_dir: str\n",
        "    labels: List[str]\n",
        "    side: int = 224           # input size (square)\n",
        "    num_frames: int = 8       # frames per clip\n",
        "    sample: str = \"uniform\"   # \"uniform\" or \"rand\"\n",
        "    train_scale: Tuple[float,float] = (0.7, 1.0)\n",
        "    train_ratio: Tuple[float,float] = (0.75, 1.333)"
      ],
      "metadata": {
        "id": "pOEfK8GlFFCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. DATA PREPARATION**\n",
        "### This function handles all logic for splitting and saving the dataset."
      ],
      "metadata": {
        "id": "A9_EREhMIlz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_splits(cfg: VideoCfg):\n",
        "    os.makedirs(cfg.splits_dir, exist_ok=True)\n",
        "    labels_map = {lab: i for i, lab in enumerate(cfg.labels)}\n",
        "    with open(os.path.join(cfg.splits_dir, \"labels_map.json\"), \"w\") as f:\n",
        "        json.dump(labels_map, f, indent=2)\n",
        "\n",
        "    items = []\n",
        "    for lab in cfg.labels:\n",
        "        for p in glob.glob(os.path.join(cfg.clips_dir, lab, \"*.mp4\")):\n",
        "            items.append((p, labels_map[lab]))\n",
        "\n",
        "    random.seed(1337)\n",
        "    random.shuffle(items)\n",
        "    n = len(items)\n",
        "    n_tr = int(0.8*n)\n",
        "    n_va = int(0.1*n)\n",
        "\n",
        "    splits = {\n",
        "        \"train.csv\": items[:n_tr],\n",
        "        \"val.csv\":   items[n_tr:n_tr+n_va],\n",
        "        \"test.csv\":  items[n_tr+n_va:],\n",
        "    }\n",
        "    for name, rows in splits.items():\n",
        "        with open(os.path.join(cfg.splits_dir, name), \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f); w.writerows(rows)\n",
        "    print({k: len(v) for k,v in splits.items()})"
      ],
      "metadata": {
        "id": "tWgRS404IoRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_data_splits(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmj70hvA6Vj",
        "outputId": "e387f5fe-08b2-4eed-bbbc-7665702e4ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train.csv': 404, 'val.csv': 50, 'test.csv': 52}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. DATASET**\n",
        "### The ClipDataset class handles video loading and preprocessing."
      ],
      "metadata": {
        "id": "DsPRxWf8JREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoClipDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      x: (T, C, H, W) float32 normalized (ImageNet) in [~N(0,1)]\n",
        "      y: int label\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path: str, cfg: VideoCfg, train: bool=True):\n",
        "        self.items = [(p, int(y)) for p, y in csv.reader(open(csv_path))]\n",
        "        self.cfg = cfg\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self): return len(self.items)\n",
        "\n",
        "    def _sample_indices(self, num_frames_total: int) -> List[int]:\n",
        "        T = self.cfg.num_frames\n",
        "        if num_frames_total <= 0:\n",
        "            return [0]*T\n",
        "        if self.cfg.sample == \"uniform\" or not self.train:\n",
        "            # uniform over the whole clip\n",
        "            idx = np.linspace(0, max(0, num_frames_total-1), T)\n",
        "            return [int(round(i)) for i in idx]\n",
        "        else:\n",
        "            if num_frames_total >= T:\n",
        "                start = np.random.randint(0, num_frames_total - T + 1)\n",
        "                return list(range(start, start+T))\n",
        "            # pad by repeating last frame\n",
        "            base = list(range(num_frames_total))\n",
        "            return base + [num_frames_total-1]*(T-num_frames_total)\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_tensor(frames: np.ndarray) -> torch.Tensor:\n",
        "        # frames: (T, H, W, C) uint8 -> (T, C, H, W) float32 [0,1]\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0\n",
        "        return x\n",
        "\n",
        "    def _train_augment_same(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply the SAME random crop/flip to all frames in a clip.\n",
        "        x: (T,C,H,W)\n",
        "        \"\"\"\n",
        "        Tn, C, H, W = x.shape\n",
        "        # RandomResizedCrop params (once) – use first frame for sizing\n",
        "        pil0 = Image.fromarray((x[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8))\n",
        "        i, j, h, w = torchvision.transforms.RandomResizedCrop.get_params(\n",
        "            pil0, scale=self.cfg.train_scale, ratio=self.cfg.train_ratio\n",
        "        )\n",
        "        out_frames = []\n",
        "        for t in range(Tn):\n",
        "            fr = TF.resized_crop(x[t], i, j, h, w,\n",
        "                                 size=[self.cfg.side, self.cfg.side],\n",
        "                                 interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
        "            out_frames.append(fr)\n",
        "        x = torch.stack(out_frames, dim=0)  # (T,C,side,side)\n",
        "\n",
        "        # Same horizontal flip for all frames\n",
        "        if random.random() < 0.5:\n",
        "            x = torch.flip(x, dims=[-1])  # flip width\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _eval_resize_center(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Resize shorter side to side, then center crop to (side, side)\n",
        "        Tn, C, H, W = x.shape\n",
        "        out = []\n",
        "        for t in range(Tn):\n",
        "            fr = x[t]\n",
        "            # keep aspect: resize so min(H,W) -> side\n",
        "            scale = self.cfg.side / min(H, W)\n",
        "            newH, newW = int(round(H*scale)), int(round(W*scale))\n",
        "            fr = TF.resize(fr, [newH, newW], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
        "            # center crop\n",
        "            top = max((newH - self.cfg.side)//2, 0)\n",
        "            left = max((newW - self.cfg.side)//2, 0)\n",
        "            fr = TF.crop(fr, top, left, self.cfg.side, self.cfg.side)\n",
        "            out.append(fr)\n",
        "        return torch.stack(out, dim=0)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        path, label = self.items[idx]\n",
        "        vr = VideoReader(path, ctx=cpu(0))\n",
        "        T_total = len(vr)\n",
        "        indices = self._sample_indices(T_total)\n",
        "\n",
        "        try:\n",
        "            frames = vr.get_batch([min(i, T_total-1) for i in indices]).asnumpy()  # (T,H,W,C)\n",
        "        except Exception:\n",
        "            frames = np.stack([vr[min(i, T_total-1)].asnumpy() for i in indices], axis=0)\n",
        "\n",
        "        x = self._to_tensor(frames)  # (T,C,H,W)\n",
        "\n",
        "        if self.train:\n",
        "            x = self._train_augment_same(x)\n",
        "        else:\n",
        "            x = self._eval_resize_center(x)\n",
        "\n",
        "        # Normalize (broadcast)\n",
        "        mean = IMAGENET_MEAN.to(x)\n",
        "        std  = IMAGENET_STD.to(x)\n",
        "        x = (x - mean) / std\n",
        "        return x, label"
      ],
      "metadata": {
        "id": "t3nn3vXSB60U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generate datasets and loaders for training, validation, and testing**"
      ],
      "metadata": {
        "id": "oaWSMQwAnd6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = os.path.join(cfg.splits_dir, \"train.csv\")\n",
        "val_csv   = os.path.join(cfg.splits_dir, \"val.csv\")\n",
        "test_csv  = os.path.join(cfg.splits_dir, \"test.csv\")\n",
        "\n",
        "vcfg = VideoCfg(\n",
        "    clips_dir=cfg.clips_dir, splits_dir=cfg.splits_dir, labels=cfg.labels,\n",
        "    side=224, num_frames=8, sample=\"uniform\"\n",
        ")\n",
        "\n",
        "train_ds = VideoClipDataset(train_csv, vcfg, train=True)\n",
        "val_ds   = VideoClipDataset(val_csv,   vcfg, train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True, persistent_workers=False\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=max(1, cfg.batch_size), shuffle=False,\n",
        "    num_workers=2, pin_memory=True, persistent_workers=False\n",
        "    )\n",
        "\n",
        "num_classes = len(vcfg.labels)\n",
        "print(\"Classes:\", num_classes, vcfg.labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhuyMRO0BCIi",
        "outputId": "bd283add-59c4-4e25-f534-8dbfcbde99fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: 13 ['smash', 'jump_smash', 'block', 'drop', 'clear', 'lift', 'drive', 'straight_net', 'cross_net', 'serve', 'push', 'tap', 'average_joe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. TRAINING AND EVALUATION**"
      ],
      "metadata": {
        "id": "5A5kc3azJqM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, loss_fn, loader):\n",
        "    total = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (batchX, batchY) in tqdm(loader, desc=\"Computing Loss\"):\n",
        "            batchX = batchX.to(device).float()\n",
        "            batchY = batchY.to(device).long()\n",
        "            loss = loss_fn(model(batchX), batchY).item()\n",
        "            total += loss\n",
        "    model.train()\n",
        "    return total / len(loader)"
      ],
      "metadata": {
        "id": "9MO6MPlDxZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_acc(model, loader):\n",
        "    correct = 0\n",
        "    totals = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (batchX, batchY) in tqdm(loader, desc=\"Computing Accuracy\"):\n",
        "            batchX = batchX.to(device).float()\n",
        "            batchY = batchY.to(device)\n",
        "            outputs = model(batchX)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            totals += batchY.size(0)\n",
        "            correct += (preds == batchY).sum().item()\n",
        "    return correct / totals"
      ],
      "metadata": {
        "id": "iFrE6CRMxapt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DINOv3**"
      ],
      "metadata": {
        "id": "pVRiC1dVnC3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnPool(nn.Module):\n",
        "    def __init__(self, dim, hidden=256):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim, hidden)\n",
        "        self.ctx  = nn.Linear(hidden, 1, bias=False)\n",
        "    def forward(self, x):           # x: (B, T, D)\n",
        "        a = torch.tanh(self.proj(x))\n",
        "        w = torch.softmax(self.ctx(a).squeeze(-1), dim=1)   # (B, T)\n",
        "        return (w.unsqueeze(-1) * x).sum(dim=1)             # (B, D)"
      ],
      "metadata": {
        "id": "3kY1Spp0_tG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DINOv3VideoClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 repo_dir, model,\n",
        "                 num_classes=20,\n",
        "                 pretrained=True,\n",
        "                 weights_path=None,\n",
        "                 fine_tune_backbone=False,\n",
        "                 complex_head=True,\n",
        "                 temporal_pool: str = \"mean\"  # \"mean\" or \"max\"\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        torch.hub._validate_not_a_forked_repo = lambda *a, **k: True\n",
        "\n",
        "        # Load DINOv3 backbone (returns per-image embedding)\n",
        "        self.backbone = torch.hub.load(\n",
        "            repo_or_dir=repo_dir,\n",
        "            model=model,\n",
        "            source='local',\n",
        "            pretrained=pretrained,\n",
        "            weights=weights_path\n",
        "        )\n",
        "\n",
        "        # (optionally) freeze\n",
        "        if not fine_tune_backbone:\n",
        "            for p in self.backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # infer embedding dim\n",
        "        with torch.no_grad():\n",
        "            mock = torch.randn(1, 3, 224, 224)\n",
        "            emb = self.backbone(mock)\n",
        "        embed_dim = emb.shape[-1]\n",
        "\n",
        "        if complex_head:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(embed_dim, 512), nn.ReLU(), nn.Dropout(0.5),\n",
        "                nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.4),\n",
        "                nn.Linear(256, num_classes)\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(nn.Dropout(0.5), nn.Linear(embed_dim, num_classes))\n",
        "\n",
        "        self.temporal_pool = temporal_pool\n",
        "\n",
        "        if temporal_pool == \"attn\":\n",
        "            self.attn_pool = AttnPool(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x can be:\n",
        "          - (B, C, H, W)   -> image path\n",
        "          - (B, T, C, H, W)-> video path (preferred)\n",
        "          - (T, C, H, W)   -> single video, unsqueezed to B=1\n",
        "        \"\"\"\n",
        "        if x.dim() == 4:\n",
        "            # Images: (B,C,H,W)\n",
        "            feats = self.backbone(x)\n",
        "            logits = self.classifier(feats)\n",
        "            return logits\n",
        "\n",
        "        if x.dim() == 5:\n",
        "            B, T, C, H, W = x.shape\n",
        "            # Flatten time into batch for backbone pass\n",
        "            x_2d = x.reshape(B*T, C, H, W)\n",
        "            feats = self.backbone(x_2d)             # (B*T, D)\n",
        "            feats = feats.view(B, T, -1)            # (B, T, D)\n",
        "\n",
        "            if self.temporal_pool == \"max\":\n",
        "                clip_feat, _ = feats.max(dim=1)\n",
        "            elif self.temporal_pool == \"attn\":\n",
        "                clip_feat = self.attn_pool(feats)\n",
        "            else:\n",
        "                clip_feat = feats.mean(dim=1)\n",
        "\n",
        "            logits = self.classifier(clip_feat)     # (B, num_classes)\n",
        "            return logits\n",
        "\n",
        "        if x.dim() == 3:  # (C,H,W) rare path\n",
        "            return self.forward(x.unsqueeze(0))\n",
        "\n",
        "        if x.dim() == 4 and x.shape[0] != x.shape[1]:\n",
        "            # If someone fed (T,C,H,W), make it (1,T,C,H,W)\n",
        "            return self.forward(x.unsqueeze(0))\n",
        "\n",
        "        raise ValueError(f\"Unexpected input shape {tuple(x.shape)}\")"
      ],
      "metadata": {
        "id": "eFyQq4vOve73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/dinov3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dZibZf_vmhl",
        "outputId": "e245aabc-7f41-4429-991b-08c2124d66f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dinov3'...\n",
            "remote: Enumerating objects: 409, done.\u001b[K\n",
            "remote: Counting objects: 100% (198/198), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 409 (delta 124), reused 61 (delta 61), pack-reused 211 (from 2)\u001b[K\n",
            "Receiving objects: 100% (409/409), 9.83 MiB | 20.83 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get base dinov3 L model\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1_JqEppwurlG0V0WNsfTiKNzejXP9Rs-U/view?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GqPS_nzw8sY",
        "outputId": "fd44e5d2-ca8c-4b98-a3dd-3263ab8d53ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_JqEppwurlG0V0WNsfTiKNzejXP9Rs-U\n",
            "From (redirected): https://drive.google.com/uc?id=1_JqEppwurlG0V0WNsfTiKNzejXP9Rs-U&confirm=t&uuid=d3ba1bd4-c4fe-47b7-8710-2e49e7976280\n",
            "To: /content/dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth\n",
            "100% 1.21G/1.21G [00:13<00:00, 87.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dinov3_L_model = DINOv3VideoClassifier(\n",
        "    repo_dir='dinov3',\n",
        "    model='dinov3_vitl16',\n",
        "    pretrained=True,\n",
        "    weights_path='dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth',\n",
        "    fine_tune_backbone=False,   # start frozen, then unfreeze\n",
        "    complex_head=True,\n",
        "    temporal_pool=\"attn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "miaEVNyVw-oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Optional: load weights from checkpoint**"
      ],
      "metadata": {
        "id": "vf01hSLgnLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load weights from previous run\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1oO037Eer4aU2OmY4XjJGNiNgzNImVOaq/view?usp=sharing\n",
        "\n",
        "dinov3_L_model.load_state_dict(torch.load('L_iter1_base.pth'))\n",
        "print(f\"Model weights loaded successfully from {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "8SpDF2X-m3_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Main training loop**"
      ],
      "metadata": {
        "id": "SbCblhb1nXdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fit(model= None, train_loader = None, valid_loader= None,\n",
        "        optimizer = None, scheduler = None,\n",
        "        num_epochs = 50, patience = 5, verbose = True,\n",
        "        use_mixup = False, mixup_alpha = 0.15, mixup_prob = 0.4,\n",
        "        use_cutmix = False, cutmix_alpha = 1.0, cutmix_prob = 0.4,\n",
        "        history = None, check_interval = 1\n",
        "       ):\n",
        "\n",
        "    model.to(device)\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=0.001) if optimizer is None else optimizer\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        warmup_epochs = 5\n",
        "\n",
        "        if epoch < warmup_epochs:\n",
        "            return (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Check to prevent ZeroDivisionError\n",
        "            cos_epochs = num_epochs - warmup_epochs\n",
        "            if cos_epochs <= 0:\n",
        "                return 1.0  # Return the last warmup multiplier (or a suitable value)\n",
        "\n",
        "            cos_epoch = epoch - warmup_epochs\n",
        "            return 0.5 * (1 + np.cos(np.pi * cos_epoch / cos_epochs))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda) if scheduler is None else scheduler\n",
        "\n",
        "    if history is None:\n",
        "        history = dict()\n",
        "        history['val_loss'] = list()\n",
        "        history['val_acc'] = list()\n",
        "        history['train_loss'] = list()\n",
        "        history['train_acc'] = list()\n",
        "        history['best_valacc'] = 0\n",
        "        history['best_valacc_model'] = None\n",
        "        history['best_valloss'] = 100\n",
        "        history['best_valloss_model'] = None\n",
        "        history['best_trainacc'] = 0\n",
        "        history['best_trainacc_model'] = None\n",
        "        history['best_trainloss'] = 100\n",
        "        history['best_trainloss_model'] = None\n",
        "\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "\n",
        "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "\n",
        "        for (X, y) in train_loop:\n",
        "            # Move input data to the same device as the model\n",
        "            X,y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            if use_mixup and np.random.rand() < mixup_prob:\n",
        "                mixed_X, y_a, y_b, lam = mixup_data(X, y, alpha=mixup_alpha)\n",
        "                outputs = model(mixed_X.type(torch.float32))\n",
        "                loss = mix_criterion(loss_fn, outputs, y_a.type(torch.long), y_b.type(torch.long), lam)\n",
        "\n",
        "            elif use_cutmix and np.random.rand() < cutmix_prob:\n",
        "                mixed_X, y_a, y_b, lam = cutmix_data(X, y, alpha=cutmix_alpha)\n",
        "                outputs = model(mixed_X)\n",
        "                loss = mix_criterion(loss_fn, outputs, y_a, y_b, lam)\n",
        "\n",
        "            else:\n",
        "                outputs = model(X.type(torch.float32))\n",
        "                loss = loss_fn(outputs, y.type(torch.long))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        # train_acc  = acc.compute().item()\n",
        "        # train_f1   = f1.compute().item()\n",
        "        # val_acc = acc.compute().item()\n",
        "        # val_f1  = f1.compute().item()\n",
        "\n",
        "        # scheduler.step()\n",
        "\n",
        "        # Clear cache to prevent CUDA OOM for extremely large models\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Losses and accuracies for epoch\n",
        "        # if (epoch + 1) % check_interval == 0 or epoch == num_epochs - 1:\n",
        "        if (epoch + 1) % check_interval == 0:\n",
        "            val_loss = compute_loss(model, loss_fn, valid_loader)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            val_acc = compute_acc(model, valid_loader)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            train_loss = compute_loss(model, loss_fn, train_loader)\n",
        "            history['train_loss'].append(train_loss)\n",
        "            train_acc = compute_acc(model, train_loader)\n",
        "            history['train_acc'].append(train_acc)\n",
        "\n",
        "            # Track four models: highest validation accuracy, lowest validation loss, highest training accuracy, lowest training loss\n",
        "            if val_acc > history['best_valacc']:\n",
        "                print('Best val acc change:', val_acc)\n",
        "                history['best_valacc'] = val_acc\n",
        "                history['best_valacc_model'] = copy.deepcopy(model.state_dict())\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                if patience_counter is None: patience_counter = 0\n",
        "                patience_counter += 1\n",
        "\n",
        "            if val_loss < history['best_valloss']:\n",
        "                print('Best val loss change:', val_loss)\n",
        "                history['best_valloss'] = val_loss\n",
        "                history['best_valloss_model'] = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if train_acc > history['best_trainacc']:\n",
        "                print('Best train acc change:', train_acc)\n",
        "                history['best_trainacc'] = train_acc\n",
        "                history['best_trainacc_model'] = copy.deepcopy(model.state_dict())\n",
        "                best_train_acc = train_acc\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                if patience_counter is None: patience_counter = 0\n",
        "                patience_counter += 1\n",
        "\n",
        "            if train_loss < history['best_trainloss']:\n",
        "                print('Best train loss change:', train_loss)\n",
        "                history['best_trainloss'] = train_loss\n",
        "                history['best_trainloss_model'] = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            end = time.time()\n",
        "            print(f\"total time for each epoch {end - start}\") # time in seconds\n",
        "            if verbose:\n",
        "                # print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "                print(f\"train loss= {train_loss:.4f} - train acc= {train_acc*100:.2f}% - valid loss= {val_loss:.4f} - valid acc= {val_acc*100:.2f}%\\n\")\n",
        "\n",
        "        else:\n",
        "            if verbose:\n",
        "                end = time.time()\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs} - total time for training loop {end - start}\\n\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}! No improvement for {patience} epochs.\\n\")\n",
        "                break\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "yXDAY8gmBImr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define training components**"
      ],
      "metadata": {
        "id": "NIcUZCSXnRle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(dinov3_L_model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "acc = MulticlassAccuracy(num_classes=num_classes, average='micro').to(device)\n",
        "f1  = MulticlassF1Score(num_classes=num_classes, average='macro').to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNULCIKv0RyL",
        "outputId": "330be02b-67af-4ba5-976a-510f11b2ae7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-474862355.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to obtain a fresh set of history\n",
        "\n",
        "new_history = dict()\n",
        "new_history['val_loss'] = list()\n",
        "new_history['val_acc'] = list()\n",
        "new_history['train_loss'] = list()\n",
        "new_history['train_acc'] = list()\n",
        "new_history['best_valacc'] = 0\n",
        "new_history['best_valacc_model'] = None\n",
        "new_history['best_valloss'] = 100\n",
        "new_history['best_valloss_model'] = None\n",
        "new_history['best_trainacc'] = 0\n",
        "new_history['best_trainacc_model'] = None\n",
        "new_history['best_trainloss'] = 100\n",
        "new_history['best_trainloss_model'] = None"
      ],
      "metadata": {
        "id": "HiTK-sxMxxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit(\n",
        "    model = dinov3_L_model,\n",
        "\n",
        "    train_loader = train_loader, valid_loader = val_loader,\n",
        "\n",
        "    optimizer = optimizer, scheduler = None,\n",
        "\n",
        "    num_epochs = 10, verbose = True, patience = 5,\n",
        "\n",
        "    use_mixup = False, mixup_alpha = 0.75, mixup_prob = 0.4,\n",
        "    use_cutmix = False, cutmix_alpha = 1.0, cutmix_prob = 0.75,\n",
        "\n",
        "    # Change to history to continue from previous run.\n",
        "    history = new_history,\n",
        "\n",
        "    # Only calculate training and validation accuracy and loss every n rounds.\n",
        "    check_interval = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9chRYo7Dxx2w",
        "outputId": "1c5bfe96-e43b-4c0f-ffdc-83693d54e6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - total time for training loop 62.69083213806152\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.22s/it]\n",
            "Computing Loss: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Accuracy: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Loss: 100%|██████████| 51/51 [01:03<00:00,  1.25s/it]\n",
            "Computing Accuracy: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val acc change: 0.2\n",
            "Best val loss change: 2.204367092677525\n",
            "Best train acc change: 0.27970297029702973\n",
            "Best train loss change: 2.085348218095069\n",
            "total time for each epoch 206.37662601470947\n",
            "train loss= 2.0853 - train acc= 27.97% - valid loss= 2.2044 - valid acc= 20.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - total time for training loop 63.30129957199097\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n",
            "Computing Loss: 100%|██████████| 7/7 [00:08<00:00,  1.17s/it]\n",
            "Computing Accuracy: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Loss: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n",
            "Computing Accuracy: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val acc change: 0.26\n",
            "Best val loss change: 1.9819190161568778\n",
            "Best train acc change: 0.38613861386138615\n",
            "Best train loss change: 1.8100769706800872\n",
            "total time for each epoch 206.37111473083496\n",
            "train loss= 1.8101 - train acc= 38.61% - valid loss= 1.9819 - valid acc= 26.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - total time for training loop 63.17489814758301\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n",
            "Computing Loss: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Accuracy: 100%|██████████| 7/7 [00:08<00:00,  1.17s/it]\n",
            "Computing Loss: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n",
            "Computing Accuracy: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val acc change: 0.28\n",
            "Best val loss change: 1.8436562504087175\n",
            "Best train loss change: 1.6530042068631041\n",
            "total time for each epoch 206.31783866882324\n",
            "train loss= 1.6530 - train acc= 34.65% - valid loss= 1.8437 - valid acc= 28.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 - Training: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - total time for training loop 63.57696795463562\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 - Training: 100%|██████████| 51/51 [01:01<00:00,  1.20s/it]\n",
            "Computing Loss: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Accuracy: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Loss: 100%|██████████| 51/51 [01:03<00:00,  1.24s/it]\n",
            "Computing Accuracy: 100%|██████████| 51/51 [01:02<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val acc change: 0.3\n",
            "Best val loss change: 1.7584921632494246\n",
            "Best train acc change: 0.41089108910891087\n",
            "Best train loss change: 1.544458109958499\n",
            "total time for each epoch 204.41567826271057\n",
            "train loss= 1.5445 - train acc= 41.09% - valid loss= 1.7585 - valid acc= 30.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 - Training: 100%|██████████| 51/51 [01:01<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - total time for training loop 61.951104402542114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 - Training: 100%|██████████| 51/51 [01:01<00:00,  1.20s/it]\n",
            "Computing Loss: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Accuracy: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]\n",
            "Computing Loss: 100%|██████████| 51/51 [01:03<00:00,  1.25s/it]\n",
            "Computing Accuracy: 100%|██████████| 51/51 [01:02<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best val acc change: 0.34\n",
            "Best train acc change: 0.4306930693069307\n",
            "Best train loss change: 1.4450528937227585\n",
            "total time for each epoch 204.20862555503845\n",
            "train loss= 1.4451 - train acc= 43.07% - valid loss= 1.7689 - valid acc= 34.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(dinov3_L_model.state_dict(), '/content/L_10e.pth')"
      ],
      "metadata": {
        "id": "fnQAOBVVOyvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate on test set**"
      ],
      "metadata": {
        "id": "rWumfwrwgrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestManager:\n",
        "    \"\"\"\n",
        "    Manages the evaluation process for a SlowFast model on a test set.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: 'Config', device: str):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.num_classes = len(config.labels)\n",
        "        self.model = self._load_model()\n",
        "        self.test_loader = self._create_dataloader()\n",
        "        self.metrics = self._initialize_metrics()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Loads the pre-trained SlowFast model and the fine-tuned checkpoint.\"\"\"\n",
        "        print(\"Loading model and best checkpoint...\")\n",
        "\n",
        "        # Disable the internal hub check for local loading\n",
        "        torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "\n",
        "        model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "        in_dim = model.blocks[-1].proj.in_features\n",
        "        model.blocks[-1].proj = nn.Sequential(\n",
        "            nn.Dropout(p=0.2), # Add a dropout layer\n",
        "            nn.Linear(in_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        # Load the state dictionary from the checkpoint file\n",
        "        ckpt = torch.load(self.config.best_model_path, map_location=self.device)\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def _create_dataloader(self):\n",
        "        \"\"\"Creates and returns the DataLoader for the test set.\"\"\"\n",
        "        test_ds = ClipDataset(os.path.join(self.config.splits_dir, \"test.csv\"), self.config, train=False)\n",
        "        return DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=max(1, self.config.batch_size),\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True,\n",
        "            # collate_fn=slowfast_collate,  # Make sure this is imported if needed\n",
        "            persistent_workers=False\n",
        "        )\n",
        "\n",
        "    def _initialize_metrics(self):\n",
        "        \"\"\"Initializes all the evaluation metrics.\"\"\"\n",
        "        return {\n",
        "            'top1': MulticlassAccuracy(num_classes=self.num_classes, average=\"micro\").to(self.device),\n",
        "            'top3': MulticlassAccuracy(num_classes=self.num_classes, top_k=3).to(self.device),\n",
        "            'f1_macro': MulticlassF1Score(num_classes=self.num_classes, average=\"macro\").to(self.device),\n",
        "            'f1_perclass': MulticlassF1Score(num_classes=self.num_classes, average=None).to(self.device),\n",
        "            'cm': MulticlassConfusionMatrix(num_classes=self.num_classes).to(self.device)\n",
        "        }\n",
        "\n",
        "    def run_inference(self):\n",
        "        \"\"\"Runs the inference loop and computes all metrics and predictions.\"\"\"\n",
        "        print(\"Starting inference on the test set...\")\n",
        "        test_loss = 0.0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad(), torch.amp.autocast(self.device, enabled=(self.device == \"cuda\")):\n",
        "            for batch_idx, (slow_fast, y) in enumerate(self.test_loader):\n",
        "                # Ensure input tensors are lists\n",
        "                if not isinstance(slow_fast, list):\n",
        "                    slow_fast = [slow_fast]\n",
        "\n",
        "                slow_fast = [t.to(self.device, non_blocking=True) for t in slow_fast]\n",
        "                y = y.to(self.device, non_blocking=True)\n",
        "\n",
        "                logits = self.model(slow_fast)\n",
        "                loss = criterion(logits, y)\n",
        "                test_loss += loss.item() * y.size(0)\n",
        "\n",
        "                # Update metrics\n",
        "                for metric in self.metrics.values():\n",
        "                    metric.update(logits, y)\n",
        "\n",
        "                # Collect per-sample predictions for later saving\n",
        "                probs = self.softmax(logits)\n",
        "                conf, pred = probs.max(dim=1)\n",
        "                topk_conf, topk_idx = probs.topk(3, dim=1)\n",
        "\n",
        "                start_idx = batch_idx * self.test_loader.batch_size\n",
        "\n",
        "                for i in range(y.size(0)):\n",
        "                    idx = start_idx + i\n",
        "                    path = self.test_loader.dataset.items[idx][0]\n",
        "                    row = {\n",
        "                        \"path\": path,\n",
        "                        \"file\": os.path.basename(path),\n",
        "                        \"true_idx\": int(y[i]),\n",
        "                        \"true_label\": self.config.labels[int(y[i])],\n",
        "                        \"pred_idx\": int(pred[i]),\n",
        "                        \"pred_label\": self.config.labels[int(pred[i])],\n",
        "                        \"pred_prob\": float(conf[i]),\n",
        "                        \"top1_label\": self.config.labels[int(topk_idx[i,0])],\n",
        "                        \"top1_prob\":  float(topk_conf[i,0]),\n",
        "                        \"top2_label\": self.config.labels[int(topk_idx[i,1])],\n",
        "                        \"top2_prob\":  float(topk_conf[i,1]),\n",
        "                        \"top3_label\": self.config.labels[int(topk_idx[i,2])],\n",
        "                        \"top3_prob\":  float(topk_conf[i,2]),\n",
        "                    }\n",
        "                    all_predictions.append(row)\n",
        "\n",
        "        test_loss /= len(self.test_loader.dataset)\n",
        "        return test_loss, all_predictions\n",
        "\n",
        "    def compute_and_print_results(self, test_loss):\n",
        "        \"\"\"Computes and prints the final metrics.\"\"\"\n",
        "        acc1 = self.metrics['top1'].compute().item()\n",
        "        acc3 = self.metrics['top3'].compute().item()\n",
        "        f1M = self.metrics['f1_macro'].compute().item()\n",
        "        percls = self.metrics['f1_perclass'].compute().detach().cpu().tolist()\n",
        "        confmat = self.metrics['cm'].compute().detach().cpu().numpy()\n",
        "\n",
        "        print(f\"\\nTEST: loss={test_loss:.4f} | acc@1={acc1*100:.2f}% | acc@3={acc3*100:.2f}% | macro-F1={f1M:.3f}\")\n",
        "        print(\"\\nPer-class F1:\")\n",
        "        for lab, s in sorted(zip(self.config.labels, percls), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {lab:15s} {s:.3f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\n",
        "        print(confmat)\n",
        "\n",
        "    def save_predictions(self, predictions: list, print_n: int=10):\n",
        "        \"\"\"Saves the list of predictions to a CSV file.\"\"\"\n",
        "        df = pd.DataFrame(predictions)\n",
        "        save_path = os.path.join(self.config.models_dir, \"test_predictions.csv\")\n",
        "        df.to_csv(save_path, index=False)\n",
        "        print(f\"\\nSaved per-sample predictions to: {save_path}\")\n",
        "        print(\"\\nQuick peek at the predictions:\")\n",
        "        print(df.head(print_n)[[\"file\", \"true_label\", \"pred_label\", \"pred_prob\", \"top2_label\", \"top2_prob\", \"top3_label\", \"top3_prob\"]])"
      ],
      "metadata": {
        "id": "A9WccyBwMrMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "test_manager = TestManager(cfg, device)\n",
        "test_loss, all_predictions = test_manager.run_inference()\n",
        "test_manager.compute_and_print_results(test_loss)\n",
        "test_manager.save_predictions(all_predictions, print_n=len(all_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO-6i20oihsn",
        "outputId": "190bdc71-3e5c-44f7-9de0-e0481cb2edb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and best checkpoint...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting inference on the test set...\n",
            "\n",
            "TEST: loss=1.5431 | acc@1=57.69% | acc@3=75.73% | macro-F1=0.447\n",
            "\n",
            "Per-class F1:\n",
            "  serve           1.000\n",
            "  jump_smash      0.857\n",
            "  lift            0.769\n",
            "  straight_net    0.696\n",
            "  cross_net       0.500\n",
            "  drop            0.400\n",
            "  clear           0.250\n",
            "  smash           0.000\n",
            "  block           0.000\n",
            "  drive           0.000\n",
            "  push            0.000\n",
            "  tap             0.000\n",
            "  average_joe     0.000\n",
            "\n",
            "Confusion Matrix (rows=true, cols=predicted):\n",
            "[[ 0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
            " [ 0  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  0  1  1  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  1  1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 10  0  1  1  0  1  0  0]\n",
            " [ 0  0  0  0  3  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  8  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  1  0  3  4  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  3  0  0  0]\n",
            " [ 0  0  0  0  0  1  1  1  2  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "\n",
            "Saved per-sample predictions to: /content/drive/MyDrive/FIT3163,3164/SlowFast/07_models/3in1_train3/test_predictions.csv\n",
            "\n",
            "Quick peek at the predictions:\n",
            "              file    true_label    pred_label  pred_prob    top2_label  \\\n",
            "0   5c708fa987.mp4          drop         clear   0.343933          drop   \n",
            "1   afcd8f974d.mp4          lift          lift   0.951497  straight_net   \n",
            "2   536c881a94.mp4    jump_smash    jump_smash   0.832652          drop   \n",
            "3   3f06a7393e.mp4         drive         clear   0.296031         drive   \n",
            "4   ab83731f5d.mp4          lift          lift   0.690797         clear   \n",
            "5   7561f13887.mp4     cross_net     cross_net   0.995091          push   \n",
            "6   e553b446af.mp4         smash     cross_net   0.997572          push   \n",
            "7   1aeba18e58.mp4  straight_net  straight_net   0.866147          lift   \n",
            "8   96eed883d7.mp4         clear         clear   0.725631    jump_smash   \n",
            "9   7daf4997a6.mp4     cross_net  straight_net   0.789317     cross_net   \n",
            "10  5f6bad2aa9.mp4  straight_net  straight_net   0.731871          lift   \n",
            "11  7c30236b94.mp4     cross_net     cross_net   0.999402         drive   \n",
            "12  e2a8610d4a.mp4          push     cross_net   0.745575          push   \n",
            "13  c67cbda531.mp4          lift          lift   0.866078          push   \n",
            "14  1ee37e4121.mp4         serve         serve   1.000000         smash   \n",
            "15  ca2df5a73c.mp4          lift  straight_net   0.164825          lift   \n",
            "16  1e6a478b28.mp4  straight_net  straight_net   0.983787          push   \n",
            "17  8ad079d707.mp4  straight_net  straight_net   0.999699          drop   \n",
            "18  03b69ddc77.mp4          lift          push   0.392367  straight_net   \n",
            "19  cdf2145af3.mp4    jump_smash    jump_smash   0.440167         clear   \n",
            "20  5ec10cded7.mp4         serve         serve   1.000000          push   \n",
            "21  055439e167.mp4          lift          lift   0.934582  straight_net   \n",
            "22  301741faa6.mp4          push     cross_net   0.928181          push   \n",
            "23  53a50cdebc.mp4     cross_net          lift   0.746533          push   \n",
            "24  59daa684b4.mp4          lift     cross_net   0.276962          lift   \n",
            "25  179cf04236.mp4    jump_smash    jump_smash   0.535707          drop   \n",
            "26  3832e42975.mp4          lift          lift   0.907455         clear   \n",
            "27  aa81076d2b.mp4          push         drive   0.824931         block   \n",
            "28  798093f8cf.mp4         serve         serve   0.999996          lift   \n",
            "29  b848f1a992.mp4  straight_net  straight_net   0.943305          lift   \n",
            "30  7e74de3477.mp4     cross_net     cross_net   0.998278          push   \n",
            "31  eabe95dd19.mp4          drop          drop   0.581834    jump_smash   \n",
            "32  eb7ba507f2.mp4  straight_net  straight_net   0.999975          drop   \n",
            "33  25ef888817.mp4          lift          lift   0.486159  straight_net   \n",
            "34  8150c8a453.mp4         clear          drop   0.351249         clear   \n",
            "35  a5681f71d6.mp4          push          lift   0.596826  straight_net   \n",
            "36  364c6bcbf1.mp4          lift          lift   0.253807          push   \n",
            "37  c44388c37b.mp4     cross_net  straight_net   0.646057          lift   \n",
            "38  65af47769e.mp4  straight_net         drive   0.457095         block   \n",
            "39  ce4e3f4c76.mp4          lift          lift   0.692277          push   \n",
            "40  37f138f5a5.mp4         clear          lift   0.467164  straight_net   \n",
            "41  ebab7d7654.mp4     cross_net  straight_net   0.474225          push   \n",
            "42  8b668ee246.mp4     cross_net     cross_net   0.991506          push   \n",
            "43  f5c1e87f95.mp4          drop    jump_smash   0.307699         drive   \n",
            "44  d24bb91e3e.mp4  straight_net  straight_net   0.999560          drop   \n",
            "45  007356ca4f.mp4         drive         clear   0.394077          lift   \n",
            "46  5fa97098fc.mp4         drive         clear   0.493458         drive   \n",
            "47  201801ea69.mp4          lift          lift   0.794014         clear   \n",
            "48  11592f13f0.mp4  straight_net  straight_net   0.965902          lift   \n",
            "49  571830f0ab.mp4          lift          lift   0.989418  straight_net   \n",
            "50  f34a948eea.mp4          push  straight_net   0.616161          push   \n",
            "51  0c5c1d1d20.mp4  straight_net          push   0.381927     cross_net   \n",
            "\n",
            "       top2_prob    top3_label     top3_prob  \n",
            "0   3.256289e-01         drive  1.067537e-01  \n",
            "1   4.677464e-02         clear  5.303856e-04  \n",
            "2   1.529011e-01           tap  8.052187e-03  \n",
            "3   1.827357e-01    jump_smash  1.693333e-01  \n",
            "4   2.818463e-01         drive  1.022623e-02  \n",
            "5   4.810508e-03           tap  3.940413e-05  \n",
            "6   1.923887e-03         drive  4.347618e-04  \n",
            "7   1.260049e-01     cross_net  5.779338e-03  \n",
            "8   9.014960e-02          drop  8.669602e-02  \n",
            "9   8.345785e-02          push  6.745435e-02  \n",
            "10  1.159686e-01     cross_net  9.442003e-02  \n",
            "11  4.059862e-04          push  1.559114e-04  \n",
            "12  2.435941e-01  straight_net  3.958537e-03  \n",
            "13  5.378108e-02  straight_net  3.300438e-02  \n",
            "14  2.741272e-09         block  7.235330e-10  \n",
            "15  1.383908e-01    jump_smash  1.200014e-01  \n",
            "16  5.658771e-03          lift  4.364227e-03  \n",
            "17  1.781091e-04           tap  4.197541e-05  \n",
            "18  1.856129e-01         block  1.201923e-01  \n",
            "19  3.553286e-01          drop  1.224390e-01  \n",
            "20  1.444980e-07         smash  5.680770e-08  \n",
            "21  3.084475e-02          push  2.115790e-02  \n",
            "22  4.616630e-02         drive  7.369052e-03  \n",
            "23  8.566122e-02     cross_net  4.693848e-02  \n",
            "24  2.734677e-01          push  1.592011e-01  \n",
            "25  4.546486e-01         clear  5.095277e-03  \n",
            "26  5.562609e-02         serve  1.285597e-02  \n",
            "27  6.379846e-02          push  3.195482e-02  \n",
            "28  1.881206e-06          drop  6.866712e-07  \n",
            "29  2.426981e-02     cross_net  1.648605e-02  \n",
            "30  1.503784e-03         drive  2.011441e-04  \n",
            "31  3.215546e-01         clear  3.990483e-02  \n",
            "32  1.937382e-05          lift  3.951435e-06  \n",
            "33  4.345156e-01         clear  4.323345e-02  \n",
            "34  2.449718e-01  straight_net  8.432995e-02  \n",
            "35  2.481117e-01          push  8.955348e-02  \n",
            "36  1.972795e-01         block  1.722384e-01  \n",
            "37  3.045061e-01         clear  2.815800e-02  \n",
            "38  1.790009e-01     cross_net  8.195259e-02  \n",
            "39  7.484183e-02  straight_net  6.326980e-02  \n",
            "40  1.956961e-01     cross_net  1.649537e-01  \n",
            "41  1.458360e-01          lift  1.351399e-01  \n",
            "42  8.237006e-03         drive  1.565029e-04  \n",
            "43  1.877254e-01         clear  1.527688e-01  \n",
            "44  3.646926e-04         clear  3.799050e-05  \n",
            "45  1.818367e-01    jump_smash  1.582913e-01  \n",
            "46  2.529123e-01         smash  1.282244e-01  \n",
            "47  1.145002e-01    jump_smash  2.682690e-02  \n",
            "48  2.074369e-02          push  1.088854e-02  \n",
            "49  4.863152e-03         clear  3.290573e-03  \n",
            "50  1.177401e-01          lift  8.870146e-02  \n",
            "51  1.504803e-01  straight_net  1.465645e-01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **end-to-end match inference & overlay**"
      ],
      "metadata": {
        "id": "nU9T0fc3nRx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics opencv-python-headless"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug-sBuM4nkRl",
        "outputId": "ae3d3ee4-bf4d-417c-d341-dbe9e3f7eefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.0/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, numpy as np, torch\n",
        "from collections import deque, defaultdict\n",
        "from ultralytics import YOLO\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oaoCKiwUgy97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_slowfast_classifier(cfg, ckpt_path):\n",
        "    torch.hub._validate_not_a_forked_repo = lambda a,b,c: True\n",
        "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r101', pretrained=True)\n",
        "    in_dim = model.blocks[-1].proj.in_features\n",
        "    model.blocks[-1].proj = torch.nn.Linear(in_dim, len(cfg.labels))\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "    model.eval().to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mx6acAn2obMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_pad_square(img_rgb: np.ndarray, side: int = 224) -> np.ndarray:\n",
        "    \"\"\"Keep aspect ratio; resize the longer side to `side`, then pad to (side, side).\"\"\"\n",
        "    h, w = img_rgb.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((side, side, 3), dtype=img_rgb.dtype)\n",
        "    scale = side / max(h, w)\n",
        "    nh, nw = int(round(h * scale)), int(round(w * scale))\n",
        "    resized = cv2.resize(img_rgb, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "    top  = (side - nh) // 2\n",
        "    bottom = side - nh - top\n",
        "    left = (side - nw) // 2\n",
        "    right = side - nw - left\n",
        "    out = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128,128,128))\n",
        "    return out\n",
        "\n",
        "def expand_box(x1, y1, x2, y2, scale: float, W: int, H: int):\n",
        "    \"\"\"Optionally enlarge the bbox to keep some context (e.g., racket).\"\"\"\n",
        "    cx, cy = (x1 + x2) / 2.0, (y1 + y2) / 2.0\n",
        "    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale\n",
        "    nx1, ny1 = int(max(0, cx - bw / 2)), int(max(0, cy - bh / 2))\n",
        "    nx2, ny2 = int(min(W - 1, cx + bw / 2)), int(min(H - 1, cy + bh / 2))\n",
        "    return nx1, ny1, nx2, ny2"
      ],
      "metadata": {
        "id": "iasMXtJYsCZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SlowFastPredictor:\n",
        "    def __init__(self, cfg, model):\n",
        "        self.cfg = cfg\n",
        "        self.model = model\n",
        "        self.mean = torch.tensor([0.45, 0.45, 0.45]).view(3,1,1).to(device)\n",
        "        self.std  = torch.tensor([0.225, 0.225, 0.225]).view(3,1,1).to(device)\n",
        "\n",
        "    def _prep(self, frames_rgb_list):\n",
        "        \"\"\"\n",
        "        frames_rgb_list: list of 32 frames, each HxWx3 in RGB\n",
        "        Returns: [slow, fast] tensors shaped (1,C,T,H,W)\n",
        "        \"\"\"\n",
        "        # Stack to (T,H,W,3) -> (T,C,H,W)\n",
        "        x = torch.from_numpy(np.stack(frames_rgb_list)).permute(0,3,1,2).float() / 255.0  # (T,C,H,W)\n",
        "        # Resize treating T as batch\n",
        "        x = F.interpolate(x, size=self.cfg.side, mode=\"bilinear\", align_corners=False)    # (T,C,224,224)\n",
        "        # Normalize\n",
        "        mean = self.mean.to(device=x.device, dtype=x.dtype)\n",
        "        std  = self.std.to(device=x.device, dtype=x.dtype)\n",
        "        x = (x - mean) / std                                                   # (T,C,224,224)\n",
        "        # (C,T,H,W)\n",
        "        x = x.permute(1,0,2,3)\n",
        "        fast = x.unsqueeze(0).to(device)             # (1,C,32,224,224)\n",
        "        slow = x[:, ::self.cfg.alpha, :, :].unsqueeze(0).to(device)  # stride-4 -> (1,C,8,224,224)\n",
        "        return [slow, fast]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_probs(self, frames_rgb_list):\n",
        "        assert len(frames_rgb_list) == self.cfg.fast_t  # 32\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
        "            inp = self._prep(frames_rgb_list)\n",
        "            logits = self.model(inp)                  # (1, num_classes)\n",
        "            probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "        return probs  # (C,)"
      ],
      "metadata": {
        "id": "PsnwdZrHnVua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate_match_video(\n",
        "    cfg,\n",
        "    video_path,\n",
        "    out_path,\n",
        "    yolo_weights=\"yolo11n.pt\", # change to your custom weights if you have them\n",
        "    person_class=0,            # COCO 'person'\n",
        "    det_conf=0.5,\n",
        "    iou=0.5,\n",
        "    pred_thr=0.60,             # minimum prob to show label\n",
        "    cooldown=12                # frames to cool after showing a shot to reduce spam\n",
        "):\n",
        "    # Get video props for the writer\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = max(1.0, cap.get(cv2.CAP_PROP_FPS))\n",
        "    W   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(out_path, fourcc, fps, (W, H))\n",
        "\n",
        "    # Load detector+tracker\n",
        "    yolo = YOLO(yolo_weights)\n",
        "\n",
        "    # Load classifier\n",
        "    clf_model = load_slowfast_classifier(cfg, cfg.best_model_path)\n",
        "    clf = SlowFastPredictor(cfg, clf_model)\n",
        "\n",
        "    # Per-track state\n",
        "    buffers = defaultdict(lambda: deque(maxlen=cfg.fast_t))            # 32-frame RGB crops per track\n",
        "    last_shown_frame = defaultdict(lambda: -99999)                     # cooldown control\n",
        "    hist = defaultdict(lambda: deque(maxlen=5))                        # small temporal smoothing buffer\n",
        "\n",
        "    frame_idx = 0\n",
        "    for res in yolo.track(source=video_path, stream=True, persist=True,\n",
        "                          classes=[person_class], conf=det_conf, iou=iou, verbose=False):\n",
        "        frame_bgr = res.orig_img  # BGR\n",
        "        h, w = frame_bgr.shape[:2]\n",
        "\n",
        "        # If no boxes/ids in this frame, just write it\n",
        "        if res.boxes is None or res.boxes.id is None:\n",
        "            writer.write(frame_bgr)\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        ids = res.boxes.id.int().cpu().numpy()\n",
        "        xyxy = res.boxes.xyxy.int().cpu().numpy()  # (N,4)\n",
        "\n",
        "        to_draw = []  # (x1,y1,x2,y2,label,prob,tid)\n",
        "\n",
        "        for j, tid in enumerate(ids):\n",
        "            x1, y1, x2, y2 = xyxy[j]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w-1, x2), min(h-1, y2)\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            # NEW: enlarge a bit for context (optional, try 1.2–1.4)\n",
        "            x1, y1, x2, y2 = expand_box(x1, y1, x2, y2, scale=1.25, W=w, H=h)\n",
        "\n",
        "            # Crop -> RGB -> letterbox to fixed square\n",
        "            crop = frame_bgr[y1:y2, x1:x2, :]\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
        "            crop_rgb = resize_pad_square(crop_rgb, side=cfg.side)  # now every frame is 224x224\n",
        "\n",
        "            buffers[tid].append(crop_rgb)\n",
        "\n",
        "            label_to_show = None\n",
        "            prob_to_show  = 0.0\n",
        "\n",
        "            # Classify when we have a full 32-frame clip\n",
        "            if len(buffers[tid]) == cfg.fast_t:\n",
        "                probs = clf.predict_probs(list(buffers[tid]))  # (C,)\n",
        "                ci = int(probs.argmax())\n",
        "                pi = float(probs[ci])\n",
        "                hist[tid].append((ci, pi))\n",
        "\n",
        "                # Small smoothing: require at least 2 of the last 3 agreeing + prob >= thr\n",
        "                if len(hist[tid]) >= 3:\n",
        "                    last3 = list(hist[tid])[-3:]\n",
        "                else:\n",
        "                    last3 = list(hist[tid])\n",
        "\n",
        "                # Choose the label with the highest mean prob among last3\n",
        "                if last3:\n",
        "                    classes = [c for c, p in last3 if cfg.labels[c] != \"average_joe\" and p >= pred_thr]\n",
        "                    if classes:\n",
        "                        # pick the most common; break ties by highest avg prob\n",
        "                        uniq = set(classes)\n",
        "                        best_c, best_score = None, -1.0\n",
        "                        for u in uniq:\n",
        "                            avgp = np.mean([p for (c, p) in last3 if c == u])\n",
        "                            score = (classes.count(u), avgp)  # (count, avgp)\n",
        "                            if score > (classes.count(best_c) if best_c is not None else -1, best_score):\n",
        "                                best_c, best_score = u, avgp\n",
        "                        if best_c is not None and (frame_idx - last_shown_frame[tid] >= cooldown):\n",
        "                            label_to_show = cfg.labels[best_c]\n",
        "                            prob_to_show = float(best_score)\n",
        "                            last_shown_frame[tid] = frame_idx\n",
        "\n",
        "            # Queue drawing if we have a confident non-background label\n",
        "            if label_to_show is not None:\n",
        "                to_draw.append((x1, y1, x2, y2, label_to_show, prob_to_show, int(tid)))\n",
        "\n",
        "        # ---- Draw all overlays on this frame ----\n",
        "        for (x1, y1, x2, y2, lab, p, tid) in to_draw:\n",
        "            color = (0, 220, 0)\n",
        "            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n",
        "            txt = f\"#{tid} {lab} {p*100:.1f}%\"\n",
        "            cv2.putText(frame_bgr, txt, (x1, max(20, y1-10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "\n",
        "        writer.write(frame_bgr)\n",
        "        frame_idx += 1\n",
        "\n",
        "    writer.release()\n",
        "    print(f\"Saved annotated video to: {out_path}\")"
      ],
      "metadata": {
        "id": "a36aCc48nXfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_video  = \"/content/drive/MyDrive/FIT3163,3164/SlowFast/01_raw/lcw_ld_2016_short/1/master.mp4\"\n",
        "out_video = \"/content/match_annotated.mp4\"\n",
        "yolo_weights = \"/content/drive/MyDrive/FIT3163,3164/YOLO/my_yolov8_1.pt\"\n",
        "\n",
        "annotate_match_video(cfg, in_video, out_video,\n",
        "                     yolo_weights=yolo_weights,  # swap if you have a better person/badminton model\n",
        "                     det_conf=0.35, iou=0.5,\n",
        "                     pred_thr=0.60, cooldown=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uex745aPnaJK",
        "outputId": "41d184dd-8bf8-4e09-8996-6a67acb9bbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved annotated video to: /content/match_annotated.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utRepM9qoTb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}