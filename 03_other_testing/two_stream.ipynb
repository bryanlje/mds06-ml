{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d00247a-3b9f-42c1-8b25-96185af8111c",
      "metadata": {
        "id": "5d00247a-3b9f-42c1-8b25-96185af8111c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Two-Stream Badminton Shot Classification\n",
        "Vision Stream (R(2+1)D) + Shuttle Stream (1D CNN) → Late Fusion\n",
        "\n",
        "Architecture:\n",
        "┌─────────────────┐    ┌──────────────────┐\n",
        "│ Video Clip      │    │ Shuttle Features │\n",
        "│ (T, H, W, 3)    │    │ (T, F)           │\n",
        "└────────┬────────┘    └────────┬─────────┘\n",
        "         │                      │\n",
        "    ┌────▼────┐            ┌────▼────┐\n",
        "    │ R(2+1)D │            │ 1D CNN  │\n",
        "    │ Vision  │            │ Temporal│\n",
        "    └────┬────┘            └────┬────┘\n",
        "         │                      │\n",
        "    ┌────▼────┐            ┌────▼────┐\n",
        "    │ 512-dim │            │ 128-dim │\n",
        "    └────┬────┘            └────┬────┘\n",
        "         │                      │\n",
        "         └──────────┬───────────┘\n",
        "                    │\n",
        "              ┌─────▼─────┐\n",
        "              │   Fusion  │\n",
        "              │   Layer   │\n",
        "              └─────┬─────┘\n",
        "                    │\n",
        "              ┌─────▼─────┐\n",
        "              │ Classifier│\n",
        "              └───────────┘\n",
        "\n",
        "Shuttle Features (per frame):\n",
        "- Position (x, y)\n",
        "- Velocity (vx, vy, speed)\n",
        "- Acceleration (ax, ay)\n",
        "- Direction (angle, angle_change)\n",
        "- Height change (dy/dt)\n",
        "\"\"\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gc9Mjw4QCZe8"
      },
      "id": "gc9Mjw4QCZe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa13fa8a-58cc-4ab4-ab15-53de21f845d5",
      "metadata": {
        "id": "aa13fa8a-58cc-4ab4-ab15-53de21f845d5"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -q numpy opencv-python opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28429991-9219-4450-9b06-25673b1e651d",
      "metadata": {
        "id": "28429991-9219-4450-9b06-25673b1e651d"
      },
      "outputs": [],
      "source": [
        "!pip install -U numpy==2.3.0 lightning-thunder thinc numba opencv-python opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca3c3022-7f32-4f4d-9fcf-3cfb2cf9d3f1",
      "metadata": {
        "scrolled": true,
        "id": "ca3c3022-7f32-4f4d-9fcf-3cfb2cf9d3f1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import random, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c31b09c-01b2-499a-bfd7-2262f2a57b4e",
      "metadata": {
        "id": "7c31b09c-01b2-499a-bfd7-2262f2a57b4e"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed=1023):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_all(2310)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a5aaa83-27c8-4caf-9ff7-459c2c155fc3",
      "metadata": {
        "id": "9a5aaa83-27c8-4caf-9ff7-459c2c155fc3"
      },
      "source": [
        "# **1. SHUTTLE FEATURE EXTRACTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cedd777-9c07-4853-98bf-ca4bf673d5c7",
      "metadata": {
        "id": "9cedd777-9c07-4853-98bf-ca4bf673d5c7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ShuttleFeatureConfig:\n",
        "    \"\"\"Configuration for shuttle feature extraction\"\"\"\n",
        "    smooth_window: int = 5  # smoothing for coordinates\n",
        "\n",
        "\n",
        "class ShuttleFeatureExtractor:\n",
        "    \"\"\"Extract temporal features from shuttle trajectory\"\"\"\n",
        "\n",
        "    def __init__(self, config: ShuttleFeatureConfig):\n",
        "        self.cfg = config\n",
        "\n",
        "    def load_shuttle_csv(self, csv_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Load shuttle CSV (Frame, Visibility, X, Y)\"\"\"\n",
        "        frames, vis, xs, ys = [], [], [], []\n",
        "        with open(csv_path, 'r') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for r in reader:\n",
        "                frames.append(int(r[\"Frame\"]))\n",
        "                vis.append(int(r[\"Visibility\"]))\n",
        "                xs.append(float(r[\"X\"]))\n",
        "                ys.append(float(r[\"Y\"]))\n",
        "\n",
        "        frames = np.array(frames)\n",
        "        vis = np.array(vis)\n",
        "        xs = np.array(xs, dtype=np.float32)\n",
        "        ys = np.array(ys, dtype=np.float32)\n",
        "\n",
        "        idx = np.argsort(frames)\n",
        "        return frames[idx], vis[idx], xs[idx], ys[idx]\n",
        "\n",
        "    def _smooth(self, arr: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Moving average smoothing\"\"\"\n",
        "        if self.cfg.smooth_window < 2:\n",
        "            return arr\n",
        "        window = self.cfg.smooth_window\n",
        "        if window % 2 == 0:\n",
        "            window += 1\n",
        "        pad = window // 2\n",
        "        padded = np.pad(arr, pad, mode='edge')\n",
        "        kernel = np.ones(window) / window\n",
        "        return np.convolve(padded, kernel, mode='valid').astype(np.float32)\n",
        "\n",
        "    def _fill_invisible(self, xs: np.ndarray, ys: np.ndarray, vis: np.ndarray):\n",
        "        \"\"\"Interpolate invisible points\"\"\"\n",
        "        valid = (vis == 1) & ~((xs == 0) & (ys == 0))\n",
        "        xs_filled = xs.copy()\n",
        "        ys_filled = ys.copy()\n",
        "\n",
        "        if not valid.any():\n",
        "            return xs_filled, ys_filled\n",
        "\n",
        "        # Forward fill\n",
        "        for i in range(1, len(xs)):\n",
        "            if not valid[i]:\n",
        "                xs_filled[i] = xs_filled[i-1]\n",
        "                ys_filled[i] = ys_filled[i-1]\n",
        "\n",
        "        # Backward fill\n",
        "        for i in range(len(xs)-2, -1, -1):\n",
        "            if not valid[i]:\n",
        "                xs_filled[i] = xs_filled[i+1]\n",
        "                ys_filled[i] = ys_filled[i+1]\n",
        "\n",
        "        return xs_filled, ys_filled\n",
        "\n",
        "    def extract_features_for_window(\n",
        "        self,\n",
        "        frames: np.ndarray,\n",
        "        vis: np.ndarray,\n",
        "        xs: np.ndarray,\n",
        "        ys: np.ndarray,\n",
        "        center_frame: int,\n",
        "        window_size: int\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract shuttle features for a temporal window.\n",
        "\n",
        "        Returns: (window_size, num_features) array\n",
        "        Features per frame: [x, y, vx, vy, speed, ax, ay, direction, dir_change, height_change]\n",
        "        \"\"\"\n",
        "        # Find frame indices\n",
        "        start_frame = center_frame - window_size // 2\n",
        "        end_frame = start_frame + window_size\n",
        "\n",
        "        # Get indices in arrays\n",
        "        frame_mask = (frames >= start_frame) & (frames < end_frame)\n",
        "\n",
        "        if not frame_mask.any():\n",
        "            # Return zero features if no data\n",
        "            return np.zeros((window_size, 10), dtype=np.float32)\n",
        "\n",
        "        # Extract window data\n",
        "        frames_win = frames[frame_mask]\n",
        "        vis_win = vis[frame_mask]\n",
        "        xs_win = xs[frame_mask]\n",
        "        ys_win = ys[frame_mask]\n",
        "\n",
        "        # Fill and smooth\n",
        "        xs_filled, ys_filled = self._fill_invisible(xs_win, ys_win, vis_win)\n",
        "        xs_smooth = self._smooth(xs_filled)\n",
        "        ys_smooth = self._smooth(ys_filled)\n",
        "\n",
        "        # Compute derivatives\n",
        "        vx = np.diff(xs_smooth, prepend=xs_smooth[0])\n",
        "        vy = np.diff(ys_smooth, prepend=ys_smooth[0])\n",
        "        speed = np.sqrt(vx**2 + vy**2)\n",
        "\n",
        "        ax = np.diff(vx, prepend=vx[0])\n",
        "        ay = np.diff(vy, prepend=vy[0])\n",
        "\n",
        "        direction = np.arctan2(vy, vx)\n",
        "        direction_change = np.diff(direction, prepend=direction[0])\n",
        "\n",
        "        height_change = vy  # positive = downward (assuming y+ is down)\n",
        "\n",
        "        # Stack features\n",
        "        features = np.stack([\n",
        "            xs_smooth, ys_smooth,\n",
        "            vx, vy, speed,\n",
        "            ax, ay,\n",
        "            direction, direction_change,\n",
        "            height_change\n",
        "        ], axis=1)  # (T, 10)\n",
        "\n",
        "        # Pad or truncate to exact window_size\n",
        "        if len(features) < window_size:\n",
        "            padding = np.zeros((window_size - len(features), 10), dtype=np.float32)\n",
        "            features = np.vstack([features, padding])\n",
        "        elif len(features) > window_size:\n",
        "            features = features[:window_size]\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf45ca22-234e-4b67-87b7-e927a321f023",
      "metadata": {
        "id": "bf45ca22-234e-4b67-87b7-e927a321f023"
      },
      "source": [
        "# **2. DATASET (Two-Stream)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7866ce-161a-4e3b-bce0-d3881d848c11",
      "metadata": {
        "id": "4e7866ce-161a-4e3b-bce0-d3881d848c11"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ClipExtractionConfig:\n",
        "    \"\"\"Configuration for clip extraction\"\"\"\n",
        "    pad_frames: int = 15  # frames before/after contact (0.5s at 30fps)\n",
        "    clip_size: Tuple[int, int] = (112, 112)  # spatial size for model input\n",
        "    bbox_expansion: float = 0.3  # expand bbox by 30% to include racket/motion\n",
        "    min_bbox_size: int = 50  # minimum bbox dimension\n",
        "\n",
        "class ClipExtractor:\n",
        "    \"\"\"Extract player-centered clips from video given tracks and contacts\"\"\"\n",
        "\n",
        "    def __init__(self, config: ClipExtractionConfig):\n",
        "        self.cfg = config\n",
        "\n",
        "    def load_tracks(self, track_csv_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load player tracks CSV\"\"\"\n",
        "        df = pd.read_csv(track_csv_path)\n",
        "        return df\n",
        "\n",
        "    def get_player_bbox(\n",
        "        self,\n",
        "        tracks_df: pd.DataFrame,\n",
        "        frame: int,\n",
        "        player_id: int\n",
        "    ) -> Optional[Tuple[int, int, int, int]]:\n",
        "        \"\"\"Get bbox for player at specific frame\"\"\"\n",
        "        row = tracks_df[(tracks_df['frame'] == frame) & (tracks_df['id'] == player_id)]\n",
        "        if len(row) == 0:\n",
        "            return None\n",
        "        row = row.iloc[0]\n",
        "        return (int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2']))\n",
        "\n",
        "    def expand_bbox(\n",
        "        self,\n",
        "        bbox: Tuple[int, int, int, int],\n",
        "        img_shape: Tuple[int, int]\n",
        "    ) -> Tuple[int, int, int, int]:\n",
        "        \"\"\"Expand bbox to include motion/racket\"\"\"\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        h, w = img_shape\n",
        "\n",
        "        # Expand\n",
        "        bw, bh = x2 - x1, y2 - y1\n",
        "        dx = int(bw * self.cfg.bbox_expansion)\n",
        "        dy = int(bh * self.cfg.bbox_expansion)\n",
        "\n",
        "        x1 = max(0, x1 - dx)\n",
        "        y1 = max(0, y1 - dy)\n",
        "        x2 = min(w, x2 + dx)\n",
        "        y2 = min(h, y2 + dy)\n",
        "\n",
        "        return (x1, y1, x2, y2)\n",
        "\n",
        "    def interpolate_missing_bbox(\n",
        "        self,\n",
        "        tracks_df: pd.DataFrame,\n",
        "        frame: int,\n",
        "        player_id: int,\n",
        "        search_window: int = 5\n",
        "    ) -> Optional[Tuple[int, int, int, int]]:\n",
        "        \"\"\"Interpolate bbox if missing at exact frame\"\"\"\n",
        "        # Try exact frame first\n",
        "        bbox = self.get_player_bbox(tracks_df, frame, player_id)\n",
        "        if bbox is not None:\n",
        "            return bbox\n",
        "\n",
        "        # Search nearby frames\n",
        "        for offset in range(1, search_window + 1):\n",
        "            # Try before\n",
        "            bbox = self.get_player_bbox(tracks_df, frame - offset, player_id)\n",
        "            if bbox is not None:\n",
        "                return bbox\n",
        "            # Try after\n",
        "            bbox = self.get_player_bbox(tracks_df, frame + offset, player_id)\n",
        "            if bbox is not None:\n",
        "                return bbox\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_clip(\n",
        "        self,\n",
        "        video_path: str,\n",
        "        tracks_df: pd.DataFrame,\n",
        "        contact_frame: int,\n",
        "        player_id: int\n",
        "    ) -> Optional[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Extract clip for one player around contact frame.\n",
        "\n",
        "        Returns: (T, H, W, C) array or None if failed\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        start_frame = max(0, contact_frame - self.cfg.pad_frames)\n",
        "        end_frame = contact_frame + self.cfg.pad_frames + 1\n",
        "\n",
        "        frames = []\n",
        "        for frame_idx in range(start_frame, end_frame):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Get bbox for this player at this frame\n",
        "            bbox = self.interpolate_missing_bbox(tracks_df, frame_idx, player_id)\n",
        "            if bbox is None:\n",
        "                # Use previous bbox if available\n",
        "                if len(frames) > 0:\n",
        "                    bbox = self._last_bbox\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            self._last_bbox = bbox\n",
        "\n",
        "            # Expand and crop\n",
        "            bbox_exp = self.expand_bbox(bbox, frame.shape[:2])\n",
        "            x1, y1, x2, y2 = bbox_exp\n",
        "\n",
        "            # Check minimum size\n",
        "            if (x2 - x1) < self.cfg.min_bbox_size or (y2 - y1) < self.cfg.min_bbox_size:\n",
        "                continue\n",
        "\n",
        "            cropped = frame[y1:y2, x1:x2]\n",
        "\n",
        "            # Resize to fixed size\n",
        "            resized = cv2.resize(cropped, self.cfg.clip_size)\n",
        "            frames.append(resized)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if len(frames) < (self.cfg.pad_frames * 2 + 1) * 0.8:  # at least 80% frames\n",
        "            return None\n",
        "\n",
        "        # Pad if needed\n",
        "        target_len = self.cfg.pad_frames * 2 + 1\n",
        "        while len(frames) < target_len:\n",
        "            frames.append(frames[-1])  # repeat last frame\n",
        "\n",
        "        return np.stack(frames[:target_len], axis=0)  # (T, H, W, C)\n",
        "\n",
        "    def extract_all_clips_for_contact(\n",
        "        self,\n",
        "        video_path: str,\n",
        "        tracks_df: pd.DataFrame,\n",
        "        contact_frame: int\n",
        "    ) -> Dict[int, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Extract clips for ALL players around a contact frame.\n",
        "\n",
        "        Returns: {player_id: clip_array}\n",
        "        \"\"\"\n",
        "        player_ids = tracks_df['id'].unique()\n",
        "        clips = {}\n",
        "\n",
        "        for pid in player_ids:\n",
        "            clip = self.extract_clip(video_path, tracks_df, contact_frame, pid)\n",
        "            if clip is not None:\n",
        "                clips[int(pid)] = clip\n",
        "\n",
        "        return clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e8553b-d61d-49c8-ad60-6bd568c6f21b",
      "metadata": {
        "id": "51e8553b-d61d-49c8-ad60-6bd568c6f21b"
      },
      "outputs": [],
      "source": [
        "class TwoStreamDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Two-stream dataset: Video clips + Shuttle features\n",
        "\n",
        "    Each sample contains:\n",
        "    - Video clip: (T, H, W, C)\n",
        "    - Shuttle features: (T, F)\n",
        "    - Label: shot class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        video_paths: List[str],\n",
        "        tracks_csv_paths: List[str],\n",
        "        shuttle_csv_paths: List[str],\n",
        "        contact_frames: Dict[str, List[int]],\n",
        "        labels: Dict[str, Dict[str, Dict[int, str]]],\n",
        "        clip_extractor,\n",
        "        shuttle_extractor: ShuttleFeatureExtractor,\n",
        "        shot_classes: List[str],\n",
        "        clip_cfg: ClipExtractionConfig,\n",
        "        augment: bool = True\n",
        "    ):\n",
        "        self.clip_extractor = clip_extractor\n",
        "        self.shuttle_extractor = shuttle_extractor\n",
        "        self.shot_classes = shot_classes\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(shot_classes)}\n",
        "        self.augment = augment\n",
        "        self.clip_cfg = clip_cfg\n",
        "\n",
        "        # Cache shuttle data\n",
        "        self.shuttle_cache = {}\n",
        "        for shuttle_csv in shuttle_csv_paths:\n",
        "            video_name = shuttle_csv.split('/')[-1].replace('_shuttle', '').replace('.csv', '.mp4')\n",
        "            frames, vis, xs, ys = shuttle_extractor.load_shuttle_csv(shuttle_csv)\n",
        "            self.shuttle_cache[video_name] = (frames, vis, xs, ys)\n",
        "\n",
        "        # Build samples\n",
        "        self.samples = []\n",
        "        for video_path, tracks_csv in zip(video_paths, tracks_csv_paths):\n",
        "            video_name = video_path.split('/')[-1]\n",
        "            if video_name not in contact_frames:\n",
        "                print(f\"video_name {video_name} not found in contact_frames {contact_frames}.\\n\")\n",
        "                continue\n",
        "\n",
        "            for cf in contact_frames[video_name]:\n",
        "                cf_key = f\"contact_{cf}\"\n",
        "\n",
        "                if video_name not in labels:\n",
        "                    print(f\"video name {video_name} not in labels.\\n\")\n",
        "                    continue\n",
        "\n",
        "                if cf_key not in labels[video_name]:\n",
        "                    print(f\"contact frame key {cf_key} not in {labels}[{video_name}].\\n\")\n",
        "                    continue\n",
        "\n",
        "                player_labels = labels[video_name][cf_key]\n",
        "                for player_id, shot_label in player_labels.items():\n",
        "                    if shot_label not in self.class_to_idx:\n",
        "                        continue\n",
        "\n",
        "                    self.samples.append((\n",
        "                        video_path,\n",
        "                        tracks_csv,\n",
        "                        video_name,\n",
        "                        cf,\n",
        "                        int(player_id),\n",
        "                        self.class_to_idx[shot_label]\n",
        "                    ))\n",
        "\n",
        "        print(f\"Two-Stream Dataset: {len(self.samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, tracks_csv, video_name, contact_frame, player_id, label_idx = self.samples[idx]\n",
        "\n",
        "        # 1. Extract video clip\n",
        "        tracks_df = pd.read_csv(tracks_csv)\n",
        "        clip = self.clip_extractor.extract_clip(video_path, tracks_df, contact_frame, player_id)\n",
        "\n",
        "        if clip is None:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        # 2. Extract shuttle features\n",
        "        if video_name in self.shuttle_cache:\n",
        "            frames, vis, xs, ys = self.shuttle_cache[video_name]\n",
        "            shuttle_features = self.shuttle_extractor.extract_features_for_window(\n",
        "                frames, vis, xs, ys, contact_frame,\n",
        "                window_size=self.clip_cfg.pad_frames * 2 + 1\n",
        "            )\n",
        "        else:\n",
        "            # Fallback: zero features\n",
        "            shuttle_features = np.zeros((self.clip_cfg.pad_frames * 2 + 1, 10), dtype=np.float32)\n",
        "\n",
        "        # 3. Augmentation (apply same to both streams)\n",
        "        if self.augment:\n",
        "            clip, shuttle_features = self._augment(clip, shuttle_features)\n",
        "\n",
        "        # 4. Normalize and convert to tensors\n",
        "        clip = clip.astype(np.float32) / 255.0\n",
        "        clip_tensor = torch.from_numpy(clip).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
        "\n",
        "        shuttle_tensor = torch.from_numpy(shuttle_features).T  # (F, T)\n",
        "\n",
        "        label = torch.tensor(label_idx, dtype=torch.long)\n",
        "\n",
        "        return clip_tensor, shuttle_tensor, label\n",
        "\n",
        "    def _augment(self, clip: np.ndarray, shuttle_features: np.ndarray):\n",
        "        \"\"\"Synchronized augmentation\"\"\"\n",
        "        # Horizontal flip (also flip shuttle x-coordinates and vx)\n",
        "        if np.random.rand() > 0.5:\n",
        "            clip = np.flip(clip, axis=2).copy()\n",
        "            # Flip x, vx, ax (indices 0, 2, 5)\n",
        "            shuttle_features[:, 0] *= -1  # x\n",
        "            shuttle_features[:, 2] *= -1  # vx\n",
        "            shuttle_features[:, 5] *= -1  # ax\n",
        "\n",
        "        # Brightness (video only)\n",
        "        if np.random.rand() > 0.5:\n",
        "            factor = np.random.uniform(0.8, 1.2)\n",
        "            clip = np.clip(clip * factor, 0, 255)\n",
        "\n",
        "        return clip, shuttle_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031689c3-cb69-4c26-8d59-19e3b8a37d58",
      "metadata": {
        "id": "031689c3-cb69-4c26-8d59-19e3b8a37d58"
      },
      "source": [
        "# **3. MODEL: Two-Stream Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8dc54ab-caeb-40a1-a7a9-c48e7c5dc21f",
      "metadata": {
        "id": "c8dc54ab-caeb-40a1-a7a9-c48e7c5dc21f"
      },
      "outputs": [],
      "source": [
        "class SpatioTemporalConv(nn.Module):\n",
        "    \"\"\"R(2+1)D block\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.spatial = nn.Conv3d(\n",
        "            in_channels, out_channels,\n",
        "            kernel_size=(1, kernel_size, kernel_size),\n",
        "            stride=(1, stride, stride),\n",
        "            padding=(0, padding, padding),\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn_spatial = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "        self.temporal = nn.Conv3d(\n",
        "            out_channels, out_channels,\n",
        "            kernel_size=(kernel_size, 1, 1),\n",
        "            stride=(stride, 1, 1),\n",
        "            padding=(padding, 0, 0),\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn_temporal = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn_spatial(self.spatial(x)))\n",
        "        x = F.relu(self.bn_temporal(self.temporal(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9ea087-a270-490e-8ea0-6bb48b01ac28",
      "metadata": {
        "id": "9c9ea087-a270-490e-8ea0-6bb48b01ac28"
      },
      "outputs": [],
      "source": [
        "class VisionStream(nn.Module):\n",
        "    \"\"\"R(2+1)D for video understanding\"\"\"\n",
        "    def __init__(self, input_channels: int = 3, hidden_dim: int = 512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = SpatioTemporalConv(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, num_blocks=2)\n",
        "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, hidden_dim, num_blocks=2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
        "        layers = []\n",
        "        layers.append(SpatioTemporalConv(in_channels, out_channels, 3, stride, 1))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(SpatioTemporalConv(out_channels, out_channels, 3, 1, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T, H, W)\n",
        "        x = self.stem(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        return x.view(x.size(0), -1)  # (B, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e51ef3-a151-4b01-88d7-894b407fc449",
      "metadata": {
        "id": "29e51ef3-a151-4b01-88d7-894b407fc449"
      },
      "outputs": [],
      "source": [
        "class ShuttleStream(nn.Module):\n",
        "    \"\"\"1D CNN for shuttle trajectory features\"\"\"\n",
        "    def __init__(self, input_features: int = 10, hidden_dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Temporal convolutions\n",
        "        self.conv1 = nn.Conv1d(input_features, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(128, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, F, T)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        return x.squeeze(-1)  # (B, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e9d1ab-dba8-4052-9025-aab8b61fe36e",
      "metadata": {
        "id": "d2e9d1ab-dba8-4052-9025-aab8b61fe36e"
      },
      "outputs": [],
      "source": [
        "class TwoStreamFusionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Two-stream fusion model for shot classification\n",
        "\n",
        "    Combines visual appearance and shuttle trajectory\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        vision_dim: int = 512,\n",
        "        shuttle_dim: int = 128,\n",
        "        fusion_dim: int = 256,\n",
        "        dropout: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vision_stream = VisionStream(input_channels=3, hidden_dim=vision_dim)\n",
        "        self.shuttle_stream = ShuttleStream(input_features=10, hidden_dim=shuttle_dim)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(vision_dim + shuttle_dim, fusion_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_dim, fusion_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "    def forward(self, video, shuttle_features):\n",
        "        # Extract features from both streams\n",
        "        vision_feat = self.vision_stream(video)  # (B, vision_dim)\n",
        "        shuttle_feat = self.shuttle_stream(shuttle_features)  # (B, shuttle_dim)\n",
        "\n",
        "        # Concatenate and fuse\n",
        "        combined = torch.cat([vision_feat, shuttle_feat], dim=1)  # (B, vision_dim + shuttle_dim)\n",
        "        fused = self.fusion(combined)  # (B, fusion_dim)\n",
        "\n",
        "        # Classify\n",
        "        logits = self.classifier(fused)  # (B, num_classes)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f46e758-17f6-4752-be48-a2ea7387dec1",
      "metadata": {
        "id": "2f46e758-17f6-4752-be48-a2ea7387dec1"
      },
      "source": [
        "# **4. TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691f2fee-d98a-4753-9d3c-32fbeffcd7ed",
      "metadata": {
        "id": "691f2fee-d98a-4753-9d3c-32fbeffcd7ed"
      },
      "outputs": [],
      "source": [
        "class TwoStreamTrainer:\n",
        "    \"\"\"Trainer for two-stream model\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        lr: float = 1e-3,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            self.optimizer, T_max=50\n",
        "        )\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (video, shuttle, target) in enumerate(self.train_loader):\n",
        "            video = video.to(self.device)\n",
        "            shuttle = shuttle.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(video, shuttle)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "        return total_loss / len(self.train_loader), correct / total\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for video, shuttle, target in self.val_loader:\n",
        "                video = video.to(self.device)\n",
        "                shuttle = shuttle.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "\n",
        "                output = self.model(video, shuttle)\n",
        "                loss = self.criterion(output, target)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "\n",
        "        return total_loss / len(self.val_loader), correct / total\n",
        "\n",
        "    def train(self, epochs: int):\n",
        "        best_val_acc = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} : Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} .. | .. Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\\n\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_twostream_model.pth')\n",
        "                print(f\"  ✓ Saved best model (val_acc: {val_acc:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fbcad0b-8742-425e-9c35-d3d9868573da",
      "metadata": {
        "id": "4fbcad0b-8742-425e-9c35-d3d9868573da"
      },
      "source": [
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ed1090-0d8d-4862-9e3c-09ce7fac0a70",
      "metadata": {
        "id": "08ed1090-0d8d-4862-9e3c-09ce7fac0a70"
      },
      "outputs": [],
      "source": [
        "shot_classes = [\n",
        "    \"block\", \"clear\", \"cross_net\",\n",
        "    \"drive\", \"drop\", \"jump_smash\",\n",
        "    \"lift\", \"push\", \"serve\",\n",
        "    \"smash\", \"straight_net\", \"tap\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbdfcd3-26f5-403e-a675-9a674e4c65b7",
      "metadata": {
        "id": "fbbdfcd3-26f5-403e-a675-9a674e4c65b7"
      },
      "outputs": [],
      "source": [
        "# 1. Setup extractors\n",
        "\n",
        "clip_cfg = ClipExtractionConfig(pad_frames=15, clip_size=(112, 112))\n",
        "clip_extractor = ClipExtractor(clip_cfg)\n",
        "\n",
        "shuttle_cfg = ShuttleFeatureConfig(smooth_window=5)\n",
        "shuttle_extractor = ShuttleFeatureExtractor(shuttle_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23036202-12b3-4983-889e-ae4458f83c7e",
      "metadata": {
        "id": "23036202-12b3-4983-889e-ae4458f83c7e"
      },
      "outputs": [],
      "source": [
        "# 2. Load data\n",
        "\n",
        "video_paths = [\n",
        "    \"data/videos/shi_vit_rally_1.mp4\",\n",
        "    \"data/videos/shi_vit_rally_2.mp4\",\n",
        "    \"data/videos/shi_vit_rally_3.mp4\"\n",
        "]\n",
        "\n",
        "tracks_paths = [\n",
        "    \"data/player_tracks/shi_vit_rally_1_tracks.csv\",\n",
        "    \"data/player_tracks/shi_vit_rally_2_tracks.csv\",\n",
        "    \"data/player_tracks/shi_vit_rally_3_tracks.csv\"\n",
        "]\n",
        "\n",
        "shuttle_paths = [\n",
        "    \"data/shuttle_tracks/shi_vit_rally_1_ball.csv\",\n",
        "    \"data/shuttle_tracks/shi_vit_rally_2_ball.csv\",\n",
        "    \"data/shuttle_tracks/shi_vit_rally_3_ball.csv\"\n",
        "]\n",
        "\n",
        "contact_frames = {\n",
        "    \"shi_vit_rally_1.mp4\": [\n",
        "        94, 114, 137, 159, 192,\n",
        "        208, 231, 263, 278, 305,\n",
        "        329, 353, 382, 409, 445,\n",
        "        460, 486, 514, 527, 555,\n",
        "        581, 606, 666, 688, 730\n",
        "    ],\n",
        "    \"shi_vit_rally_2.mp4\": [\n",
        "        62, 86, 107, 130, 172,\n",
        "        198, 226, 257, 286, 337,\n",
        "        347, 367, 379, 417, 450,\n",
        "        471, 502, 539, 566, 591,\n",
        "        628, 663, 678, 710, 754,\n",
        "        767, 795, 829, 850, 886,\n",
        "        901, 940, 968, 989\n",
        "    ],\n",
        "    \"shi_vit_rally_3.mp4\": [\n",
        "        19, 42, 83, 96, 122,\n",
        "        148, 172, 203, 215, 249,\n",
        "        264, 286, 309, 349, 379,\n",
        "        396, 428\n",
        "    ]\n",
        "}\n",
        "\n",
        "labels = {\n",
        "    \"shi_vit_rally_1.mp4\": {\n",
        "        \"contact_94\": {\"1\": \"serve\", \"2\": \"negative\"},\n",
        "        \"contact_114\": {\"1\": \"negative\", \"2\": \"cross_net\"},\n",
        "        \"contact_137\": {\"1\": \"cross_net\", \"2\": \"negative\"},\n",
        "        \"contact_159\": {\"1\": \"negative\", \"2\": \"lift\"},\n",
        "        \"contact_192\": {\"1\": \"drop\", \"2\": \"negative\"},\n",
        "        \"contact_208\": {\"1\": \"negative\", \"2\": \"push\"},\n",
        "        \"contact_231\": {\"1\": \"lift\", \"2\": \"negative\"},\n",
        "        \"contact_263\": {\"1\": \"negative\", \"2\": \"drop\"},\n",
        "        \"contact_278\": {\"1\": \"push\", \"2\": \"negative\"},\n",
        "        \"contact_305\": {\"1\": \"negative\", \"2\": \"push\"},\n",
        "        \"contact_329\": {\"1\": \"straight_net\", \"2\": \"negative\"},\n",
        "        \"contact_353\": {\"1\": \"negative\", \"2\": \"cross_net\"},\n",
        "        \"contact_382\": {\"1\": \"lift\", \"2\": \"negative\"},\n",
        "        \"contact_409\": {\"1\": \"negative\", \"2\": \"clear\"},\n",
        "        \"contact_445\": {\"1\": \"jump_smash\", \"2\": \"negative\"},\n",
        "        \"contact_460\": {\"1\": \"negative\", \"2\": \"block\"},\n",
        "        \"contact_486\": {\"1\": \"lift\", \"2\": \"negative\"},\n",
        "        \"contact_514\": {\"1\": \"negative\", \"2\": \"smash\"},\n",
        "        \"contact_527\": {\"1\": \"block\", \"2\": \"negative\"},\n",
        "        \"contact_555\": {\"1\": \"negative\", \"2\": \"cross_net\"},\n",
        "        \"contact_581\": {\"1\": \"straight_net\", \"2\": \"negative\"},\n",
        "        \"contact_606\": {\"1\": \"negative\", \"2\": \"lift\"},\n",
        "        \"contact_666\": {\"1\": \"drop\", \"2\": \"negative\"},\n",
        "        \"contact_688\": {\"1\": \"negative\", \"2\": \"lift\"},\n",
        "        \"contact_730\": {\"1\": \"drive\", \"2\": \"negative\"}\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1239fb94-f2af-47b3-a602-1b8a5a0f0dfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1239fb94-f2af-47b3-a602-1b8a5a0f0dfd",
        "outputId": "006fd07e-863b-402d-c466-2949932b36db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_2.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "video name shi_vit_rally_3.mp4 not in labels.\n",
            "\n",
            "Two-Stream Dataset: 25 samples\n"
          ]
        }
      ],
      "source": [
        "# 3. Create dataset\n",
        "\n",
        "dataset = TwoStreamDataset(\n",
        "    video_paths, tracks_paths, shuttle_paths,\n",
        "    contact_frames, labels,\n",
        "    clip_extractor, shuttle_extractor,\n",
        "    shot_classes, clip_cfg, augment=True\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6010f101-7230-4e45-910b-3d5d38ca1329",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6010f101-7230-4e45-910b-3d5d38ca1329",
        "outputId": "04741bec-2dd6-4a5b-c52c-18914eeb2fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 7,156,748\n"
          ]
        }
      ],
      "source": [
        "# 4. Create model\n",
        "\n",
        "model = TwoStreamFusionModel(\n",
        "    num_classes=len(shot_classes),\n",
        "    vision_dim=512,\n",
        "    shuttle_dim=128,\n",
        "    fusion_dim=256\n",
        ")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d33ad6-2346-410d-a42e-9dc20c02e4c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "23d33ad6-2346-410d-a42e-9dc20c02e4c4",
        "outputId": "33a67881-c7d7-4ddf-9bd8-d7b9e05db552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 : Train Loss: 2.3785, Train Acc: 0.1200 .. | .. Val Loss: 2.4070, Val Acc: 0.2400\n",
            "\n",
            "  ✓ Saved best model (val_acc: 0.2400)\n",
            "Epoch 2/50 : Train Loss: 2.3957, Train Acc: 0.2000 .. | .. Val Loss: 2.4856, Val Acc: 0.2400\n",
            "\n",
            "Epoch 3/50 : Train Loss: 2.7000, Train Acc: 0.1600 .. | .. Val Loss: 5.3684, Val Acc: 0.2400\n",
            "\n",
            "Epoch 4/50 : Train Loss: 2.2383, Train Acc: 0.2400 .. | .. Val Loss: 2.4033, Val Acc: 0.2400\n",
            "\n",
            "Epoch 5/50 : Train Loss: 2.6668, Train Acc: 0.2000 .. | .. Val Loss: 2.5562, Val Acc: 0.2400\n",
            "\n",
            "Epoch 6/50 : Train Loss: 2.2048, Train Acc: 0.1200 .. | .. Val Loss: 2.3230, Val Acc: 0.2400\n",
            "\n",
            "Epoch 7/50 : Train Loss: 2.7638, Train Acc: 0.1600 .. | .. Val Loss: 2.3577, Val Acc: 0.2400\n",
            "\n",
            "Epoch 8/50 : Train Loss: 2.0411, Train Acc: 0.2400 .. | .. Val Loss: 3.0675, Val Acc: 0.2400\n",
            "\n",
            "Epoch 9/50 : Train Loss: 2.5960, Train Acc: 0.2400 .. | .. Val Loss: 3.3220, Val Acc: 0.2400\n",
            "\n",
            "Epoch 10/50 : Train Loss: 2.3080, Train Acc: 0.2000 .. | .. Val Loss: 2.9196, Val Acc: 0.2400\n",
            "\n",
            "Epoch 11/50 : Train Loss: 2.3006, Train Acc: 0.1200 .. | .. Val Loss: 2.7678, Val Acc: 0.2400\n",
            "\n",
            "Epoch 12/50 : Train Loss: 2.6075, Train Acc: 0.2000 .. | .. Val Loss: 2.5269, Val Acc: 0.2400\n",
            "\n",
            "Epoch 13/50 : Train Loss: 1.9937, Train Acc: 0.2400 .. | .. Val Loss: 2.3498, Val Acc: 0.2400\n",
            "\n",
            "Epoch 14/50 : Train Loss: 2.0148, Train Acc: 0.2800 .. | .. Val Loss: 2.4396, Val Acc: 0.2400\n",
            "\n",
            "Epoch 15/50 : Train Loss: 2.0108, Train Acc: 0.2400 .. | .. Val Loss: 2.9788, Val Acc: 0.2400\n",
            "\n",
            "Epoch 16/50 : Train Loss: 2.3317, Train Acc: 0.2800 .. | .. Val Loss: 3.4824, Val Acc: 0.2400\n",
            "\n",
            "Epoch 17/50 : Train Loss: 2.1384, Train Acc: 0.2800 .. | .. Val Loss: 2.9453, Val Acc: 0.2400\n",
            "\n",
            "Epoch 18/50 : Train Loss: 2.0645, Train Acc: 0.2400 .. | .. Val Loss: 2.4820, Val Acc: 0.2400\n",
            "\n",
            "Epoch 19/50 : Train Loss: 1.9793, Train Acc: 0.3200 .. | .. Val Loss: 2.7782, Val Acc: 0.2400\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1814573959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoStreamTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3962961171.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3962961171.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuttle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mshuttle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuttle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 5. Train\n",
        "\n",
        "trainer = TwoStreamTrainer(model, train_loader, val_loader, lr=1e-3)\n",
        "trainer.train(epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "468549c4-c65d-4c01-860e-b891c99de493",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "468549c4-c65d-4c01-860e-b891c99de493",
        "outputId": "ffd7fa9d-14d9-4250-9f31-421e9fca9470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: opencv-python-headless\n",
            "Version: 4.12.0.88\n",
            "Summary: Wrapper package for OpenCV python bindings.\n",
            "Home-page: https://github.com/opencv/opencv-python\n",
            "Author: \n",
            "Author-email: \n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: numpy\n",
            "Required-by: albucore, albumentations\n"
          ]
        }
      ],
      "source": [
        "!pip show opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nZYHFlOJxzY"
      },
      "id": "3nZYHFlOJxzY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}